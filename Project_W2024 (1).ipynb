{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project\n",
    "\n",
    "In this Project, you will bring together many of the tools and techniques that you have learned throughout this course into a final project. You can choose from many different paths to get to the solution. \n",
    "\n",
    "### Business scenario\n",
    "\n",
    "You work for a training organization that recently developed an introductory course about machine learning (ML). The course includes more than 40 videos that cover a broad range of ML topics. You have been asked to create an application that will students can use to quickly locate and view video content by searching for topics and key phrases.\n",
    "\n",
    "You have downloaded all of the videos to an Amazon Simple Storage Service (Amazon S3) bucket. Your assignment is to produce a dashboard that meets your supervisorâ€™s requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project steps\n",
    "\n",
    "To complete this project, you will follow these steps:\n",
    "\n",
    "1. [Viewing the video files](#1.-Viewing-the-video-files)\n",
    "2. [Transcribing the videos](#2.-Transcribing-the-videos)\n",
    "3. [Normalizing the text](#3.-Normalizing-the-text)\n",
    "4. [Extracting key phrases and topics](#4.-Extracting-key-phrases-and-topics)\n",
    "5. [Creating the dashboard](#5.-Creating-the-dashboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful information\n",
    "\n",
    "The following cell contains some information that might be useful as you complete this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"c56161a939430l3396553t1w744137092661-labbucket-rn642jaq01e9\"\n",
    "job_data_access_role = 'arn:aws:iam::744137092661:role/service-role/c56161a939430l3396553t1w7-ComprehendDataAccessRole-1P24MSS91ADHP'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Viewing the video files\n",
    "([Go to top](#Capstone-8:-Bringing-It-All-Together))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source video files are located in the following shared Amazon Simple Storage Service (Amazon S3) bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-26 20:17:33  410925369 Mod01_Course Overview.mp4\r\n",
      "2021-04-26 20:10:02   39576695 Mod02_Intro.mp4\r\n",
      "2021-04-26 20:31:23  302994828 Mod02_Sect01.mp4\r\n",
      "2021-04-26 20:17:33  416563881 Mod02_Sect02.mp4\r\n",
      "2021-04-26 20:17:33  318685583 Mod02_Sect03.mp4\r\n",
      "2021-04-26 20:17:33  255877251 Mod02_Sect04.mp4\r\n",
      "2021-04-26 20:23:51   99988046 Mod02_Sect05.mp4\r\n",
      "2021-04-26 20:24:54   50700224 Mod02_WrapUp.mp4\r\n",
      "2021-04-26 20:26:27   60627667 Mod03_Intro.mp4\r\n",
      "2021-04-26 20:26:28  272229844 Mod03_Sect01.mp4\r\n",
      "2021-04-26 20:27:06  309127124 Mod03_Sect02_part1.mp4\r\n",
      "2021-04-26 20:27:06  195635527 Mod03_Sect02_part2.mp4\r\n",
      "2021-04-26 20:28:03  123924818 Mod03_Sect02_part3.mp4\r\n",
      "2021-04-26 20:31:28  171681915 Mod03_Sect03_part1.mp4\r\n",
      "2021-04-26 20:32:07  285200083 Mod03_Sect03_part2.mp4\r\n",
      "2021-04-26 20:33:17  105470345 Mod03_Sect03_part3.mp4\r\n",
      "2021-04-26 20:35:10  157185651 Mod03_Sect04_part1.mp4\r\n",
      "2021-04-26 20:36:27  187435635 Mod03_Sect04_part2.mp4\r\n",
      "2021-04-26 20:36:40  280720369 Mod03_Sect04_part3.mp4\r\n",
      "2021-04-26 20:40:01  443479313 Mod03_Sect05.mp4\r\n",
      "2021-04-26 20:40:08  234182186 Mod03_Sect06.mp4\r\n",
      "2021-04-26 20:40:33  207718047 Mod03_Sect07_part1.mp4\r\n",
      "2021-04-26 20:42:07  125592110 Mod03_Sect07_part2.mp4\r\n",
      "2021-04-26 20:45:10  508500301 Mod03_Sect07_part3.mp4\r\n",
      "2021-04-26 20:46:16  320126756 Mod03_Sect08.mp4\r\n",
      "2021-04-26 20:46:43   41839508 Mod03_WrapUp.mp4\r\n",
      "2021-04-26 20:46:55   34148489 Mod04_Intro.mp4\r\n",
      "2021-04-26 20:48:24   84959465 Mod04_Sect01.mp4\r\n",
      "2021-04-26 20:48:25  345182970 Mod04_Sect02_part1.mp4\r\n",
      "2021-04-26 20:51:34  218661651 Mod04_Sect02_part2.mp4\r\n",
      "2021-04-26 20:53:32  430140637 Mod04_Sect02_part3.mp4\r\n",
      "2021-04-26 20:56:03   22036605 Mod04_WrapUp.mp4\r\n",
      "2021-04-26 20:57:18   49187118 Mod05_Intro.mp4\r\n",
      "2021-04-26 20:58:19  245798071 Mod05_Sect01_ver2.mp4\r\n",
      "2021-04-26 20:58:50  233314835 Mod05_Sect02_part1_ver2.mp4\r\n",
      "2021-04-26 20:59:14  348545306 Mod05_Sect02_part2.mp4\r\n",
      "2021-04-26 20:59:17  239142711 Mod05_Sect03_part1.mp4\r\n",
      "2021-04-26 21:06:04  267533559 Mod05_Sect03_part2.mp4\r\n",
      "2021-04-26 21:06:06  212502220 Mod05_Sect03_part3.mp4\r\n",
      "2021-04-26 21:06:48  206317022 Mod05_Sect03_part4_ver2.mp4\r\n",
      "2021-04-26 21:06:48   60361230 Mod05_WrapUp_ver2.mp4\r\n",
      "2021-04-26 21:09:14   35397860 Mod06_Intro.mp4\r\n",
      "2021-04-26 21:09:24  845633599 Mod06_Sect01.mp4\r\n",
      "2021-04-26 21:10:47  326126684 Mod06_Sect02.mp4\r\n",
      "2021-04-26 21:12:26   19790740 Mod06_WrapUp.mp4\r\n",
      "2021-04-26 21:12:56  131249036 Mod07_Sect01.mp4\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: moviepy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.0.3)\n",
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (4.39.3)\n",
      "Requirement already satisfied: accelerate in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.29.2)\n",
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from moviepy) (4.66.2)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from moviepy) (2.31.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from moviepy) (1.26.4)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from moviepy) (2.34.0)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from moviepy) (0.4.9)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from imageio<3.0,>=2.5->moviepy) (10.2.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from imageio-ffmpeg>=0.2.0->moviepy) (69.1.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install moviepy torch transformers accelerate nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib confmisc.c:767:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:4554:(_snd_config_evaluate) function snd_func_card_driver returned error: No such file or directory\n",
      "ALSA lib confmisc.c:392:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:4554:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1246:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:4554:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5033:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2501:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "ALSA lib confmisc.c:767:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:4554:(_snd_config_evaluate) function snd_func_card_driver returned error: No such file or directory\n",
      "ALSA lib confmisc.c:392:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:4554:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1246:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:4554:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5033:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2501:(snd_pcm_open_noupdate) Unknown PCM default\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from moviepy.editor import VideoFileClip\n",
    "import boto3\n",
    "import shutil\n",
    "import re\n",
    "from unicodedata import normalize\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output, Video\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transcribing the videos\n",
    " ([Go to top](#Capstone-8:-Bringing-It-All-Together))\n",
    "\n",
    "Use this section to implement your solution to transcribe the videos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video 1/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod01_Course Overview.mp4\n",
      "Mod01_Course Overview.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod01_Course Overview.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod01_Course Overview.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod01_Course Overview.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video: CUR-TF-200-ACMNLP-1/video/Mod01_Course Overview.mp4\n",
      "Text:  Hi, and welcome to Amazon Academy Machine Learning Foundations. In this module, you'll learn about the course objectives, various job roles in the machine learning domain, and where you can go to learn more about machine learning. After completing this module, you should be able to identify course prerequisites and objectives, indicate the role of the data scientist in business, and identify resources for further learning. We're now going to look at the prerequisites for taking this course. Before you take this course, we recommend that you first complete AWS Academy Cloud Foundations. You should also have some general technical knowledge of IT, including foundational computer literacy skills like basic computer concepts, email, file management, and a good understanding of the Internet. We also recommend that you have intermediate skills with Python programming and a general knowledge of applied statistics. Finally, general business knowledge is important for this course. This includes insight into how information technology is used in business. It's also important to have business-related skill sets, such as communication skills, leadership skills, and an orientation towards customer service. leadership skills, and an orientation towards customer service. In this course, you'll be introduced to the key concepts of machine learning, its tools, and its uses. You'll also be introduced to, and work with, some of the AWS services for machine learning. You'll learn how to recognize how machine learning and deep learning are part of artificial intelligence, describe artificial intelligence and machine learning and deep learning are part of artificial intelligence, describe artificial intelligence and machine learning terminology, identify how machine learning can be used to solve a business problem, describe the machine learning process, list the tools available to data scientists, and identify when to use machine learning instead of traditional software development methods. As part of this course, you'll also learn how to implement a machine learning pipeline. This includes how to formulate a problem from a business request, obtain and secure data for machine learning, build a Jupyter notebook by using Amazon SageMaker, outline the process for evaluating data. Explain why data needs to be pre-processed. And use open source tools to examine and pre-process data. You will also use Amazon SageMaker to train and host a machine learning model. Use cross-validation to test the performance host a machine learning model. Use cross-validation to test the performance of a machine learning model. Use a hosted model for inference. Create an Amazon SageMaker hyperparameter tuning job to optimize a model's effectiveness. And finally, how to use managed Amazon machine learning services to solve specific machine learning problems in forecasting, computer vision, and natural language processing. We'll now review the course outline. To achieve the course objectives, you'll complete the following modules. To start, in module 2 you'll get an introduction to machine learning. In module 3, you'll learn how to implement a machine learning pipeline with Amazon SageMaker. Modules 4, 5, and 6 describe how to apply managed Amazon machine learning services for problems in forecasting, computer vision, and natural language processing. Finally, Module 7 is a summary of the course. It also includes an overview of steps you can take to work towards the AWS Certified Machine Learning specialty. The next five slides provide more detail about the subtopics covered in each module. The purpose of Module 2 is to introduce you to major concepts for understanding machine learning. Section 1 describes the overall field of machine learning and how machine learning relates to artificial intelligence and deep learning. In Section 2, you'll learn about some of the most common business problems you can solve with machine learning. Section 3 describes the general workflow for solving machine learning problems. You'll also learn some of the more common machine learning terms. In Section 4, you'll review some of the commonly used tools by machine learning professionals. And lastly, in Section 5, you'll get an overview of some of the common challenges you'll face when working with machine learning problems. In Module 3, you'll get an introduction to Amazon SageMaker and how you can use it to implement a machine learning pipeline. The module focuses on the application of machine learning to solve problems with several public domain datasets as examples of the machine learning pipeline. Section 1 introduces you to defining business problems and the datasets we will use during this module. Section 2 through 8 describe the phases of the machine learning pipeline by using computer vision as an example application. In Section 2, you'll learn how to collect and secure data. Section 3 describes different techniques for evaluating data. In Section 4, you'll learn about the process of feature engineering. Section 5 describes the steps you'll take to train a model with SageMaker. In Section 6, you'll get an overview of the options in SageMaker for hosting and using a model. Finally, sections 7 and 8 cover how to evaluate and tune your model with SageMaker. In this module, you'll be introduced to using machine learning to create forecasts based on a time series data. In section 1, you'll be introduced to forecasting and some of its common applications. Section 2 outlines some of the pitfalls of using time series data to make forecasts. Finally, in Section 3, you'll get an overview of how to use Amazon Forecast. In this module, you'll learn about using machine learning for computer vision. Section 1 describes the general problems you can solve with computer vision. In Section 2, you'll learn about the process for analyzing images and videos. And in Section 3, you'll learn the steps you'll need to take to prepare datasets for computer vision. In this module, you'll be introduced to natural language processing with machine learning. In Section 1, you'll learn about the general set of problems you can solve with natural language processing. Section 2 reviews some of the managed Amazon machine learning services you can use to address natural language processing problems. These services include Amazon Transcribe, Amazon Translate, Amazon Lex, Amazon Comprehend, and Amazon Polly. Module 7 is the final module of the course. In this module, you'll review what you've learned throughout this course. You'll also be introduced to the next steps you should take if you want to achieve the AWS Certified Machine Learning specialty. Section 1 of this module summarizes the topics you've covered in this course. In Section 2, you'll learn more about the AWS documentation. You'll also review two common frameworks for applying AWS services. And finally, section three describes the steps you should take if you want to continue working towards the AWS certified machine learning specialty. In this section, you'll learn about some of the more common job roles for machine learning professionals. If you're interested in a data scientist role, focus on developing analytical, statistical, and programming skills. As a data scientist, you'll use those skills to collect, analyze, and interpret large data sets. Some universities now offer degrees in data science, but data scientists often have degrees in related fields like statistics, math, computer science, or economics. As a data scientist, you'll need technical competencies in statistics, machine learning, programming languages, and data analytics. If you'd like to have a career as a machine learning engineer, the skills you'll need will be similar to a data scientist's skill set. Like data scientists, machine learning engineers also require technical competencies in statistics and machine learning. However, you'll focus more on programming skills and software architecture than analysis and interpretation. As a machine learning engineer, you'll apply those programming and architecture skills to design and develop machine learning systems. Machine learning engineers often have previous experience with software development, and they rely more heavily on programming and software engineering than other machine learning roles. You might also be interested in a career in science where you can apply machine learning technology to your field. Machine learning is having an impact in everything from astronomy to zoology, so there are many different paths open to you. As an applied science researcher, your primary focus will be on the type of science you're working on. You'll need some of the same skills as a data scientist, but you'll also need to know how to apply those skills to your chosen domain. Thus, applied science roles also require technical competencies in statistics and machine learning. Many software developers are now integrating machine learning into their applications. If you're interested in a career as a software developer, you should also include machine learning technology in your studies. As a machine learning developer, your primary focus will be software development skills. But you'll also need some of the same skills as a data scientist, so make sure you take coursework in statistics and applied mathematics. And here's a final note for this module. We recommend reviewing your student guide. In your student guide, you'll find links to documentation and other resources you'll use throughout the course. That's it for this introduction. Thanks for watching. We'll see you in the next video.\n",
      "Processing video 2/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod02_Intro.mp4\n",
      "Mod02_Intro.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod02_Intro.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod02_Intro.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod02_Intro.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod02_Intro.mp4\n",
      "Text:  Hi, and welcome to Module 2 of AWS Academy Machine Learning. In this module, we're going to introduce machine learning. We'll first look at the business problems that can be solved by machine learning. We'll then talk about terminology, process, tools, and some of the challenges you'll face. process, tools, and some of the challenges you'll face. After completing this module, you should be able to recognize how machine learning and deep learning are part of artificial intelligence, describe artificial intelligence and machine learning terminology, identify how machine learning can be used to solve a business problem, describe the machine learning process, list the tools available to data scientists, and identify when to use machine learning instead of traditional software development methods. You're now ready to get started with Section 1. See you in the next video.\n",
      "Processing video 3/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod02_Sect01.mp4\n",
      "Mod02_Sect01.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod02_Sect01.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod02_Sect01.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod02_Sect01.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod02_Sect01.mp4\n",
      "Text:  Hi, and welcome to Section 1. In this section, we're going to talk about what machine learning is. This course is an introduction to machine learning, which is also known as ML. But first, we'll discuss where machine learning fits into the larger picture. Machine learning is a subset of artificial intelligence, or AI. This is a broad branch of computer science that's focused on building machines that can do human tasks. Deep learning is a subdomain of machine learning. To understand where these all fit together, we'll discuss each one. As we just mentioned, machine learning is a subset of a broader computer science field known as artificial intelligence. AI focuses on building machines that can perform tasks a human would typically perform. In contemporary popular culture, you've probably seen AIs in movies, television, or works of fiction. For example, you might have seen AIs that control the world around them, or that start acting on their own initiative. These AIs started as computer agents that perceive their environments and take actions to achieve a specific goal, though maybe not the outcome their creators originally wished for. Other fictional AIs interact extensively with humans as helpers or workers, and they generally do a better job working with humanity, but they're more general in purpose. These kinds of AIs are examples of Artificial General Intelligence, or AGI. They have the capacity to learn or understand any task that a human can. AI problems typically span many fields of research, such as natural language processing, reasoning, knowledge representation, learning, perception, and physical environment interaction. AI isn't yet a reality for us, unless we are all truly living in a simulation. But every year, we move closer to it in each of those areas. You might have also read or seen commentary on the ethics of creating AI. Not all views are positive, perhaps partly in fear of the malicious fictional AIs that want to destroy humanity or use them as power sources. Or perhaps they're concerned about the risk of mass unemployment because an intelligent machine could work 24-7 and not need any breaks. Don't worry, though. We're not going to build the next rogue AI in this course. Maybe in the next one. If you do a search, you'll probably find many definitions of machine learning. There isn't a universally agreed-upon definition, so we'll start by looking at a couple of definitions. For example, we could say, machine learning is the scientific study of algorithms and statistical models to perform a task by using inference instead of instructions. This isn't a bad starting point. The key point here is, using algorithms and statistical models instead of instructions. To help you better understand this, we'll apply this idea to a concrete example. Suppose you need to write an application that determines if an email message is spam or not. Without machine learning, you'd need to write a complex series of decision statements using if and else statements. You'd also need to use words in the subject or body, the number of links, and the length of the message to determine if an email message is spam. It would be hard and labor-intensive to build a large set of rules covering every possibility. With machine learning, however, you could use a list of email messages that were marked as spam or not spam and train a machine learning model. The model would then learn which patterns of words, length, and other attributes are good indicators of spam messages. If you presented the model with an email message it hadn't seen before, the model would perform a prediction to say whether the message was spam or not spam. Deep learning represents a significant leap forward in the capabilities for artificial intelligence and machine learning. The theory behind deep learning was created from how the human brain works. An artificial neural network, or ANN, is inspired by the biological neurons found in the brain, although the implementation has become very different. Artificial neurons have one or more inputs and a single output. These neurons fire, or activate, their outputs based on a transformation of the inputs. A neural network is composed of layers of these artificial neurons with connections between the layers. There are typically input, output, and hidden layers in the network. The output of a single neuron connects to the inputs of all the neurons in the next layer. The network is then asked to solve a problem. The input layer is populated from the training data, and the neurons activate throughout the layers until an answer is presented in the output layer. The accuracy of the output is then measured. If the output doesn't meet your threshold, the training is repeated, but with slight changes to the weights of the connections between neurons. The neural network will do this repeatedly. Each time it strengthens the connections that lead to success and diminishes the connections that lead to failure. As you'll see in this course, machine learning practitioners spend a lot of time optimizing the ML models, selecting the best data features to train with, and selecting the models with the best results. In contrast, deep learning practitioners spend almost no time on those tasks. Instead, they spend their time modeling data with different ANN architectures. Though the theory for deep learning goes back decades, the hardware needed to run deep learning problems wasn't generally accessible until recently, but now that it's available, you can use deep learning to address problems that are more complex than the problems you could have worked on before. Mainstream machine learning is a recent occurrence. Rapid advancements in machine and deep learning only started around the mid-2000s. This is partly because Moore's Law and the rise of cloud computing resulted in easier access to larger, faster, and cheaper compute and storage capabilities. You can now rent computing power for a few hours for pennies. Before this, you needed substantial investments to buy and operate large-scale compute clusters on your own. In 2012, neural networks started to be used in the ImageNet Large-Scale Visual Recognition Challenge, a machine learning competition for image recognition. The accuracy rate jumped up to about 82% and has been steadily climbing ever since. In fact, it exceeded human performance in 2015. Here are some of the key takeaways for this section. First, artificial intelligence is the broad field of building machines to perform human tasks. Also, machine learning is a subset of AI. It focuses on using data to train machine learning models so they can make predictions. Deep learning is a technique inspired from human biology. It uses layers of artificial neurons to build networks that solve problems. And last, advancements in technology, cloud computing, and algorithm development have led to a corresponding advance in machine learning capabilities and applications. That's it for this section. We'll see you in the next video.\n",
      "Processing video 4/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod02_Sect02.mp4\n",
      "Mod02_Sect02.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod02_Sect02.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod02_Sect02.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod02_Sect02.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod02_Sect02.mp4\n",
      "Text:  Hi and welcome back. In this section, we're going to look at the types of business problems machine learning can help you solve. Machine learning is used all across your digital lives. Your email spam filter is the result of a machine learning program that was trained with examples of spam and regular email messages. Based on books you're reading or products you bought, machine learning programs can predict other books or products you're likely to be interested in. Again, the machine learning program was trained with data from other readers' habits and purchases. When detecting credit card fraud, the machine learning program was trained on examples of transactions that turned out to be fraud, along with normal transactions. You can probably think of many more examples, from social media applications, using facial detection to group your photos, to detecting brain tumors in brain scans or finding anomalies in x-rays. There are three main types of machine learning. There's supervised learning, where a model uses known inputs and outputs to generalize future outputs. There's unsupervised learning, where the model doesn't know inputs or outputs, so it finds patterns in the data without help. And there's reinforcement learning, where the model interacts with its environment and learns to take actions that will maximize rewards. It's important to know the different types of ML because the type will guide you towards selecting algorithms that make sense for solving your business problem. Let's look more into each of these types. Supervised learning is a popular type of ML because it's widely applicable. It's called supervised learning because there needs to be a supervisor, a teacher, who can show the right answers, so to speak. Like any student, a supervised algorithm needs to learn by example. Essentially, it needs a teacher who uses training data to help it determine the patterns and relationships between the inputs and outputs. If you want to build an application to detect credit card fraud, you'd need training data that includes examples of fraud and examples of normal transactions. Within supervised learning, there are different types of problems, Within supervised learning, there are different types of problems, classification and regression. There are two subtypes of classification problems. The first is binary classification. Think back to the example with identifying fraudulent transactions. The target variable in this example is limited to two options, fraudulent or not fraudulent. This is a binary classification problem. There are also multi-class classification problems. These ML problems classify an observation into one of three or more categories. Say that you have an ML model that predicts why a customer is calling your store so you can reduce the number of transfers needed before the customer gets to the correct customer support department. In this case, the different customer support departments represent the variety of potential target variables, which could be many different departments, much more than just two. There are also regression problems. In a regression problem, you're no longer mapping an input to a defined number of categories. Instead, you're mapping an input to a continuous value, like an integer. One example of an ML regression problem is predicting the price of a company's stock. Computer vision is a good example of supervised learning. Is this a cat or a dog? Is there a tumor in this x-ray? Computer vision is often built with deep learning models. It automates the extraction, analysis, classification, and understanding of useful information from a single image or a sequence of images. from a single image or a sequence of images. Computer vision enables machines to identify people, places, and things in images with accuracy at or above human levels, and with greater speed and efficiency. The image data can take many forms, such as single images, video sequences, views from multiple cameras, or three-dimensional data. You'll learn more about computer vision later in this course. We'll now discuss unsupervised machine learning. Sometimes all you have is the data. There's no supervisor in the room. In unsupervised learning, labels aren't provided like they are with supervised learning. You don't know all the variables and patterns. In these instances, the machine has to uncover and create the labels itself. These models use the data they're presented with to detect emerging properties of the entire dataset. Then they construct patterns from those properties. Clustering is a common subcategory of unsupervised learning. This kind of algorithm groups data into different clusters based on similar features. It does this to better understand the attributes of a specific cluster. For example, by analyzing customer purchasing habits, unsupervised algorithms can identify groups of customers that are associated with the size tier of a company. The advantage of unsupervised algorithms is that they enable you to see patterns in the data that you weren't aware of before. Natural language processing is also known as NLP. This is another area of machine learning that's experiencing growth. This is another area of machine learning that's experiencing growth. If you've ever used Alexa or any other voice assistant, they'll use NLP to try and answer your question. NLP isn't just about speech. It's also about written text. NLP shows up in many applications. For example, NLP is used with chat or call center bots, which are automated systems that help you get your bank balance or order food from a restaurant. You can use NLP in translation tools, which convert text between languages. For example, you might use applications that translate menus in real time. NLP is also used in voice-to-text translations, which convert spoken words into text. used in voice-to-text translations, which convert spoken words into text. And finally, NLP can be used in sentiment analysis, which you can use to analyze the sentiment of comments and reviews of products, music, and movies. These sentiments could be used to give the movie an audience rating. You'll learn more about NLP later in this course. Another kind of machine learning that's been gaining popularity recently is reinforcement learning. Unlike other machine learning, reinforcement learning continuously improves its model by mining feedback from previous iterations. In reinforcement learning, an agent continuously learns, through trial and error, as it interacts in an environment. Reinforcement learning is broadly useful when the reward of a desired outcome is known, but the path to achieving it isn't. And that path requires a lot of trial and error to discover. Take the example of AWS DeepRacer. In the AWS DeepRacer simulator, the agent is the virtual car. The environment is a virtual racetrack. The actions are throttle and steering inputs to the car. And the goal is completing the racetrack as quickly as possible without deviating from the track. The car needs to learn the desired driving behavior to reach the goal of completing the track. For the car to learn this, AWS DeepRacer teams use rewards to incentivize their model to learn the desired driving behavior. In reinforcement learning, the thing driving the learning is called the agent. In this case, it's the AWS DeepRacer car. The environment is the place where the agent learns, which in this example would be the marked racetrack. When the agent does something in the environment that provokes a response, such as crossing a boundary it shouldn't cross, that's called an action. That response is called a reward or penalty depending on whether the agent did something to be reinforced or discouraged in the model. As the agent moves within the environment, its action should start receiving more rewards and fewer penalties until it meets the desired business outcome. Self-driving vehicles bring together many machine and deep learning algorithms and models to solve the problem of driving from point A to point B. Two of its main tasks are the continuous detection of the environment and forecasting changes. These involve detecting objects and localizing and predicting the movement of the detected objects. The outputs of these findings act as inputs to other systems that make decisions on what they should do with the vehicle's various controls. There are use cases in self-driving vehicles that require real-time responses to the environment. For example, if a previously hidden pedestrian walks out from behind an obstacle, the vehicle brakes need to be applied immediately. There can be no latency or room for error with these actions. Not every problem should be solved with machine learning. Sometimes regular programming will work well for your needs. If you're interested in exploring a potential machine learning solution, look for the existence of large datasets and a large number of variables. Machine learning is often the best choice if you're uncertain of the business logic or procedures needed to obtain an answer or accomplish a task. Machine learning systems can be complex. The supporting infrastructure, management support, and technical expertise need to be in place to help ensure the project's success. Here are the key takeaways for this section, where we explored some machine learning applications that are already part of everyday life. First, machine learning problems can be grouped into three categories. Supervised learning is where you have training data where you already know the answer. Unsupervised learning is where you have data but are looking for insights within the data. Reinforcement learning is where the model learns based on experience and feedback. Most business problems are supervised learning problems. That's it for this section, we'll see you in the next video.\n",
      "Processing video 5/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod02_Sect03.mp4\n",
      "Mod02_Sect03.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod02_Sect03.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod02_Sect03.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod02_Sect03.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod02_Sect03.mp4\n",
      "Text:  Hi and welcome back. This is section 3 and we're going to give you a quick, high-level overview of machine learning terminology and a typical workflow. We will cover these topics in more detail later in this course, but for now we'll focus on the larger picture. So to begin, you should always start with the business problem you or your team believe could benefit from machine learning. From there, you want to do some problem formulation. In this phase, one task is to articulate your business problem and convert it to an ML problem. After you've formulated the problem, you move on to the data preparation and pre-processing phase. You'll pull data from one or more data sources. These data sources might have differences in data or types that need to be reconciled so you can form a single, cohesive view of your data. You'll need to visualize your data and use statistics to determine if the data is consistent and can be used for machine learning. We'll look at some of the data sources later in the course. This example data has four columns containing data from three different data sources. The sources had slightly different ways of representing data and the results are shown in the table. In ML problems, columns represent features and rows represent instances. There are some issues here with the data in some of the instances. In some cases, you'll need a subject matter expert or a functional expert to understand the authenticity of the data. For example, the date that's represented as 11-2-1969 could be November 2nd or February 11th in the year 1969. Someone who owns or manages the data pool would be able to clarify this ambiguity. Also, the word mail can probably be attributed to an import issue where cells shifted position. But there could be an outside chance where it's the actual location, Mali, a city that's the capital of the Republic of Maldives. At times, this error identification isn't as simple, and you'll need an SME to review the data. You'll learn about the role of experts later in this course. Remember that one of the largest impacts you can have on the success of experts later in this course. Remember that one of the largest impacts you can have on the success of a machine learning project is to have consistent and correct data. After your data is in good shape, it's time to train your model. This is where the process gets very iterative and fluid. You'll likely go through many multiple passes of feature engineering, training, evaluating, and tuning before you find a model that meets your business goals. Feature engineering is the process of selecting or creating the features your model will be trained with. Features are the columns of data you have in your dataset. The goal of the model is to correctly estimate the target value for the new data. The ML algorithm will use the features to predict the target. In this example, the target data is the average number of steps taken in a week. Selecting the correct features can involve adding, removing, or calculating new features. You might want to make the data formats consistent. The consistent formats could be later used in the model, or you can make these changes for cosmetic reasons. Depending on the problem you want to solve with this data, you might not even need to include the name feature in the example data. What about the country feature? If this were a traditional database, you might want to move country to a lookup table, then reference it. Most ML algorithms want the data for an instance in a single row. ML algorithms need numerical data to process. You could consider turning the country text into the country's ISO code. However, the model might interpret the numerical value as having meaning, so the UK's ISO code value of 44 would be more significant than the ISO code value of the US, which is 01. In this case, splitting the data into multiple columns is fine. This is known as categorical encoding, and you'll learn about this later in the course. For other types of data, you could convert the text value into a numerical value. For example, you could use 0 or 1 to represent male or female. These numeric values can be used more easily by the model. What about the remaining features, like age, birth month, which is shown as BM in the table, or day of week, which is shown as DOW. Extracting the age, birth month, and day of week might be appropriate depending on the problem you're trying to solve. Does age impact the target variable? What about which day of the week they were born on? Don't worry if this sounds complicated. You'll learn more about feature engineering later in this course. After your data is cleaned and you've identified the features you want to use, it's time to train a model. You won't use all the data to train your model. In fact, you need to hold some of the data so you have some data to test with. Typically, you'll use about 80% of the data to train with, and you'll save the rest of the data for testing. Next, you'll train a model with training data. In the diagram, the model uses the XGBoost algorithm. The model itself has some parameters you can set. These parameters will alter how the algorithm works and they're known as hyperparameters. The output of the training job will be a trained model. With the trained model, you can use some of the test data to see how well the model performs. You'll take an instance the model hasn't seen and use it to perform a prediction. Because you already know the target in your test data, you can compare the two values. From these comparisons, you can calculate metrics, which give you data on how well the model is performing. You'll then make changes to the model's data, features or hyperparameters, until you find the model that yields the best results. When training your model, there's a real danger of overfitting or underfitting the model. Your model is overfitting your training data when you see the model performs well on the training data but doesn't perform well on the evaluation data. This is because the model is memorizing the data it saw and can't generalize to unseen examples. Your model is underfitting the training data when the model performs poorly on the training data. This is because the model can't capture the relationship between the input examples, which are often called X, and the target values, which are often called Y. Understanding model fit is important for understanding the root cause of poor model accuracy. This understanding will guide you to take corrective steps. You can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction error on the training data and the evaluation data. We'll show you steps you can take to avoid this later in this course. After you've retrained the model and you're satisfied with the results, you deploy your model to deliver the best possible predictions. Later in this course, we'll walk you through these different phases and give you hands-on experience with each of them. But knowing the process is also useful when using the managed services we'll also explore later, where certain Amazon ML services will do the bulk of the work for you. Here are some of the key takeaways for this section. First, we looked at how the machine learning pipeline process can guide you through the process of training and evaluating a model. The iterative process can be broken into three broad steps, data processing, model training, and model evaluation. That's it for this video. We'll see you in the next one.\n",
      "Processing video 6/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod02_Sect04.mp4\n",
      "Mod02_Sect04.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod02_Sect04.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod02_Sect04.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod02_Sect04.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod02_Sect04.mp4\n",
      "Text:  Welcome back. In this section, we'll look at some of the tools you'll be using throughout the rest of this course. Before we start, this list isn't an exhaustive list of all the tools available today. We're only going to cover them at a high level, but it's a good place to get started. First, there's the Jupyter Notebook. The Jupyter Notebook is an open-source web application you can use to create and share documents that contain live code, equations, visualizations, and narrative text. Uses include data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more. JupyterLab is a web-based interactive development environment for Jupyter notebooks, code, and data. JupyterLab is flexible. You can use it to configure and arrange the user interface to support a wide range of workflows in data science, scientific computing, and machine learning. JupyterLab is extensible and modular. You can write plugins that add new components and integrate with existing ones. Later in this course, you'll use Amazon SageMaker, which hosts both Jupyter Notebooks and JupyterLab. Pandas is an open-source Python library. It's used for data handling and analysis. Pandas represents data in a table similar to a spreadsheet. This table is known as a Pandas data frame. Matplotlib is a Python library for creating scientific, static, animated, and interactive visualizations in Python. You'll use it to generate plots of your data later in this course. Seaborn is another data visualization library for Python that's built on Matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphs. NumPy is one of the fundamental scientific computing packages in Python. It contains functions for n-dimensional array objects. It also has useful math functions such as linear algebra, Fourier transform, and random number capabilities. Scikit-learn is an open-source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities. Scikit-learn is built on NumPy, SciPy, and Matplotlib. It's a good tool for exploring machine learning. Although you'll only use it to borrow a few functions in this course, you might want to consider exploring it after you complete this course. Moving up from individual libraries and packages, there are also tools that contain production-ready frameworks. We already mentioned Scikit-learn, which is a good library for machine learning. The frameworks supported on AWS, such as TensorFlow and Keras, also include libraries you can use for machine learning. All the frameworks listed here are supported on AWS and can be used from Amazon SageMaker. AWS also provides compute instances that are tuned for machine learning in both the cloud and at the edge. Compute instances can be optimized for learning and inference. Another AWS resource you can use are certain Amazon Machine Images or AMIs. We offer prepackaged AMIs that contain many of the popular frameworks. Finally, there's Amazon SageMaker, which is an AWS service with many capabilities. First, SageMaker can deploy machine learning instances running Jupyter Notebooks and Jupyter Lab. It manages the deployment of these compute resources, so you only need to connect to the Jupyter environment. SageMaker also provides tools for labeling data, training models, and hosting trained models. AWS Marketplace also provides a selection of ready-to-use model packages and algorithms from third-party machine learning developers. AWS also provides a set of managed machine learning services and you can integrate them into your applications even if you don't have substantial machine learning experience. For computer vision, Amazon Recognition provides object and facial recognition for both image and video. Also, Amazon Textract can extract text from images. Speech services include Amazon Polly, which can speak text. Another speech service is Amazon Transcribe, which converts spoken audio to text. For language, Amazon Comprehend uses NLP to find insights and relationships in text. Also, Amazon Translate can translate text into different languages. If you want to work with chatbots, Amazon Lex helps you build interactive conversational applications that use voice or text. For forecasting, Amazon Forecast uses machine learning to combine time series data with additional variables so you can build forecasts. And finally, if you'd like to work with recommendations, Amazon Personalize can help you create individual personalized recommendations for customers. These managed services have already been trained in many aspects of the problem domain. You only need to provide your specific data to get started. We're going to look at many of these managed services in the second half of this course, after you learn how to do things on your own. The key takeaways for this section include these points. First, Python is the most popular language for performing machine learning tasks. Jupyter notebooks provide you with a web-based, hosted development environment for machine learning. You'll use Jupyter Notebooks frequently in machine learning. There are a large number of open-source tools, such as Pendus, that you'll use often as a machine learning practitioner. Finally, depending upon your requirements, you might start with low-level frameworks to create your own solution. You might also use tools such as Amazon SageMaker to help with some of the heavy lifting. Or you could simply use and adapt one of the managed Amazon ML services for your specific problem domain. That's it for this video. We'll see you in the next one.\n",
      "Processing video 7/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod02_Sect05.mp4\n",
      "Mod02_Sect05.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod02_Sect05.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod02_Sect05.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod02_Sect05.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod02_Sect05.mp4\n",
      "Text:  Hi, welcome back. This is section 5 and we're going to discuss challenges with machine learning. You'll come across many challenges in machine learning. There are a lot of poor quality and inconsistent data available. A significant portion of your job will be getting access to or generating enough good data that's representative of the problem you want to solve. A key issue to watch out for is under or overfitting the model. It's not all about the data, although it mostly is. Do you have data science experience? Is staffing a team of data scientists cost-effective? Does management support using machine learning? What does the business landscape look like? Are the problems too complex to formulate into a machine learning problem? Can the resulting model be explained to the business? If it can't be explained, it might not get adopted. What's the cost of building, updating, and operating a machine learning solution? Finally, how does the technology map? Does the business unit have access to the data that's needed? Can the data be secured to meet any regulatory requirements? What tools and frameworks will be used? How will the solution integrate with other systems? These are important questions. To be successful, you'll need to be able to answer and address them. Many machine learning problems can be solved today by using existing models and without substantial machine learning knowledge. We've already talked about the AWS managed services for machine learning. You can add sophisticated machine learning capabilities to your applications with only some basic developer skills for calling APIs. There are other pre-built models you can use or adapt. One example is YOLO, which means you only look once. YOLO is a popular computer vision model. In addition to these scenarios, you can use the AWS marketplace if you'd prefer to buy models and services from independent software vendors instead of developing your own. Here are the key takeaways for this section. First, you'll face many machine learning challenges. The biggest ones that you can directly influence are related to data. You should consider managed services to solve machine learning problems within the domains they support, such as using Amazon Recognition for computer vision problems. That's it for this section. We'll see you in the next video.\n",
      "Processing video 8/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod02_WrapUp.mp4\n",
      "Mod02_WrapUp.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod02_WrapUp.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod02_WrapUp.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod02_WrapUp.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod02_WrapUp.mp4\n",
      "Text:  It's now time to review the module. Here are the main takeaways for this module. First, we looked at defining machine learning and how it fits into the broader AI landscape. We also looked at the types of problems machine learning can help us solve and how machine learning applies learning algorithms to develop models from large datasets. We then looked at the machine learning pipeline and the different stages for developing a machine learning application. Finally, we introduced some of the tools and services you can use, before discussing some of the challenges with machine learning. In summary, in this module you learned how to recognize how machine learning. In summary, in this module, you learned how to recognize how machine learning and deep learning are part of artificial intelligence, describe artificial intelligence and machine learning terminology, identify how machine learning can be used to solve a business problem, describe the machine learning process, list the tools available to data scientists. Identify when to use machine learning instead of traditional software development methods. Thanks for watching, we'll see you in the next video.\n",
      "Processing video 9/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod03_Intro.mp4\n",
      "Mod03_Intro.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod03_Intro.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod03_Intro.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod03_Intro.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod03_Intro.mp4\n",
      "Text:  Welcome back to AWS Academy Machine Learning. This is module three, and we're going to work through the entire machine learning pipeline by using Amazon SageMaker. This module will discuss a typical process for handling a machine learning problem. The machine learning pipeline can be applied to many machine learning problem. The machine learning pipeline can be applied to many machine learning problems. The focus is on supervised learning, but the process you learn in this module can be adapted to other types of machine learning as well. This is a large module and we'll be covering a lot of material. At the end of this module, you'll be able to formulate a problem from a business request, obtain and secure data for machine learning, build a Jupyter notebook by using Amazon SageMaker, outline the process for evaluating data, explain why data needs to be preprocessed, use open-source tools to examine and pre-process data. Use Amazon SageMaker to train and host a machine learning model. Use cross-validation to test the performance of a machine learning model. Use a hosted model for inference. And finally, create an Amazon SageMaker hyperparameter tuning job to optimize a model's effectiveness. We're ready to get started. See you in the next video.\n",
      "Processing video 10/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect01.mp4\n",
      "Mod03_Sect01.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod03_Sect01.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod03_Sect01.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod03_Sect01.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect01.mp4\n",
      "Text:  Hi, and welcome back to module 3. This is section 1, and we're going to take a look at some of the data sets we'll use in this module. We'll also look at guidance for how to formulate a business problem. Before we get started, here's a reminder of the machine learning pipeline we looked at in the previous module, and how that maps to the sections in this module. This section, Section 1, will cover how to formulate a problem. It will also cover the datasets we'll use throughout this module. Section 2 will discuss how to obtain and secure data for your machine learning activities. In Section 3, we'll show you tools and techniques for gaining an understanding of your data. Then in Section 4, we'll look at pre-processing your data so it's ready to train a model. Section 5 will cover selecting and training an appropriate machine learning model. Section 6 will show you how to deploy a model so you can make a prediction. Section 7 will examine the process of evaluating the performance of a machine learning model. And finally, in Section 8 we'll look at tuning the model. The machine learning pipeline is an iterative process. When you work on a real-world problem, you might find yourself iterating many times before you arrive at a solution that meets your business's needs. In this first section, we'll examine how to think about turning a business requirement into a machine learning problem. The first step in this phase is to simply define the problem you want to solve and the goal you want to reach. Understanding the business goal is key because you'll use it to measure the performance of your solution. It's not unusual to solidify the business problem before you can begin targeting a solution. There are a lot of other questions you could ask to develop a good understanding of the problem. With more information about the problem, you can begin framing an approach. First, can the problem even be solved by machine learning? Or would a traditional approach make more sense? Is this a supervised or unsupervised machine learning problem? Do you have labeled data to train a supervised model? There are many questions you could ask yourself and the business. Ultimately, you should try to validate the use of machine learning, and you should make sure you have access to the right people and data. You should also try to come up with the simplest solution to the problem. Here's an example. You want to identify fraudulent credit card transactions so you can stop the transaction before it processes. That's your problem. Now what's the business goal or outcome driving this problem statement? In this case, say that the intended outcome is a reduction in the number of customers who end their membership to the credit card as a result of a fraudulent transaction. From a business perspective, how do you define success given this problem and the desired outcome? This is the stage where you need to move from qualitative statements to quantitative statements that can be easily measured. Continuing with the example, a metric you could use to define success for this problem might be a 10% reduction in the number of customers who file claims for fraudulent transactions within a six-month period. So now you've defined the business side of your problem. It's time to start thinking about this in terms of your machine learning model itself. What's the actual output you want to see from your model? You want to be specific here. It should be a statement that reflects what an ML model could actually output. An example might be, the model will output whether or not a credit card transaction is fraudulent or not fraudulent. Now that you know what you want your ML model to actually achieve, you can use this information to determine the type of ML you're working with. You can use this information to determine the type of ML you're working with. If you have historical data where customers filed reports for fraud transactions, you can use this data for your machine learning purpose. This historical data falls under the supervised learning approach, where the labels are already defined. Recall from earlier in this course that supervised ML types are categorized into two groups, classification and regression. In the credit card example, the desired output of categorizing a transaction is fraud or not fraud. So you can see that you're dealing with a binary classification problem. Throughout this module, you'll see several datasets being used. You can access these datasets and many more from the UC Irvine Machine Learning Repository. The first dataset contains numerical information about the composition of wine along with the quality of the wine. The question you might want to ask on this dataset is, based on the composition of the wine, could we predict the quality and therefore the price? In addition to that question, we'll also use this dataset to view statistics, deal with outliers, and scale numerical data. The second dataset is a car evaluation database. This dataset is heavily text-based. This enables you to explore the encoding categorical data, which converts the text values into numbers that can be processed by machine learning. The third dataset is a biomedical dataset, which you'll also use in the labs. The question to answer for this dataset is, based on the biomechanical features, can you predict if a patient has an abnormality? This dataset will take you through the entire end-to-end process. You'll end with a trained model that's been tuned and that you can use to make a prediction. In this section, we looked at how business problems need to be converted into an ML problem. We also looked at some of the key questions to ask, which are, what is defining success? Can you measure the outcome or impact if your solution is implemented? Most business problems fall into one of two categories. The first category is classification, which can be binary or multi-class. Ask yourself, does the target belong to a class? And the second category is regression. For this, ask yourself, can you predict a numerical value? That's it for section one. We'll see you in the next video.\n",
      "Processing video 11/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect02_part1.mp4\n",
      "Mod03_Sect02_part1.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod03_Sect02_part1.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod03_Sect02_part1.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---                                                   \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/logging/__init__.py\", line 1100, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/logging/__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/logging/__init__.py\", line 678, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/logging/__init__.py\", line 368, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 542, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 531, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 775, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_9325/1957697782.py\", line 78, in <module>\n",
      "    process_video(video_index, output_directory)\n",
      "  File \"/tmp/ipykernel_9325/1957697782.py\", line 27, in process_video\n",
      "    result = pipelin(f\"audio_output/{audio_file_name}\")\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/pipelines/automatic_speech_recognition.py\", line 285, in __call__\n",
      "    return super().__call__(inputs, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1167, in __call__\n",
      "    logger.warning_once(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/utils/logging.py\", line 329, in warning_once\n",
      "    self.warning(*args, **kwargs)\n",
      "Message: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\n",
      "Arguments: (<class 'UserWarning'>,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod03_Sect02_part1.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect02_part1.mp4\n",
      "Text:  Hi, welcome back. We're now going to look at a few ways you can collect and secure data. In this section, we'll explore some of the techniques and challenges associated with collecting and securing the data that's needed for machine learning. Consider again the original example about predicting credit card fraud. You've further formulated the problem. But what data do you need to actually train your model so you can get the desired output and subsequently achieve your intended business outcome? Do you have access to the data? If so, how much data do you have and where is it? What solution can you use to bring all this data into one centralized repository? The answers to these questions are essential at this stage. The good news for a budding data scientist is that there are many places where you can obtain data. Private data from you or your existing customer already exists, including everything from log files to customer invoice databases. Private data can be useful depending on the problem you're trying to solve. In many cases, private data is found in many different systems. We'll look at how to bring these sources together shortly. Sometimes, you want to use data that was collected and made available by a commercial organization. you want to use data that was collected and made available by a commercial organization. Companies such as Reuters, Change Healthcare, Dun & Bradstreet, and Foursquare maintain databases you can subscribe to. They include curated news stories, anonymized healthcare transactions, global business records, and location data. If you supplement your own data with commercial data, you can get useful insights you wouldn't have gotten otherwise. There are also many open source data sets, ranging from wine quality to movie reviews. These data sets are made available for use in research or for teaching purposes. AWS, Kaggle, and the UCI machine learning repositories are good places to find open source data sets. Government and health organizations are other sources of data that could be useful. Supervised machine learning problems need a lot of data. These are also called observations, and you already need to know the target answer or prediction for that data. These are also called observations, and you already need to know the target answer or prediction for that data. This kind of data, where you already know the target answer or prediction, is called labeled data. Each observation in your data is made up of two elements, the target and the features. The target is the answer you want to predict. So in the credit card transaction example, the target of any given observation is either fraud or not fraud. A feature is an attribute of the example that you can use to identify patterns for predicting the target answer. A feature in the credit card example could be the date of the transaction, the vendor, or the amount in dollars of the transaction. You might wonder if the source of the target is fraud or not fraud. Typically, this information is discovered only after the transaction is complete and the actual card owner notices a fraudulent transaction on their statement. This information would be recorded with the transaction for exactly the purpose of using it to train a future model. So given what you know about the elements of an ML dataset, we'll return to one of the original questions. What data do you need to actually train your model to reach the desired output and subsequently your intended business outcome? This is an example of a stage in the ML pipeline when it's crucial to get domain expertise to help you answer this question. With domain knowledge, you can start determining the features and target data your model will need to make accurate predictions. Your data should be representative of the data you'll have when you're using the model to make a prediction. For example, if you want to predict credit card fraud, you need to collect data for positive or fraudulent transactions. You also need to collect data for negative or non-fraudulent transactions. You need both types of data so the machine learning algorithm can find patterns that will distinguish between the two types. Suppose your average amount of fraudulent transactions is actually 3%, but your training dataset only includes a very small fraction of fraudulent observations, say 0.4%. In this case, it'll be difficult for your model to truly learn patterns related to fraudulent transactions that it might encounter in production. There are many different services in AWS where you could find or store your data. Here are some key services you might use. Amazon Simple Storage Service is also known as Amazon S3. It provides object-level storage. With S3, you can store as much data as you want in the form of objects, which you can think of as files. They could be CSV files or files of other formats you need. S3 can be accessed through the web-based AWS Management Console. You can also access S3 programmatically through the API and SDKs or with third-party solutions, which also use the API and SDKs or with third-party solutions, which also use the API and SDKs. If your training data is already in S3 and you're planning to run training jobs several times with different algorithms and parameters, you could use Amazon FSx for Lustre. It's a file system service that speeds up your training jobs by serving your S3 data to Amazon SageMaker at high speeds. The first time you run a training job, FSx for Lustre automatically copies data from S3 and makes it available to SageMaker. You can use the same Amazon FSx file system for subsequent iterations of training jobs, which prevents repeated downloads of common S3 objects. Alternatively, your training data might already be in Amazon Elastic File System or Amazon EFS. If so, we recommend using EFS as your data source for training data. It can launch your training jobs directly from the service without needing data movement, which results in faster training start times. This is often the case in environments where data scientists have home directories in Amazon EFS. They can quickly iterate on their models by bringing in new data, sharing data with colleagues, and experimenting with different fields or labels in their data set. For example, a data scientist can use a Jupyter notebook to do an initial cleansing on a training set and launch a training job from Amazon SageMaker. They could then use their Jupyter notebook to drop a column and relaunch the training job and finally compare the resulting models to see which one works better. There are many other AWS services and resources where you might find data. For example, you could use Amazon Relational Database Service or Amazon RDS, a managed relational database service. You could also use Amazon Redshift, which is a managed data warehouse service. Another option is Amazon Timestream, a managed time series database designed specifically to handle large amounts of data from the Internet of Things, or IoT. You could even spin up your own instances on Amazon Elastic Compute Cloud, which is also known as Amazon EC2, and host your own database on these instances. When you have data sources, you'll need to extract useful data from these sources when assembling your data for machine learning. We'll look at this next. That's it for part one of this section. We'll see you again for part two where we'll review how to extract, transform, and load data.\n",
      "Processing video 12/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect02_part2.mp4\n",
      "Mod03_Sect02_part2.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod03_Sect02_part2.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod03_Sect02_part2.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod03_Sect02_part2.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect02_part2.mp4\n",
      "Text:  Hi, welcome back. We'll continue exploring data collection by reviewing how to extract, transform, and load data. Data is typically spread across many different systems and data providers. This presents a challenge. You'll need to bring all these data sources together into something that can be consumed by a machine learning model. You can do this through extract, transform, and load, which is also known as ETL. The steps in ETL are defined this way. In the extract step, you pull the data from the sources to a single location. During extraction, you might need to modify the data, combine matching records, or do other tasks that transform the data. Finally, in the load step, the data is loaded into a repository such as Amazon S3. A typical ETL framework has several components. As an example, consider the diagram. First, the Crawler A program connects to a data store, which can be a source or a target. It progresses through a ranked list of classifiers to determine the schema for your data. Then, it creates metadata tables in the AWS Glue Data Catalog. A job defines the business logic that's needed to perform ETL work. To run the job, you'll need to use a schedule or event. As a final note, the services we just discussed exist in the transform partition of the ETL process. AWS Glue is a fully managed ETL service that makes it simple and cost-effective to categorize your data, clean it, enrich it, and move it reliably between various data stores. AWS Glue consists of a central metadata repository known as the AWS Glue Data Catalog. This is an ETL engine that automatically generates Python or Scala code. It also provides a flexible scheduler that handles dependency resolution, job monitoring, and retries. AWS Glue is serverless, so you don't need to set up or manage any infrastructure. You can use the AWS Glue console to discover data, transform it, and make it available for search and queries. The console calls the underlying services to orchestrate the work needed to transform your data. You can also use the AWS Glue API operations to interface with the AWS Glue services. This way, you can edit, debug, and test your Python or Scala Apache Spark ETL code using a familiar development environment. AWS Glue is well-suited to machine learning because it can receive labeled data that can be used for training. Here's an example. Say that you provide AWS Glue with training data that teaches the model what duplicate records in the data source look like. Then, AWS Glue can identify the duplicates and present them for further analysis by a data engineer. AWS Glue enables the orchestration of complex ETL jobs. In the example, AWS Glue crawls the data sources and presents the information to clients as a data catalog. AWS Glue can run your ETL jobs based on an event, such as getting a new data set. For example, you can use an AWS Lambda function to trigger your ETL jobs to run as soon as new data becomes available in Amazon S3. You can also register this new data set in the AWS Glue Data Catalog as part of your ETL jobs. Although managed tools are available in AWS to manipulate data, a data scientist will also write scripts in their Jupyter notebook to handle data. A very simple extract and load script is shown here. The Imports and Variables section imports the libraries that are used. Note that Bodo3 is the library for AWS. Variables are also set here for the zip files web location and a local folder for extraction. The download and extract section makes a web request, saving the bytes from the URL as a stream. This stream is passed to the zip file function, which is then used to extract the data. With the extracted files in a folder, the upload to S3 section enumerates the folder's files and uploads each file to Amazon S3. If you discover that this script is used often, it should be migrated to a standalone function that can be imported by other Python applications. That's it for part two of this section. We'll see you again for part three where we'll review how to secure your data.\n",
      "Processing video 13/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect02_part3.mp4\n",
      "Mod03_Sect02_part3.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod03_Sect02_part3.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod03_Sect02_part3.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod03_Sect02_part3.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect02_part3.mp4\n",
      "Text:  Hi, welcome back. We'll continue exploring data collection by reviewing how to secure your data. It's important to consider the security of your data. Though the data sets used in this course are all public, real data about customer transactions or health records need to be kept secure. You can use AWS Identity and Access Management, which is also known as IAM. It's a service that controls access to resources. Make sure you're securing your data within AWS correctly so you can avoid data breaches. The diagram shows a simple IAM policy that allows only read access to a specific S3 bucket for the listed role. In addition to controlling access to data, you need to make sure your data is secure. It's a good practice and it might also be legally required for certain data types, such as financial data or healthcare records. AWS provides encryption features for storage services, typically for data that's at rest or in transit. You can often meet these encryption requirements by enabling encryption on the object or service you want to protect. For data in transit, you must use secure transports like Secure Sockets Layer, Transport Layer Security, or SSL TLS. Another aspect to consider is compliance audits. When dealing with data from regulated industries, you'll often need to audit access to the data. AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your entire AWS infrastructure. CloudTrail provides an event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. This event history simplifies security analysis, resource change tracking, and troubleshooting. You can also use CloudTrail to detect unusual activity in your AWS accounts. All these features can help you simplify operational analysis and troubleshooting. Here are the key takeaways for this section. We looked at the first step in solving machine learning problems, obtaining the data required to train your machine learning model. We also reviewed how ETL can be used to obtain data from multiple sources. Services like AWS Glue can make it used to obtain data from multiple sources. Services like AWS Glue can make it easy to obtain data from multiple data stores. Finally, make sure you understand your security requirements. These are based on both business need and any regulatory requirements. Also make sure your data is secure. Only authorized users should be able to access your data and it should be encrypted where possible. That's it for Section 2, we'll see you in the next video.\n",
      "Processing video 14/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect03_part1.mp4\n",
      "Mod03_Sect03_part1.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod03_Sect03_part1.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod03_Sect03_part1.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod03_Sect03_part1.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect03_part1.mp4\n",
      "Text:  Hi and welcome back. This is section 3 and we're going to cover how to evaluate your data. In this section, we'll look at different data formats and types. We'll also look at how you can visualize and analyze the data before feature engineering. Before you can start running statistics on your data to better understand what you're working with, you need to ensure it's in the right format for analysis. For Amazon SageMaker, algorithms support training with data in CSV format. Many of the tools you'll use to explore, visualize, and analyze the data can also read it in CSV format. Generally speaking, you'll need to have at least some domain knowledge for the problem you're trying to solve with machine learning. For example, if you're developing a model to predict if a set of symptoms indicates a disease, you'd need to know the relationship between the symptoms and the disease. Data typically needs to be in numeric form, so machine learning algorithms can use the data to make predictions. We'll look at ways you can convert text data in the next section. For now, we'll just explore the data and try to gain some insights into the overall data set. One popular open source Python library is Pandas. It can take data in various formats, reformat it, and load it into a tabular representation of your data, presenting it in rows and columns. Some of the formats that Pandas can reformat and load include CSV, Excel, Pickle, and JavaScript Object Notation, or JSON. pandas also has data analysis and manipulation features and we'll use them throughout this module loading data can be as simple as the example which pulls in the CSV file from the specified URL when you load data into pandas it's stored as a pandas dataframe. In the pandas documentation, a dataframe is described as a general 2D labeled, size-mutable tabular structure with potentially heterogeneously typed column. A more helpful way to think of a dataframe is to think of it as a spreadsheet or a SQL table. Like a table or spreadsheet, a data frame will have rows which are also known as instances, and it will have columns which are also known as attributes. The shape property of a data frame describes the number of rows and columns it has. Each column in a data frame is a series. A series is a one-dimensional labeled array. A series can store data of any type. To learn more about data structures in Pandas, see the Pandas documentation. Along with data, you can load a data frame with row labels and column labels. The row labels are known as an index, and the column labels are known as columns. If you loaded your data from a CSV file with a header row, the columns will be created from the first line of the file. You can change that behavior, however. If you don't have column names in the source file, you can pass them as a parameter. When performing data analysis, it's important to make sure you're using the correct data types. In many cases, Pandas will correctly infer the correct data types when it loads data, and you can move on. If you have domain knowledge or access to a domain expert, they can often identify data type issues. You can use either D types or the info function to obtain information on the column types as shown in the example. If you don't have the correct data types, you need to figure out why this is the case. Often a numeric column could have been missing data or it could be a single text value. For example, in the car dataset, the number of doors can be 2, 3, 4, or 5 more. After you've analyzed the data, you can convert a column to the correct data type using Pandas. That's it for part 1 of this section. We'll see you again for part 2, where we'll review how to describe your data.\n",
      "Processing video 15/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect03_part2.mp4\n",
      "Mod03_Sect03_part2.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod03_Sect03_part2.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod03_Sect03_part2.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod03_Sect03_part2.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect03_part2.mp4\n",
      "Text:  Hi, welcome back. We'll continue exploring how to describe your data. Now that your data is in a readable format, you can perform descriptive statistics on the data to better understand it. Descriptive statistics help you gain valuable insights into your data so that you can effectively pre-process the data and prepare it for your ML model. We'll look at how you can do that and discuss why it's so important. First, descriptive statistics can be organized into a few different categories. Overall statistics include the number of rows and the number of columns in your dataset. This information, which relates to the dimensions of your data, is very important. For example, it can indicate that you have too many features, which can lead to high dimensionality and poor model performance. Attribute statistics are another type of descriptive statistic, specifically for numeric attributes. They're used to get a better sense of the shape of your attributes. This includes properties like the mean, standard deviation, variance, and minimum and maximum values. If you need to look at relationships between more than one variable, you can consider multivariate statistics. They mostly relate to the correlations and relationships between your attributes. For cases when you have multiple variables or features, you might want to look at the correlations between them. It's important to identify correlations between attributes because a high correlation between two attributes can sometimes lead to poor model performance. When features are closely correlated and they're all used in the same model to predict the response variable, there could be problems. For example, the model loss might not converge to a minimum state. So be aware of highly correlated features in your dataset. Mean and median are two different measures describing the extent that your data is clustered around some value or position. Mean can be a useful method for understanding your data when the data is symmetrical. However, if your data is skewed or contains outliers, then median tends to provide the better metric for understanding your data as it relates to central tenancy. For instance, if you have outliers with large values, the mean can be skewed one way, and it wouldn't serve as an accurate representation of where your values are truly centered. Median isn't affected by outliers in the same way. We'll talk more about outliers soon. Statistics are available, and they can be viewed on numerical data by using methods such as DESCRIBE. There are also other methods to calculate the mean, median, and others. You can also view statistics on single or multiple columns. You can even group data by specific values. For categorical attributes, you can look at the frequency of attribute values in your dataset. That information will give you some idea about what is inside that categorical variable. The diagram here shows the car dataset, which is made up of several categorical values, buying, maint, lug boot, safety, and class. Safety can be either low, medium, or high. From the describe function, you can see that there are three unique values, with low being the most frequent. values, with low being the most frequent. Looking at the class column, it appears that the top value of the four is UNACC, which stands for unaccounted. This accounts for 1,210 of the 1,728 values, or 70%. This might suggest an imbalance. For a target variable that's also of a categorical type, you can look at the class distribution to see whether there's a class imbalance in your dataset. Imbalanced data can mark a disproportionate ratio for your classes. For instance, your dataset is made up of credit card transactions, but only a tenth of a percent is labeled as fraud. In this case, your algorithm might not learn well enough to predict examples of credit card fraud. Visualization could help you gain insights into your data that you might not be aware of otherwise. A histogram is often a good visualization technique for seeing the overall behavior of a particular feature. With a histogram, you can answer questions like, is the feature data normally distributed? How many peaks are there in the data? Is there any skewness for that particular feature? When using histograms for your data visualization, values are binned. The taller peaks of the histogram indicate the most common values. For numerical features, you can use density plots and box plots, in addition to histograms, to get an idea of what's inside that particular feature. Like a histogram, these visualizations will help you answer questions like, what's the range of the data? The peak of the data? Are there any outliers? Are there any special features? Answering these questions helps you understand your data better and can also help you decide if you need to do more specialized data preprocessing. A box plot is a method for graphically depicting groups of numerical data through their quartiles. When you have more than two numerical variables in a feature dataset, you might want to look at their relationship. A scatter plot is a good way to identify any special relationships among those variables. In this case, the left diagram has sulfates and alcohol. They are two numerical variables. In this case, the left diagram has sulfates and alcohol. They are two numerical variables. Suppose you want to show the relationship between these variables. You can use a scatter plot to help you visualize that. There are plots scattered around, and the correlation among them might not be that high because the data is scattered. However, you might find some relatively positive relationships between the two variables. Scatter plot matrices help you look at the relationship between multiple different features. In Pendis, you can easily create scatter plot matrices based on the columns you want to look at. This example has three columns, and it will give the pairwise scatter plot for any two columns. With a scatter plot, you might want to identify special regions that a particular subset of data could fit into. In the example, is there a relationship between alcohol sulfates and quality? You could plot those values against good and poor quality wines like the example. Plotting gives you an idea of how useful particular variables can be if you're using them for a classification problem. That's it for part 2 of this section. We'll see you again for part 3 where we'll review correlations and the takeaways for this section.\n",
      "Processing video 16/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect03_part3.mp4\n",
      "Mod03_Sect03_part3.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod03_Sect03_part3.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod03_Sect03_part3.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod03_Sect03_part3.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect03_part3.mp4\n",
      "Text:  Hi, welcome back. Now we'll review how to find correlations in your dataset. How can you quantify the linear relationship among the variables you're seeing in a scatterplot? A correlation matrix is a good tool in this situation. It conveys both the strong and weak linear relationships among numerical variables. Correlation can go as high as 1 or as low as minus 1. When the correlation is 1, this means those two numerical features are perfectly correlated with each other. It's like saying y is proportional to x. When the correlation of those two variables is minus one, it's like saying y is proportional to minus x. Any linear relationship in between can be quantified by the correlation. So if the correlation is zero, this means there's no linear relationship. But it doesn't mean that there's no relationship, it's just an indication that there's no linear relationship between those two variables. However, looking at a number isn't always straightforward. Often it's easier to view the numbers when they're represented by colors. Now we'll look at the heat map. The highest number, 1, in dark green, and minus 1 is in dark brown. The color gives you both the positive and negative directions, and it also shows how strong the correlations are. We can use the seaborne heat map function to show the correlation matrix. Looking at the chart, there's some correlation between citric acid and fixed acidity. That would be expected in wine because citric acid contributes to the acidity of the wine. However, there isn't much correlation between fixed acidity and pH. pH is a measurement of the strength of those acids present, but fixed acidity is a measure of the quantity. In this particular dataset, there doesn't appear to be a correlation here. Some key takeaways from this section of the module include these points. The first step is to get your data into a format that can be used easily. PENDIS is a popular Python library for working with data. Descriptive statistics will help you gain insights into the data. You can use visualizations to examine the dataset in more detail. That's it for this section, we'll see you again in the next video.\n",
      "Processing video 17/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect04_part1.mp4\n",
      "Mod03_Sect04_part1.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod03_Sect04_part1.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod03_Sect04_part1.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod03_Sect04_part1.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect04_part1.mp4\n",
      "Text:  Hi, and welcome to section 4. In this section, we're going to look at feature engineering. Feature engineering is one of the most impactful things you can do to improve your machine learning model. We'll now look at what it is. There are two things that can help make your models more successful. The first is feature selection and the second is feature extraction or the process of creating features. In feature selection you select the most relevant features and discard the rest. You can apply feature selection to prevent redundancy or irrelevance in the existing features. You can also use it to limit the number of features to help prevent overfitting. Feature extraction builds valuable information from raw data by reformatting, combining, and transforming primary features into new ones. This process continues until it yields a new data set that can be consumed by the model to achieve your goals. As the diagram shows, feature extraction covers a range of activities, from dealing with missing data to converting text data into numerical data. Although the list isn't exhaustive, it should give you some idea of the data handling that's needed to get data into a useful state. Many of the tasks are no different than any other job working with data. You'll want to make sure data is in the correct format, that it's consistently represented, correctly spelled, among other tasks. For example, you might combine data or extract data into multiple columns. Or you could also remove columns altogether. Specific to machine learning, you'll need to convert text columns to numerical values. You'll also need to decide how to handle outliers and potentially rescale your data. Next, we'll look at some of the more common tasks in this section. Most machine learning algorithms work best with numerical data. You'll need to make sure that all columns in your dataset contain numeric data by converting or encoding it. You might need to make several passes through the data sheet before you can encode it. For example, you might have variability in the text values, such as rows that contain both medium and MED as values. If the categorical data has order to it, you'll want to encode the text into numerical values that capture this ordinal relationship. Say you have data showing maintenance costs. You might encode low to 1, medium to 2, high to 3, and very high to 4. to 4. After you've made sure your categorical data is all uniform, you can use tools like Skykit Learn and Pandas to encode your data. If the categorical data doesn't have any order to it, then you'll need to break the data into multiple columns. This will help make sure you don't introduce an ordinal relationship to the data that isn't there. For example, suppose you assigned a value of 1 to the first color, such as red, and you then assigned 2 to the next value, say blue. The model could interpret blue as being more important than red because blue has a higher numeric value. Encoding non-ordinal data into multiple columns or features is a better way. Think of the new features like a checkbox. Consider the example. There are three features that were generated. The value 1 indicates that the instance has that feature, like its color. That's it for this section. We'll see you again in the next video.\n",
      "Processing video 18/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect04_part2.mp4\n",
      "Mod03_Sect04_part2.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod03_Sect04_part2.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod03_Sect04_part2.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod03_Sect04_part2.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect04_part2.mp4\n",
      "Text:  Hi, welcome back. We'll continue exploring feature engineering by reviewing how to clean your dataset. In addition to converting string data to numerical data, you'll need to clean your dataset for several other potential problem areas. Before encoding the string data, make sure the strings are all consistent. You'll also need to make sure variables use a consistent scale. For example, if one variable describes the number of doors in a car, the scale will probably be between 2 and 8. But if another variable describes the number of cars of a particular type sold in the state of California, the scale will type sold in the state of California, the scale will probably be in the thousands. Some data items might also capture more than one variable in a single value. For instance, suppose the dataset includes variables that combine safety and maintenance into a single variable, such as safe high maintenance. single variable, such as safe high maintenance. You'll need to train your machine learning system for both variables and also split that single variable into two separate variables. You might also encounter data sets that are missing data for some variables, and some data sets will include outliers. We'll cover techniques for dealing with these situations in this section. You might find that data is missing. For example, some columns in your data set could be missing data because of a data collection error, or maybe data wasn't collected on a particular feature until the data collection process was underway. Missing data can make it difficult to accurately interpret the relationship between the related feature and the target variable. So regardless of how the data ended up being missed, it's important for you to deal with this issue. Unfortunately, most machine learning algorithms can't handle missing values automatically. You'll need to use human intelligence to update missing values with data that's meaningful and relevant to the problem. Most Python libraries for data manipulation include functions for finding missing data. So how do you decide if you should drop or impute missing values. This question is answered in part by better understanding how those values came to be missing in the first place and how much data the missing values represent within your larger data set. For instance, say the missing values are randomly spread throughout your data set and don't represent a larger portion of its respective row or column. In this case, imputation is most likely the better option. In contrast, say that you have a column or row that has a large percentage of missing values. In this case, dropping the entire row or column would be preferred over imputation. If you decide to drop rows with missing data, you can use built-in functions to do this. For example, Panda's dropNa function can drop all rows with missing data, or you can drop specific data values by using a subset. As an alternative to dropping missing values, you can impute values for those missing values. There are different ways to impute a missing value. For categorical values, the missing value is usually replaced with the mean, the median, or the most frequent values. For numerical or continuous variables, the missing value is usually replaced with the mean or the median. You can impute a single row of missing data, which is known as univariate. You can also do this for multiple rows, which is known as multivariate. We'll now look at a univariate example. Here the scikit-learn imputer function is being used to impute some missing values. It's a fairly small dataset, but there are two missing values. The missing value was imputed by the strategy of the mean. To do this, you first calculate the mean. Here it's the mean of 3 and 2, which is 2.5. Then you'll impute the mean value for the missing value. Some data libraries include an impute package that provides more complex ways to impute data. Examples include k-nearest-neighbor, soft impute, multiple imputation by chain equations, and others. That's it for part 2 of this section. We'll see you again for part 3 where we'll review how to work with outliers in your data.\n",
      "Processing video 19/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect04_part3.mp4\n",
      "Mod03_Sect04_part3.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod03_Sect04_part3.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod03_Sect04_part3.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod03_Sect04_part3.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect04_part3.mp4\n",
      "Text:  Hi, welcome back. We'll continue exploring feature engineering by describing how to work with outliers. You might also need to clean your data based on any outliers that exist. Outliers are points in your dataset that lie at an abnormal distance from other values. They're not always something you want to clean up because they can add richness to your dataset. But they can also make it harder to make accurate predictions because they skew values away from the other, more normal values related to that feature. An outlier might also indicate that the data point actually belongs to another column. You can think of outliers as falling into two broad categories. might also indicate that the data point actually belongs to another column. You can think of outliers as falling into two broad categories. The first is a single variation for just a single variable, or a univariate outlier. The second is a variation of two or more variables, or a multivariate outlier. One of the more common ways to find univariate outliers is with a box plot. A box plot shows how far a data point is to the mean for that variable. The box in the plot shows the data values within two quartiles of the mean. Values outside that range are represented by the lines extending from the box, which are sometimes called whiskers. A scatter plot can be an effective way to see multivariate outliers. For example, this diagram shows the amount of sulfates and alcohol in a collection of wines. With the scatter plot, you can quickly visualize whether there are multivariate outliers for the two variables. The origin of your outlier will most likely inform how you deal with it during this pre-processing phase of the pipeline, or possibly later during feature engineering. There are several different approaches to dealing with outliers. You could delete the outlier if your outlier is based on an artificial error. This means the outlier isn't natural and was introduced because of some failure like incorrectly entered data. You could also transform the outlier by taking the natural log of a value. This, in turn, reduces the variation caused by the extreme outlier value, which would then reduce the outlier's influence on the overall dataset. Finally, you could use the mean of the feature and impute that value to replace the outlier value. Again, this would be a good approach if the outlier was caused by artificial error. This isn't an exhaustive list, but it describes the most common options. After you've extracted features, you'll need to select the most appropriate features for training your model. There are three main feature selection methods. Filter methods use statistical methods to measure the relevance of features by their correlation with the target variable. Wrapper methods measure how useful a subset of a feature is. They do this by training a model on the feature and then measuring how successful the model is. Filters are faster and cheaper than wrapper methods because they don't involve training the models repeatedly. Wrappers typically find the best subset of features, but there's a risk of overfitting compared to using subsets of features from filter methods. Embedded methods are algorithm-specific, and they might use a combination of both filters and wrappers. Filter methods use a proxy measure instead of the actual model's performance. They're fast to compute, but they can still capture how useful the feature set is. Here are some common measures. The first is Pearson's correlation coefficient, which measures the statistical relationship or association between two continuous variables. The second is linear discriminant analysis, or LDA. This is used to find a linear combination of features that separates two or more classes. The third is analysis of variance, or ANOVA. This is used to analyze the differences among group means in a sample. And finally, chi-square is a single number that tells you how much difference exists between your observed counts and the counts you'd expect if there were absolutely no relationships in the population. Filters are usually less computationally intensive than wrappers, but they produce a feature set that isn't tuned to a specific type of predictive model. This lack of tuning means a feature set from a filter is more general than one from a wrapper. The filter also usually has a lower prediction performance than a wrapper. lower prediction performance than a wrapper. However, the filter's feature set doesn't contain the assumptions of a prediction model, so it's more useful for exposing relationships between features. Many filters provide feature ranking instead of an explicit best feature subset, and the cutoff point in the ranking is chosen through cross-validation. Filters have also been used as a pre-processing step for wrappers, which enables a wrapper to be used on larger problems. Wrapper methods use a predictive model to score feature subsets. Each new subset is used to train a model, which is then tested on a holdout set. The score for that subset is calculated by counting the number of mistakes made on that holdout set, The score for that subset is calculated by counting the number of mistakes made on that holdout set, or the error rate of the model. Because wrappers train a new model for each subset, they're computationally intensive. However, they usually provide the best performing feature set for that particular type of model or problem. Forward selection starts with no features and adds them until the best model is found. Backward selection starts with all features, drops them one at a time, and then selects the best model. Embedded methods combine the qualities of filter and wrapper methods. They're implemented by algorithms that have their own built-in feature selection methods. Some of the most popular examples of these methods are lasso and ridge regression. They have built-in penalization functions to reduce overfitting. Here are some key takeaways from this section of the module. First, feature engineering involves selecting the best features for machine learning. Pre-processing gives you better data to work with, and better data typically provides better results. Two categories for preprocessing are converting data to numerical values and cleaning up dirty data by removing missing data and cleaning outliers. Finally, how you handle dirty data impacts your model. That's it for section 4, we'll see you in the next video!\n",
      "Processing video 20/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect05.mp4\n",
      "Mod03_Sect05.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod03_Sect05.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod03_Sect05.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod03_Sect05.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect05.mp4\n",
      "Text:  Hi, welcome back to module 3. This is section 5 on training. In this section, we're going to look at how to select a model and train it with the data we have preprocessed. At this point, you've done a lot to clean and prepare your data, but that doesn't mean your data is completely ready to train the algorithm. Some algorithms may not be able to work with training data in a data frame format. Some file formats, like CSV, are commonly used by various algorithms, but they do not make use of that optimization that some of the file formats, like RecordIO Protobuf, can use. Many Amazon SageMaker algorithms support training with data in a CSV format. Amazon SageMaker requires that a CSV file doesn't have a header record and that the target variable is in the first column. Most Amazon SageMaker algorithms work best when you use the optimized protobuf record I-O format for the training data. Using this format allows you to take advantage of pipe mode when training the algorithms that support it. In pipe mode, your training job streams data directly from Amazon S3. When using the CSV format, the target variable in your training dataset should be the first column on the left and your features should be to the right of the target variable column. Evaluating a model with the same data that it trained on will lead to overfitting. Recall overfitting is where your model learns the particulars of a dataset too well. It's essentially memorizing the training data, rather than learning the relationships between features and labels. This means the model isn't learning from those relationships and patterns to apply them to new data in the future. Holdout is when you split your data into multiple sets, commonly sets for training data, validation data, and testing data. Training data, which includes both features and labels, feeds into the algorithm you've selected to produce your model. You then use the model to make predictions over the validation dataset, which is where you'll likely notice things you'll want to tweak, and tune, and change. Then, when you're ready, you run the test dataset, which only includes features since you want the labels to actually be predicted. The performance you get here with the test dataset is what you can reasonably expect to see in production. A common split when using the holdout method is using 80% of the data for a training set, 10% for validation, and 10% for test. Or if you have a lot of data, you can split it into 70% training, 15% validation, and 15% test. So for a small dataset, we can use k-fold cross-validation to utilize as much of the data as possible, while still having relatively good metrics, in order to choose which model is better. K-fold cross-validation randomly partitions the data into K different segments. For each segment, we'll use the rest of the data outside of it for training in order to do a validation on that particular segment. Let's look at an example. Here we have a 5-fold cross-validation. The available training data is separated into five different chunks. For the training of the first model, we are using all those chunks as the training data, and then we are going to calculate the metrics on this test piece. For the second model, we are going to use these pieces as training. After the model is trained, you apply it to this test piece. We do the same thing five times. We use all the training data and we test it on five different models on different chunks of the test data, eventually testing it on all data points. One other thing to note about splitting your data, data in a specific order can lead to biases on your model. This is especially true if you're working with structured data. For example, the wine data is ordered by the quality column. When you run your model against your test data, this ordered pattern will be applied, biasing the model. It might also mean that some targets are missing from the training data. Typically, randomizing your data set prior to splitting is sufficient, and many libraries will provide functions for this. With smaller sets, it is sometimes useful to use stratified sampling. Stratified sampling ensures that the training and test sets have approximately the same percentage of samples of each target class as the complete set. An internet search will give you many ways to shuffle and split the data. One of the easiest is to use the train test split function from sklearn. Amazon SageMaker provides four different ways you can train models. The built-in algorithms available can be easily deployed from the AWS console, CLI, or a Jupyter notebook. Containers are used behind the scenes when you use one of the Amazon SageMaker built-in algorithms, but you do not have to deal with them directly. Amazon SageMaker supported frameworks provide pre-built containers to support deep learning frameworks such as Apache MXNet, TensorFlow, PyTorch, and Chainer. It also supports machine learning libraries such as Skykit Learn and SparkML by providing pre-built Docker images. If you use the Amazon SageMaker Python SDK, they are deployed using their respective Amazon SageMaker SDK estimator class. If there is no pre-built Amazon SageMaker container image that you can use or modify for an advanced scenario, you can package your own script or algorithm to use with Amazon SageMaker. You can use any programming language or framework to develop your container. For an example, if your team works and builds ML models in R, you can build your own containers to train and host an algorithm in R as well. Someone else may have already developed and tuned a model. It is worth looking in the AWS marketplace to find available models. Amazon SageMaker provides high-performance, scalable machine learning algorithms optimized for speed, scale, and accuracy. algorithms optimized for speed, scale, and accuracy. For supervised learning, Amazon SageMaker includes XGBoost and Linear Learner algorithms for classification and quantitative problems. There is also a factorization machine to address recommendation and time series prediction problems. Amazon SageMaker includes support for unsupervised learning, such as with K-means clustering, and principal component analysis, PCA, to solve problems like identifying customer groupings based on purchasing behavior. Finally, there are a selection of specialized algorithms for processing images and other deep learning tasks. Let's look a little closer at three of the most commonly used built-in algorithms and their use cases. XGBoost, or Extreme Gradient Boosting, is a popular and efficient open-source implementation of the Gradient Boosted Trees algorithm. Gradient boosting is a supervised learning algorithm that attempts to accurately predict a target variable by combining an ensemble of estimates from a set of simpler, weaker models. XGBoost has done remarkably well in machine learning competitions because it robustly handles a variety of data types, relationships, and distributions. The large number of hyperparameters can be tweaked and tuned for improved fit. This flexibility makes XGBoost a solid choice for problems in regression, classification and ranking. The Amazon SageMaker Linear Learner algorithm provides a solution for both classification and regression problems. With the Amazon SageMaker algorithm, you can simultaneously explore different training objectives and choose the best solution from your validation set. You can also explore a large number of models and choose the best one for your needs. Compared with methods that provide a solution for only continuous objectives, the Amazon SageMaker Linear Learner algorithm provides a significant increase in speed over naive hyperparameter optimization techniques. K-Means is an unsupervised learning algorithm. It attempts to find discrete groupings within data where members of a group are as similar as possible to one another and as different as possible from members of other groups. You define the attributes that you want the algorithm to use to determine similarity. To train a model in Amazon SageMaker, you create a training job. The training job includes the URL of the Amazon S3 bucket where you stored the training data, the URL of the S3 bucket where you want to store the output of the job. The Amazon Elastic Container Registry path where the training code is stored. The compute resources that you want Amazon SageMaker to use for model training. Compute resources are ML compute instances managed by Amazon SageMaker. Amazon SageMaker provides a selection of instance types optimized to fit different machine learning use cases. Instance types comprise varying combinations of CPU, GPU, memory, and networking capacity, and give you the flexibility to choose the appropriate mix of resources for building, training, and deploying your ML models. Each instance type includes one or more instance sizes, allowing you to scale your resources to the requirements of your target workload. Some key takeaways from this section of the module include split data into training and testing sets helps you validate the model's accuracy. K-fold cross-validation can help with smaller data sets. Two key algorithms for supervised learning are XGBoost and LinearLearner. Use K-means for unsupervised learning. And use Amazon SageMaker to train models. That's it for Section 5, I hope to see you in the next video.\n",
      "Processing video 21/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect06.mp4\n",
      "Mod03_Sect06.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod03_Sect06.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod03_Sect06.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod03_Sect06.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect06.mp4\n",
      "Text:  Hi and welcome back. This is section 6 and we're going to look at hosting and using the model. In this section, we'll look at how you can deploy your trained model so it can be consumed by applications. After you've trained, tuned, and tested your model you'll learn more about testing in the next section, you're now ready to deploy your model. If you're thinking that we're looking at the phases out of order, here's why we're discussing deployment now. If you want to test your model and get performance metrics from it, you first need to make an inference or prediction from the model, and this typically requires deployment. Deployment for testing is different from production, although the mechanics are the same. Amazon SageMaker provides everything you need to host your model for simple testing and evaluation, from a few requests to deployments handling tens of thousands of requests. There are two ways you can deploy your model. For single predictions, you can deploy your model with Amazon SageMaker hosting services. SageMaker will deploy multiple compute instances, which run your model behind a load-balanced endpoint. Applications can call the API at the endpoint to make predictions. With this model, you can scale the number of instances up or down based on demand. To get predictions for an entire dataset, use Amazon SageMaker Batch Transform. Instead of deploying and maintaining a permanent endpoint, SageMaker will spin up your model and perform the predictions for the entire dataset you provide. It will then store the results in Amazon S3 before it shuts down and terminates the compute instances. It's useful for performing batch predictions when you test the model. You can quickly run your entire validation set against the model without writing any code to process and collate the individual results. The goal of the deployment phase is to provide a managed environment to host models for providing inference securely and with low latency. After your model is deployed into production, you should monitor your production data and retrain your model if necessary. Newly deployed models need to reflect the current production data. New data is accumulated over time and it could potentially identify alternative or new outcomes. And so deploying a model is not a one-time exercise. Instead, it's a continuous process. With one click, you can deploy your model on Amazon ML instances that can automatically scale across multiple availability zones for higher redundancy. Just specify the type of instance and the maximum and minimum number of instances desired. SageMaker will take care of the rest. It will launch the instances, deploy your model, and set up the secure HTTPS endpoint for your application. secure HTTPS endpoint for your application. Your application only needs to include an API call to this endpoint to achieve inference with low latency and high throughput. With this architecture, you can integrate your new models into your application in minutes because changes to the model no longer need changes to the application code. SageMaker manages your production compute infrastructure on your behalf. It can perform health checks, apply security patches, and conduct other routine maintenance, all with built-in Amazon CloudWatch monitoring and logging. After you've trained the model, you can create the endpoint either in code or by using the SageMaker console. If you're planning to host only a single model, you can create an endpoint for that model. But if you're planning to host multiple models, you need to create a multi-model endpoint. Multi-model endpoints provide a scalable and cost-effective solution for deploying large numbers of models. They use a shared serving container that's enabled to host multiple models. This reduces hosting costs by improving endpoint utilization compared to using single model endpoints. It also reduces deployment overhead because SageMaker manages loading models in memory and scaling the models based on the traffic patterns to them. When you deploy machine learning models into production to make predictions on new data, you need to make sure you apply the same data processing steps that were used in training to each inference request. Otherwise, you can get incorrect prediction results. By using inference pipelines, you can reuse the data processing steps from model training during inference without maintaining two separate copies of the same code. This helps ensure the accuracy of your predictions and reduces development overhead. Because SageMaker is a managed service, inference pipelines are completely managed. When you deploy the pipeline model, the service installs and runs the sequence of containers on each EC2 instance in the endpoint or each batch transform job. Additionally, the sequence of feature processing and inference runs with low latency because the containers are collated on the same EC2 instances. Some key takeaways from this section of the module include these points. You can deploy your trained model by using SageMaker to handle API calls from applications or to perform predictions using a batch transformation. The goal of your model is to generate predictions to answer the business problem. Be sure that your model can generate good results before you deploy to production. Finally, use multi-model endpoint support to save resources when you have multiple models to deploy. That's it for this section. We'll see you in the next video.\n",
      "Processing video 22/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect07_part1.mp4\n",
      "Mod03_Sect07_part1.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod03_Sect07_part1.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod03_Sect07_part1.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod03_Sect07_part1.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect07_part1.mp4\n",
      "Text:  Hi, welcome back to module 3. In this section, we'll look at how you can evaluate your model's success in predicting results. At this point, you've trained your models. It's now time to evaluate that model to determine if it will do a good job predicting the target on new and future data. Because future instances have unknown target values, you need to assess how the model will perform on data where you already know the target answer. You'll then use this assessment as a proxy for performance on future data. This is the reason why you hold out a sample of your data for evaluating or testing. An important part of this phase involves choosing the most appropriate metric for your business situation. Think back to the earlier section on problem formulation. During that phase, you define your business problem and outcome, and then you craft a business metric to evaluate success. The model metric you choose at this phase should be linked to that business metric as much as possible. There's often a high correlation between the two metrics. In addition to considering your business problem and success metric, the type of ML problem you're working with will influence the model metric you choose. Throughout the rest of this module, we'll look at examples of common metrics used in classification problems. We'll also look at common metrics used in regression problems. We're going to start by considering a simple binary classification problem. Here's a specific example. Imagine that you have a simple image recognition model that's labeling data as either cat or not cat. After the model's been trained, you can use the test dataset you held back to perform predictions. To help examine the performance of the model, you can compare the predicted values with the actual values. If you plot the values into a table like the example, you can start getting some insights into how well the model performed. In a confusion matrix, you can get a high-level comparison of how the predicted class is matched up against the actual classes. If the actual label or class is cat, which is identified as P for positive, and the predicted label or class is also cat, then you have a true positive. This is a good outcome for your model. Similarly, if you have an actual label of not cat, which is identified as N for negative, and the predicted label or class is also not cat, then you have a true negative. This is also a good outcome for your model. In both these cases, your model predicted the correct outcome when it used the testing data. There are two other possible outcomes, and both aren't considered good outcomes. The first one is when the actual class is negative, so you got not cat, but the predicted class is positive, or cat. This is called a false positive because the prediction is positive but incorrect. Finally, there are false negatives. These happen when the actual class is positive, so you got cat, but the predicted class is negative, or not cat. That's it for part one of this section. We'll see you again for part two where we'll review calculating classification metrics.\n",
      "Processing video 23/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect07_part2.mp4\n",
      "Mod03_Sect07_part2.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod03_Sect07_part2.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod03_Sect07_part2.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod03_Sect07_part2.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect07_part2.mp4\n",
      "Text:  Hi, welcome back. We'll continue exploring how to evaluate your model. The diagram shows the confusion matrix of how two different models performed on the same data. Can you tell which one's better? Which is better isn't a good question to ask. What do you mean by better? Does better mean making sure you find all the cats? Even if it means you'll get many false positives? Or does better mean making sure the model is the most accurate? It's difficult to see just by looking at the two charts. What if you're trying several models, using multiple folds, and have hundreds of data points to compare? To do that, you'll need to calculate more metrics. The first metric is sensitivity. This is sometimes referred to as recall, hit rate, or true positive rate. Sensitivity is the percentage of positive identifications. In the cat example, it represents what percentage of cats were correctly identified. To calculate sensitivity, take the number of true positives, or the number of positive identifications of cats, and divide that by the total number of actual cats. In this example, 60% of cats that were cats were correctly identified as cats. Specificity is sometimes referred to as selectivity or true negative rate. Specificity is the percentage of negatives correctly identified. In the cat example, this is the number of images that were not cats that were correctly identified as not cats. To calculate specificity, take the number of true negatives and divide that by the total number of actual negatives. So for the example, that's the number of not cats that were correctly identified divided by the total number of actual not cats. This means that in the example 64% of not-cats were identified as not-cats. Now that you have these metrics for each model, knowing what your business goal is makes it easier to decide which model to use. Which model would you choose if you wanted to make sure you'll identify as many cats as possible? Model B would be a good answer, if you're not concerned about having many false positives, that is. If you're not concerned about having incorrectly identified not-cats. Which model would you choose if you wanted to make sure you identified animals that were not cats? Model A might work for this scenario. Again, it would depend on how many false negatives you can tolerate. If this was a classification of patients who had heart disease or not, which model would be best? This is where it gets interesting. A fun website might get a bad reputation if it can't identify cats correctly, but if you're trying to diagnose patients, your focus will probably be very different. It's important to understand the trade-offs you're making when you decide which model to use. There are also other metrics that can help you make your decisions. That's it for part 2 of this section. We'll see you again for part 3 where we'll start looking at thresholds.\n",
      "Processing video 24/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect07_part3.mp4\n",
      "Mod03_Sect07_part3.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod03_Sect07_part3.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod03_Sect07_part3.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod03_Sect07_part3.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect07_part3.mp4\n",
      "Text:  Hi, welcome back. We'll continue exploring how to evaluate your model. Classification models are going to return a probability for the target. This is a value of the input belonging to the target class, and it will be between 0 and 1. To convert the value to a class, you need to determine the threshold to use. You might think it's 50%, but you could change it to be lower or higher to improve your results. As you've seen with sensitivity and specificity, there's a trade-off between correctly and incorrectly identifying classes. Changing the threshold can impact that outcome. We're going to take a look at how you can visualize this. A receiver operating characteristic graph is also known as an ROC graph. It summarizes all the confusion matrices that each threshold produced. To build one, you calculate and plot the sensitivity, or true positive rate, against the false positive rate on a graph for each threshold value. You can calculate the false positive rate by subtracting the specificity from 1. After you plot those points, you can draw a line between them. The dotted black line from 0, 0 to 1, 1 means that the sensitivity or true positive rate is equal to the false positive rate. The point at 1, 1 means that you've correctly identified all the cats, but you've also incorrectly identified all the not-cats. This is bad. Any point on this line means that the proportion of correctly classified samples is the same as the proportion of incorrectly classified samples. The point at represents that there are zero true positives and zero false positives. A model that has high sensitivity and low false positive rate is usually the goal, so it's considered to be better when the line between the threshold recordings is closer towards the top left corner. If you had the data from two models, you could plot out the ROC curve for each model and compare them. However, that can be tedious. There's another graph you can use for this which we'll look at next. Another evaluation metric you can use is the area under the curve receiver operator curve, which is also known as an AUC ROC. The AUC part is the area under the plotted line. When the AUC is higher, it means the model will be better at predicting cats as cats and not cats as not cats. You can use the AUC to quickly compare models with each other. With the four numbers from our confusion matrix, you can calculate the model's accuracy. This is also known as its score. You can do this by adding up the correct predictions and then dividing that number by the total number of predictions. Though accuracy is a widely used metric for classification problems, it has limitations. This metric isn't as effective when there are a lot of true negative cases in your dataset. Think about the cat-not-cat example. If most of your accuracy is based on true negatives, it says that your model is good at predicting what isn't a cat. In this case, you might not feel confident in your model's ability to predict cats after you roll it out into production. This leads to an example of why it's important to make sure that the metric you choose for model evaluation aligns to your business goal. Think about the credit card fraud example. In this case, using accuracy as your main metric probably isn't a good idea because you have a lot of true negatives. Your high true negative number might hide the fact that your model's ability to identify cases of fraudâ€”that is, to identify true positivesâ€”isn't ideal. As a credit card company, it's probably unacceptable to have less than almost perfect performance identifying fraud cases. That would drive customers away, which would be the opposite of what you'd want to achieve from a business standpoint. This is why two other metrics are often used in these situations. The first one is precision, which essentially removes the negative predictions. Precision is the proportion of positive predictions that are actually correct. You can calculate it by taking the true positive and dividing it by true positive plus false positive. When the cost of false positives is high in your particular business situation, precision might be a good metric. Think about a classification model that identifies email messages as spam or not. In this case, you don't want your model to label an email message as spam and thus prevent your users from seeing that message when it's actually legitimate. Or consider an example of a model that needs to predict whether a patient has a terminal illness. In this case, using precision as your evaluation metric doesn't account for false negatives in your model. Here, for the model to be successful, it's crucial that it doesn't falsely predict the absence of illness in a patient who actually has that illness. Sensitivity would be a better metric to use for this situation. But it doesn't always need to be one or the other. The F1 score combines precision and sensitivity together. It gives you one number that quantifies the overall performance of a particular ML algorithm. You should consider using an F1 score when you have a class imbalance but want to preserve the equality between precision and sensitivity. But what do you do if you're dealing with a regression problem? In that case, there are other common metrics you can use to evaluate your model, including the mean squared error. The mean squared error is frequently used. Its general purpose is the same as what you saw with classification metrics. You determine the prediction from the model, and you compare the difference between the prediction and the actual outcome. More specifically, you take the difference between the prediction and actual value, square that difference, and then sum up all the squared differences for all the observations. In Skykit Learn, you can use the mean squared error function directly from the metrics library. There are other metrics you can use for linear models, such as R squared. So you've trained your model, performed a batch transformation on your test data, and calculated your metrics. Now what will you do? You'll use these metrics to help you tune the model. You could select a different set of features and train the model again. After you retrain the model, ask yourself, which was the better model? The metrics will help inform you. You could also use different data and retrain the model with the same features. Remember k-fold cross-validation from earlier in this module? Finally, you could tune the parameters of the model itself, which is the subject of the next section. Here are key takeaways from this section of the module. To evaluate the model, you need to have data that the model hasn't seen. This could be either a holdout set or you could use k-fold cross-validation. Different machine learning models use different metrics. Classification can use the confusion matrix and the AUCROC that you can generate from it. Regression can use mean squared. That's it for section 7. See you in the next video.\n",
      "Processing video 25/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect08.mp4\n",
      "Mod03_Sect08.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod03_Sect08.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod03_Sect08.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod03_Sect08.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod03_Sect08.mp4\n",
      "Text:  Hi, and welcome back to module 3. This is section 8. In this section, we're going to take a look at how you can tune the model's hyperparameters to improve model performance. Recall from an earlier module that hyperparameters can be thought of as the knobs that tune the machine learning algorithm to improve its performance. Now that we're looking more explicitly at tuning models, it's time to look more specifically at the different types of hyperparameters and how to perform hyperparameter optimization. There are a couple of different categories of hyperparameters. The first kind are model hyperparameters. The first kind are model hyperparameters. They help define the model itself. As an example, consider a neural network for a computer vision problem. For this case, additional attributes of the architecture need to be defined, like filter size, pooling, and the stride or padding. The second kind are optimizer hyperparameters. They relate to how the model learns patterns based on data, and they're used for a neural network model. These types of hyperparameters include optimizers like gradient descent and stochastic gradient descent. They can also include optimizers that use momentum, like atom, or that initialize the parameter weights with methods like Xavier initialization or He initialization. The third kind are data hyperparameters. They relate to the attributes of the data itself. These include attributes that define different data augmentation techniques, like cropping or resizing for image-related problems. They're often used when you don't have enough data or enough variation in your data. Tuning hyperparameters can be very labor-intensive. used when you don't have enough data or enough variation in your data. Tuning hyperparameters can be very labor intensive. Traditionally this was done manually by someone who had domain experience related to the hyperparameter and the use case. This person would manually select the hyperparameters based on their intuition and experience. Then they would train the model and score it on the validation data. This process would be repeated over and over again until they achieved satisfactory results. This manual process isn't always the most thorough and efficient way of tuning hyperparameters. With SageMaker, you can perform automated hyperparameter tuning with Amazon SageMaker Automatic Model Tuning. It finds the best version of a model by running multiple training jobs on your dataset by using the algorithm and hyperparameter ranges you specify. It then chooses the hyperparameter values that results in a model that performs the best as measured by a metric you choose. It uses Gaussian process regression to predict which hyperparameter values might be most effective at improving fit. It also uses Bayesian optimization to balance exploring the hyperparameter space and exploiting specific hyperparameter values when appropriate. And importantly, automatic model tuning can be used with built-in algorithms from SageMaker, pre-built deep learning frameworks, and bring your own algorithm containers. Suppose that you want to solve a binary classification problem on a fraud data set. Your goal is to maximize the area under the AUC curve metric of the algorithm by training a linear learner algorithm model. You don't know which values of the learning rate, beta 1, beta 2, and epochs you should use to train the best model. To find the best values for these hyperparameters, you can specify ranges of values that SageMaker hyperparameter tuning will then search. It will find the combination of values that results in the training job that performs the best, as measured by the objective metric that you chose. In the example, SageMaker Hyperparameter Tuning launches training jobs that use hyperparameter values in the ranges you specified, and then returns the training job with the highest AUC. Hyperparameter tuning might not necessarily improve your model. It's an advanced tool for building machine solutions. As such, it should be considered part of the scientific method process. When you build complex machine learning systems, like deep learning neural networks, exploring all possible combinations is impractical. To improve optimization, use the following guidelines when you create hyperparameters. First, instead of using all hyperparameters, limit the number of hyperparameters to the ones you think would give you good results. The range of values for the hyperparameters you choose to search can significantly affect the success of hyperparameter optimization. Although you might want to specify a large range that covers every possible value for a hyperparameter, you'll get better results by limiting your search to a small range of values. If you get the best metric values within a part of a range, consider limiting the range to only that part. During hyperparameter tuning, SageMaker attempts to figure out if your hyperparameters are log-scaled or linear-scaled. Initially, it assumes that hyperparameters are linear-scaled. If they should be log-scaled, it might take time for SageMaker to discover that on its own. If you know that a hyperparameter should be log scaled and you can convert it yourself, doing so can improve hyperparameter optimization. Running more hyperparameter tuning jobs concurrently gets more work done quickly, but a tuning job improves only through successive rounds of experiments. Typically, running one training job at a time achieves the best results with the least amount of compute time. Say that you have a distributed training job that runs on multiple instances. In this case, hyperparameter tuning uses the last reported objective metric from all instances of that training job as the value of the objective metric for that training job. Design distributed training jobs so that they report the objective metric you want. Now that you've gone through the end-to-end process of training and tuning a machine learning model, it's worth talking about Amazon SageMaker Autopilot. This service can help you find a good model with little effort or input on your part. With Autopilot, you create a job that supplies the test, training, and target. Autopilot will analyze the data, select appropriate features, and then train and tune the models. It will document the metrics and find the best model based on the provided data. The results include the winning model and metrics and a Jupyter notebook you can use to investigate the results. Although using Autopilot doesn't remove your need to pre-process the data, it can save you time during feature selection and model tuning. Some key takeaways from this section of the module include these points. First, model tuning is important for finding the best solution to your business problem. Hyperparameters can be tuned for the model, optimizer, and data. SageMaker can perform automatic hyperparameter tuning. And finally, overall model development can be accelerated by using autopilot. That's it for this video, see you in the next one.\n",
      "Processing video 26/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod03_WrapUp.mp4\n",
      "Mod03_WrapUp.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod03_WrapUp.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod03_WrapUp.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod03_WrapUp.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod03_WrapUp.mp4\n",
      "Text:  It's now time to review the module and wrap up with a knowledge check. In this module, you learned how to formulate a problem from a business request, obtain and secure data for machine learning, build a Jupyter notebook by using Amazon SageMaker. Outline the process for evaluating data. Explain why data needs to be pre-processed. Use open source tools to examine and pre-process data. Use Amazon SageMaker to train and host a machine learning model. Use cross-validation to test the performance of an ML model, use a hosted model for inference, and create an Amazon SageMaker hyperparameter tuning job to optimize a model's effectiveness. That concludes this module. Thanks for watching. We'll see you again in the next video.\n",
      "Processing video 27/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod04_Intro.mp4\n",
      "Mod04_Intro.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod04_Intro.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod04_Intro.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod04_Intro.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod04_Intro.mp4\n",
      "Text:  Hi, and welcome to Module 4 of AWS Academy Machine Learning. In this module, we're going to look at forecasting. We'll start with an introduction to forecasting and look at how time series data is different from other kinds of data. Then, we're going to look at Amazon Forecast, a service that helps you simplify building forecasts. At the end of this module, you'll be able to describe the business problem solved with Amazon Forecast, describe the challenges of working with time series data, list the steps required to create a forecast by using Amazon Forecast. And use Amazon Forecast to make a prediction. See you in the next video!\n",
      "Processing video 28/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod04_Sect01.mp4\n",
      "Mod04_Sect01.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod04_Sect01.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod04_Sect01.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod04_Sect01.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod04_Sect01.mp4\n",
      "Text:  Hi, and welcome to Section 1. We'll get started by reviewing what forecasting is and some use cases for it. Forecasting is an important area of machine learning. It's important because there are so many opportunities for predicting future outcomes based on historical data. Many of these opportunities involve a time component. However, while the time component adds additional information, it also makes time series problems more difficult to handle compared to other types of predictions. You can think of time series data as falling into two broad categories. The first type is univariate data, which means there's just one variable. The second one is multivariate data, which means there's more than one variable. There are several common patterns in time series data. The first pattern is a trend. With a trend, you get a pattern with the values increasing, decreasing, or staying the same over time. There are seasonal patterns. These reflect times of the year, month, day, or other patterns. Cyclical patterns are similar to seasonal patterns. These are patterns that repeat, like a large retail sale event that happens the same time each year. Finally, there are changes in the data over time that appear to be random or that have no discernible pattern. There are many uses for forecasting. You can use forecasting in marketing applications, such as for sales forecasting or demand projections. It could also be used in inventory management systems that anticipate required inventory levels. Forecasting energy consumption can help predict when and where energy is needed. And weather forecasting systems can be used for governments and commercial applications, such as agriculture. That's it for this section. See you in the next video.\n",
      "Processing video 29/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod04_Sect02_part1.mp4\n",
      "Mod04_Sect02_part1.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod04_Sect02_part1.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod04_Sect02_part1.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod04_Sect02_part1.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod04_Sect02_part1.mp4\n",
      "Text:  Hi and welcome back. This is section 2 and we're going to focus on processing time series data because it can be different from other types of data you've been using so far. Time series data is data that is captured in chronological sequence over a defined period of time. Introducing time into a machine learning model has a positive impact because the model can derive meaning from changes in the data points over time. Time series data tends to be correlated. This means that there's a dependency between data points. This has mixed results for forecasting. This is because you're dealing with a regression problem, and regression assumes that data points are independent. You need to develop a method for dealing with data dependence so you can increase the validity of the predictions. In addition to the time series data, you can add related data to augment a forecasting model. For example, suppose you want to make a prediction about retail sales. You could include information about the product being sold, such as item identification or sales price, along with the number of units sold per time period. The third type of data is metadata about the dataset. For instance, say that you have a retail dataset. You might want to include metadata, like a brand name or a genre for music or videos, so you can group results. It's better to have more data. When you work with multiple data sources, you'll face the challenge of handling the timestamp of the data. You'll observe differences in the timestamp format and other challenges such as incomplete data. However, you might be able to infer missing data in some cases. For example, say you have some data that contains both the month and the day, but no year. Observe whether the data seems to sequence through the month numbers in the database, repeating after 12. If it does so, you could add the year if you knew when the data started. You could then infer future years based on the order of the data. Much timestamp data is stored in UTC format, but not all data is. You should check if the timestamp is in local or universal time. Sometimes the timestamp doesn't represent the time you think it does. For example, suppose you have a database of cars that were serviced at a garage. Does the timestamp indicate the time the car arrived, was completed, or picked up? Or does it indicate when the final entry was entered into the system? Say you're trying to model the hourly caloric intake of patients. However, you only have daily data. Then you'll need to adjust your target timescale. Also, your data might not have any timestamps. There could be other ways to extrapolate a time series depending on the data and domain. For example, you might have wavelength measurements or vectors within an image. As a final note, remember that daylight savings is different around the world. Also because of daylight savings, time might even occur twice a year in their time zones. A common occurrence in real-world forecasting problems is missing values in the raw data. Missing values makes it harder for a model to generate a forecast. The primary example in retail is an out-of-stock situation in demand forecasting. If an item goes out of stock, the sales for the day will zero. If the forecast is generated based on those zero sales values, the forecast will be incorrect. There are many reasons why values can be marked as missing. Missing values can occur because of no transaction. They can also occur because of possible measurement errors. For example, a service that monitored certain data wasn't working correctly. Or as another example, the measurement couldn't happen correctly. In retail, the primary example for an inability to take correct measurements is an out-of-stock situation in demand forecasting. This means that demand doesn't equal sales on that day. There are several ways you can calculate the missing data. The first method is forward fill. This uses the last known value for the missing value. Building on that idea, moving average uses the average of the last known values to calculate the missing value. Backward fill uses the next known value after the missing value. The danger here is that you're using the future to calculate the past, which is bad in forecasting. This method is also known as look-ahead and should be avoided. Interpolation uses an equation to calculate the missing value. You can also use a zero fill. This is often used in retail because missing sales data shouldn't be calculated. The missing data represents that there were no orders on that day. It would be wise to investigate why this happened, but in this case, you don't want to fill in the missing value. You might get data at different frequencies. For example, you might have sales data that includes the exact timestamp the sale was recorded, but have inventory data that only contains the year, month, and day of the inventory level. When you have data that's at a different frequency than other datasets, or data that's not compatible with your question, you might need to downsample. Downsampling is moving from a more finely grained time to a less finely grained time. As the example shows, this could be converting an hourly dataset to a daily dataset. When downsampling, you need to decide how to combine the values. In the previous case of sales data, summing the quantity makes the most sense. If the data is temperature, you might want to find the average. Understanding your data helps you decide what's the best course of action. The opposite of downsampling is upsampling, when you move from a less finely grained time to a more finely grained time. The problem with upsampling is that it's extremely difficult to achieve in most cases. Suppose you want to upsample your sales data from daily sales to hourly sales. Unless you have some other data source to reference, you wouldn't be able to do this. There are cases when you need to do something, perhaps to match the frequency of another time series. Or you might have an irregular time series or specific domain knowledge that would help. In those cases, you need to be careful how you make the conversion. For the retail example, the best you could do is create a single order for the day at a specified hour. For temperature, you could copy the daily temperature into each hourly slot, or use a formula to calculate a curve. In data science, outliers have a mix of positive and negative attributes. The same is true of time series data. Suppose you were examining sales data, and you had an order that has an unusually high number of items. You might not want to include that in your forecast calculations because the order size might never be repeated. Removing these outliers and anomalies is known as smoothing. Smoothing your data can help you deal with outliers and other anomalies. There are a few reasons why you might consider smoothing. First, during data preparation, you remove error values and could also remove outliers. You might also want to smooth your data to generate features. For visualization, you could smooth your data to reduce the noise in a plot. It's important to understand why you are smoothing the data and the impact that it might have. The outcome might be to reduce noise and create a better model. But an equally important question is, could your smoothing compromise the model? Is the model expecting noisy data? Will you also be able to smooth the data in production? That's it for part one of this section. We'll see you again for part two, where we'll review more time series specific challenges and the tools and algorithms that can help us wrangle your data.\n",
      "Processing video 30/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod04_Sect02_part2.mp4\n",
      "Mod04_Sect02_part2.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod04_Sect02_part2.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod04_Sect02_part2.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod04_Sect02_part2.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod04_Sect02_part2.mp4\n",
      "Text:  Hi, welcome back. We'll continue exploring wrangling time series data. Seasonality in data is any kind of repeating observation where the frequency of the observation is stable. For example, in sales, you typically see higher sales at the end of a quarter and into the fourth quarter. Consumer retail sees even higher sales in the fourth quarter. Be aware that data can have multiple types of seasonality in the same data set. There are many times when you should incorporate seasonality information into your forecast. For instance, localized holidays are a good example for sales. The chart shows that the total revenue generated by arcades has a strong correlation with the number of computer science doctorates awarded in the US. But correlations do not mean causation. If you disagree, see the source for the chart. There are many other correlations plotted on the site, and none of them make any sense. With your own data, be careful that you're not seeing and acting on correlations that don't have meaning in the real world. Here's an experiment. If you generate two random time series datasets of numbers between 0 and 1, you'll find that they have a very low correlation. But if you introduce the same slope to both datasets, you'll see a very strong correlation. You need to know how stable a system is. The level of stability, or stationarity, can inform how much you should expect the system's past behavior to inform future behavior. A system with low stability won't be successful at predicting the future. You'll often want to determine the trend for a time series. want to determine the trend for a time series. But if you adjust the series for the trend, it can be difficult to compare it with another series that was also adjusted for the trend. This is because the trends might dominate the values in the series. This could then lead to overestimates in correlation between the two series like we discussed previously. Autocorrelation is one of the special problems you face with time series data. As you've seen in other machine learning problems, the goal of building an ML model is to make sure you're separating the signal from the noise. Autocorrelation is a form of noise because separate observations aren't independent of each other. noise because separate observations aren't independent of each other. A time series with autocorrelation might overstate the accuracy of the model that's produced. Some of the algorithms you'll look at in this module can help correct for autocorrelation. These factors, along with seasonality, will influence the model you'll select to produce your forecast. Some algorithms handle seasonality and autocorrelation, but others do not. Pendus was developed with financial data analysis in mind. As such, it's good at handling time series data. First, you can set the index for your Pendas data frame to be a date time. You can then use date and time to select your data. You can use ranges that contain partial dates. You can also extract date parts such as year, month, weekday name, and more. For grouping and resampling tasks, Pendus has built-in functions to do both. Finally, Pendus can give you insights into auto-correlation. For more information about Pendus and the time series, refer to the Pendus documentation. One of the tasks in building a forecasting application is to choose an appropriate algorithm. Your choice of algorithm should be determined by the type of dataset you're using and the features of that dataset. Amazon Forecast supports these five algorithms, but there are others. Each algorithm can handle data with slightly different characteristics. For example, take autoregressive integrated moving average, which is also known as ARIMA. It removes autocorrelations that could influence the pattern of observations. Or take exponential smoothing, which is also known as ETS. This algorithm is useful for datasets with seasonality. You can find out more about these algorithms in the Amazon Forecast documentation. Some key takeaways from this section of the module include time series data is sequence data that includes a time element which makes it different from regular datasets. Some of the time challenges include dealing with different time formats, handling missing data through downsampling, upsampling, and smoothing, dealing with seasonality such as weekdays and yearly cycles, avoiding bad correlations. Appendix has excellent time series support with functions for dealing with time. Appendix has excellent time series support with functions for dealing with time. There are five algorithms used by Amazon Forecast, ARIMA, DeepAR+, ETS, NPTS, and PROFIT. That's it for this section, we'll see you in the next video.\n",
      "Processing video 31/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod04_Sect02_part3.mp4\n",
      "Mod04_Sect02_part3.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod04_Sect02_part3.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod04_Sect02_part3.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod04_Sect02_part3.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod04_Sect02_part3.mp4\n",
      "Text:  Hi and welcome back. In this section, we'll look at how you can use Amazon Forecast to create a predictor and generate forecasts. When you generate forecasts, you can apply the machine learning development pipeline you've seen throughout this course. But you still need data. You need to import as much data as you have, both historical data and related data. You'll want to do some basic evaluation and feature engineering before you use the data to train a model so you can meet the requirements of Amazon Forecast. To train a predictor, you need to choose an algorithm. If you're not sure which algorithm is the best for your data, Amazon Forecast can choose for you. To do this, select AutoML as your algorithm. You also need to select a domain for your data. If you're not sure what the best fit is, you can also select a custom domain. Domains have specific types of data they require. When you have a trained model, you can then use the model to make a forecast using an input data set group. After you've generated a forecast, you can query the forecast. You can also export it to a bucket in Amazon S3. And finally, you can encrypt the data in the forecast before exporting it. The overall process for working with Amazon Forecast is to import historical and related data. Amazon Forecast inspects the data, identifies key data, and selects an appropriate algorithm. It uses the algorithm to train and optimize a custom model and produce a predictor. You create forecasts by applying the predictor to your dataset. You can then retrieve these forecasts in the AWS Management Console, or you can export the forecasts as comma-delimited files. You can also use an API and AWS CLI commands to create and retrieve forecasts. When you work with Amazon Forecast, you can select the domain you're working in. There are domains ranging from retail to web traffic, and there's also a custom option for everything else. By selecting a domain, you improve the efficiency of the predictor. Each domain has specific types of data that you'll supply when you build the predictor. For example, the retail domain expects data for the item identifiers, a timestamp for the observation, the number of sales for that item, and the specified timestamp. Here's an example of the data you'd need to provide for a retail demand forecast. For the time series, you need the time when the transaction took place, ideally in UTC format, the item ID of the item, and how many items were sold. The metadata for the item might include the category, the item color, and other attributes. The link back to the time series data will be only the item ID, because item metadata typically doesn't change. Related data for creating a more useful forecast could include the sales price or other promotion data. To link this back to the item, you must include the timestamp and the item ID. Here's an example of the data you'd need to provide for a web traffic forecast. For the time series, you need the web page ID, the number of page views per month, and the timestamp. Related data for creating a more useful forecast could include the page category, such as navigation or content category. You'll also need the geographic identifier for the web client. For metadata, you might also need to provide the region and the sales promotion information. Amazon forecast predictors use an algorithm to train a model. They then use the model to make a forecast using an input dataset group. To help you get started, Amazon Forecast provides predefined algorithms, ARIMA, DeepAR+, ETS, NPTS, and PROFIT. You can also use the AutoML feature. It will try all the algorithms to see which one's the best at predicting data. When you prepare data for training and machine learning, you typically hold back data to use when you validate and score the model. The data that you hold back is usually a random sample of your available data. With time series data, you must process your data differently because of a correlation between time. When you import your data, Amazon Forecast breaks it into training and test datasets, which the diagram shows. The training data is used to train the model, which is then tested against the data that was held back. You can specify multiple backtest windows, which will split the data multiple times, train the model, and use metrics to determine which model gives the best results. The default backtest window is 1. You can change how Amazon Forecast splits the data by setting the backtest window offset parameter when you create the predictor. If you don't set this value, the algorithms use default values. After you've trained a model, you will need to measure its accuracy, which you'll learn about next. The first Amazon Forecast evaluation metric is the weighted quantile loss, or W quantile loss. When Amazon Forecast creates a forecast, it provides probabilistic predictions at three distinct quantiles, 10%, 50%, and 90%. These prediction quantiles show you how much uncertainty is associated with each forecast. A P10 quantile predicts that 10% of the time the true value will be less than the predicted value. For example, suppose that you are a retailer. You want to forecast product demand for winter gloves that sell well only during the fall and winter. Say that you don't have sufficient storage space and the cost of invested capital is high, or that the price of being overstocked on winter gloves concerns you. Then you might use the P10 Quantile to order a relatively low number of winter gloves. You know that the P10 forecast overestimates the demand for your winter gloves only 10% of the time, so you'll be sold out of your winter gloves for 90% of the time. A P50 quantile predicts that 50% of the time the true value will be less than the predicted value. Continuing the winter gloves example, say you know that there will be a moderate amount of demand for the gloves and you aren't concerned about being overstocked. Then you might choose to use the P50 quantile to order gloves. A P90 quantile predicts that 90% of the time the true value will be less than the predicted value. Suppose you determine that being understocked on gloves will result in large amounts of lost revenue. For example, the cost of not selling gloves is extremely high or the cost of invested capital is low. In this case, you might choose to use the P90 quantile to order gloves. Amazon Forecast also calculates the associated loss at each quantile. Weighted Quantile Loss calculates how far off the forecast a certain quantile is from actual demand in either direction. Lower W Quantile Loss metrics mean that the model's forecasts are more reliable. The Root Mean Square Error, or RMSE, is another method for evaluating the reliability of your forecasts. Like W quantile loss, RMSE calculates how far off the forecasted values were from the actual test data. The RMSE finds the difference between the actual target value in the dataset and the forecasted value for that time period, and it then squares the differences. The example shows how to calculate RMSE. The RMSE value represents the standard deviation of the prediction errors. This test is good for forecast validity when the errors are mostly of the same size, that is, there aren't many outliers. Lower RMSE metrics indicate that the model's forecasts are more reliable. Here is an example of how a web retailer might use the accuracy metrics to evaluate a forecast. The retailer wants to predict the demand for sales of a particular brand of shoes. They input the sales records for this brand into Amazon Forecast to create a predictor. The predictor provides a forecasted demand of 1,000 pairs with the P10, P50, and P90 values shown. The weighted quantile loss values indicate that 10% of the time there will be fewer than 880 pairs sold, 50% of the time fewer than 1,050 pairs will be sold, and 90% of the time fewer than 1,200 pairs will be sold. The retailer can then use these values to determine which level of inventory to hold. They can base their decision on their assessment of the risk that they won't be able to fulfill orders, or that they'll have excess inventory. Some key takeaways from this section of the module include, you can use Amazon Forecast to train and use a model for time series data. There are specific schemas defined for domains such as retail and EC2 capacity planning, or you can use a custom schema. You need to supply at least the time series data, but can also provide metadata and related data to add more information to the model. As with most supervised machine learning problems, your data is split into training and testing data, but takes into account the time element. Use RMSE and W quantile loss metrics to evaluate the efficiency of the model. That's it for this video, we'll see you in the next one.\n",
      "Processing video 32/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod04_WrapUp.mp4\n",
      "Mod04_WrapUp.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod04_WrapUp.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod04_WrapUp.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod04_WrapUp.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod04_WrapUp.mp4\n",
      "Text:  Hi, welcome back. It's now time to review the module and wrap it up. In this module, you learned how to describe the business problem solved by Amazon Forecast, describe the challenges of working with time series data, list the steps required to create Forecast by using Amazon Forecast, and use Amazon Forecast to make a prediction. Thanks for participating. See you in the next module.\n",
      "Processing video 33/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod05_Intro.mp4\n",
      "Mod05_Intro.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod05_Intro.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod05_Intro.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod05_Intro.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod05_Intro.mp4\n",
      "Text:  Welcome back to AWS Academy Machine Learning. This is Module 5, and we have a great topic for you today, Computer Vision. In this module, we'll start with an overview of the computer vision space, and you'll learn about some of the use cases and terminology. Next, we'll explore details about analyzing image and video with managed services from Amazon Web Services or AWS. Finally, we'll look at how you can use your own customized data sets for performing object detection. At the end of this module, you'll be able to describe the use cases for computer vision, describe the Amazon Managed Machine Learning services available for image and video analysis, list the steps required to prepare a custom dataset for object detection, describe how Amazon SageMaker Ground Truth can be used to prepare a custom dataset. And finally, use Amazon Recognition to perform facial detection. Thanks for watching, we'll see you in the next video.\n",
      "Processing video 34/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod05_Sect01_ver2.mp4\n",
      "Mod05_Sect01_ver2.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod05_Sect01_ver2.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod05_Sect01_ver2.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod05_Sect01_ver2.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod05_Sect01_ver2.mp4\n",
      "Text:  Hi, welcome back. This is section one, and we're going to introduce computer vision. Computer vision is an exciting space in machine learning. You can think of computer vision as the automated extraction of information from digital images. Using computer vision, machines can identify people, places, and things in images with an accuracy that's at or above human levels, and with greater speed and efficiency. Computer vision is often built with deep learning models. It automates the extraction, analysis, classification, and understanding of useful information from a single image or a sequence of images. The image data can take many forms, such as single images, video sequences, views from multiple cameras, or three-dimensional data. Computing power and algorithms have advanced over the last 10 years. This has led to an increase in capabilities and easier access to computer vision technologies. So how is computer vision being used? Here are some of the primary use cases for computer vision. You can use image and facial recognition to improve public safety and home security, or as a way to authenticate access to personal devices. You can also use it to automatically classify images for content management and analysis. Autonomous driving is partly enabled by computer vision technologies, and so are safety features of cars, such as lane detection or collision avoidance. Medical image analysis with computer vision can improve the accuracy and speed of a patient's medical diagnosis. This can result in better treatment outcomes and life expectancy for the patient. And finally, in manufacturing, well-trained computer vision is incorporated into robotics. This can improve quality assurance and operational efficiencies. These are just a few examples, and you can probably think of more. Computer vision problems can be broken down into a few areas. Content recognition is about identifying things in images. It's a classification problem, but it's a complex one with several layers. In the picture here, what's represented? Is it breakfast, lunch, or dinner? Would the classification only be food? The answer depends on what model you use to perform the classification. Models must be trained, and the training data provides the algorithm with data for it to learn from. Say that you have a model that was trained with pictures of different types of food. You might expect the image to output categories such as milk, peaches, mashed potato, chicken nuggets, and salad. If you trained the model with different images, it could classify objects as tray, cutlery, and napkin instead. When you work with images, you might want to know what kinds of objects are in the image and the location of those objects. Object detection provides the image categories and where the objects are located in the image. There's a set of coordinates defining the location of a box surrounding the image, which is known as the bounding box. Bounding boxes for detection are typically top, left, width, and height coordinates surrounding the images. You can use these coordinates in your applications. the images. You can use these coordinates in your applications. When objects are detected in an image, there's a confidence number usually associated with that object. This percentage indicates the probability that the object belongs to a specific class. This confidence level is important when you want to determine an action that's based on object detection, especially in facial detection applications or cases where the action has significance. Object segmentation is also known as semantic segmentation. It's like object detection, but you go into more detail to get fine boundaries for each detected object. Basically it's a fine-grained inference for predicting each pixel in the image. Some applications that require object segmentation include autonomous vehicles and advanced computer-human interactions. Though object segmentation is a key problem in the field of computer vision, we won't be covering it in this course. Video adds another dimension to computer vision. With video, you get more data to work with, so you can capture the movement of people or objects, which are referred to as instances. For example, you can detect people who enter and leave frames, and also deal with moving cameras. Here's a use case for computer vision. Building on detection and tracking, you can analyze shopper behavior in your retail store by studying the path each person follows. If you use face analysis, you can understand other details about shoppers, such as average age ranges, gender distribution, and expressed emotions without identifying them. Here's another computer vision use case. You can also analyze images to identify actions using the motion in the video. For example, activities such as delivering a package or dancing. Looking at this image of a baseball player, some examples from the image could include capturing the batter's accuracy, the pitcher's pitching style, the type of pitch slow ball, slider and others, the inning, and the batter's performance versus the specific pitcher. Managers could use all that data to coach players on how to improve their performance, and they do. Coaches can also use the data during the game to make game-time decisions. Say you want to initiate various actions based on the speed of the baseball leaving the bat and its trajectory. A hit that's calculated by an ML model could lead to an audio or visual warning about a possible foul ball into the crowd. Or it could result in a preemptive alarm that a hit has a high probability of being a home run. This means that events following a home run could be both well-timed and automated, such as playing music or setting off fireworks when the home run is hit by the home team. To wrap up this section, here are some key takeaways for this section. First, we covered how computer vision is the automated extraction of information from images. You can divide computer vision into two distinct areas, image analysis and video analysis. Image analysis includes object classification, detection, and segmentation. Video analysis includes instance tracking, action recognition, and motion estimation. Thanks for watching. We'll see you in the next video.\n",
      "Processing video 35/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod05_Sect02_part1_ver2.mp4\n",
      "Mod05_Sect02_part1_ver2.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod05_Sect02_part1_ver2.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod05_Sect02_part1_ver2.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod05_Sect02_part1_ver2.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod05_Sect02_part1_ver2.mp4\n",
      "Text:  Welcome back. In this section, we'll explore image analysis in more detail. And in part two, we'll take a closer look into video analysis. To start, we'll introduce the main Amazon service we'll be using, Amazon Recognition. Amazon Recognition is a computer vision service that's based on deep learning. You can use it to add image and video analysis to your applications. There are many uses for Amazon Recognition, including creating searchable image and video libraries. Amazon Recognition makes both images and stored videos searchable, so you can discover the objects and scenes that appear in them. You can use Amazon Recognition to build a face-based user verification system, so your applications can confirm user identities by comparing their live image with a reference image. Amazon Recognition interprets emotional expressions, such as happy, sad, or surprise. It can also interpret demographic information from facial images, such as gender. Amazon Recognition can also detect inappropriate content in both images and stored videos. And finally, Amazon Recognition can recognize and extract text content from images. Before we go further, here's a quick note on security. You need to check if the applications you build using Amazon Recognition fall under any regulatory restrictions as defined in your field or country. Security and compliance for Amazon Recognition is a shared responsibility between AWS and the customer. For more information about this topic, see the AWS Compliance page. Amazon Recognition is an AWS-managed service. With a managed service, Amazon hosts the machine learning models, maintains an API, and scales out to meet demand for you. You can benefit from a set of models that constantly learn and improve. Also, you can focus on building applications that use the API and optionally training the service to understand your unique business needs. There are various resources you can use to access and interact with Amazon Recognition, such as APIs, SDKs, and commands for the AWS command line interface, which is also known as the AWS CLI. The languages supported by the SDKs include JavaScript, Python, PHP, .NET, Ruby, Python, PHP, .NET, Ruby, Java, Go, Node.js, and C++. Finally, Amazon Recognition integrates with other AWS services. For example, if you need storage, you can use Amazon's Simple Storage Service, or S3. For authentication and authorization, you can use AWS Identity and Access Management, which is also known as IAM. This diagram illustrates an image search feature where users can take pictures and get information about the real estate properties they're viewing. First, the user takes a picture with their mobile device. The user then initiates a search, which causes the application to upload to Amazon S3. S3 is configured to call other services when a write event occurs. In this case, the bucket passes the S3 path of the new object to AWS Lambda. When the Lambda function is called, it uses the Amazon Recognition SDK to call the service. Amazon Recognition analyzes the image, detects aspects of the property, creates labels, and passes the information back to Lambda as an object formatted in JavaScript Object Notation or JSON. Lambda then stores the labels and confidence score in Amazon Elastic Search Service, which is also known as Amazon ES. Application users can now identify aspects of a property using the objects that were detected in the image. In this example architecture, the system checks uploaded images for inappropriate content. Like the previous example, processing begins when the user uploads content. First, the user uploads an image to Amazon S3. Second, the S3 bucket is configured to call a Lambda function when an object is written to the bucket. Third, Lambda calls Amazon Recognition via the SDK. Amazon Recognition then analyzes the images for inappropriate content and sends the response back to Lambda. Fourth, if the content is appropriate, the content is approved. Fifth, if the content isn't appropriate, the content can be sent for manual inspection. And finally, if the content isn't approved, a notification is sent to the user. In this final use case, the system analyzes a video feed for sentiment analysis. First, an in-store camera captures video that's then sent to a back office or a cloud-based application. Typically, an application like this uses Amazon Kinesis to stream the video. Second, the application uses the SDK to send the video to Amazon Recognition for further analysis. Visual sentiment is extracted along with other attributes such as age. Third, the discovered attributes are sent to Amazon Kinesis. Fourth, a Lambda function extracts the data from the stream. Fifth, the data is then written to S3. Next, the data is loaded into Amazon Redshift on a regular basis. And finally, tools like Amazon QuickSight can be used to generate reports from the data. Amazon Recognition is designed to integrate into your applications through the API and SDKs. API operations are provided for detecting labels, faces, recognizing celebrities, and detecting unsafe images. To perform a prediction, provide the service with an image object in Amazon S3 or upload a byte stream of an image. Images can be in JPEG or PNG formats. Amazon Recognition processes the image, performs the prediction, and returns a JSON object with the results. When Amazon Recognition performs predictions, it often returns multiple labels. Each label has a confidence level. This confidence level indicates how likely the label was found in the image. Like this example shows, labels can also have hierarchies. When you find instances of objects, you need to understand where the detected object is in the image. For each instance, the results from Amazon Recognition include a bounding box that contains the starting coordinate of top, left, and box dimensions of width, height. Like the example, you can use this information to determine the location of the detected object in the image. It's important to note that all findings contain a confidence score. You can use the confidence score in your applications to tune your response to predictions. With a higher score, it's more likely that the object was correctly labeled. That's it for Part 1 of this section. We'll see you again for Part 2, where we'll explore facial detection.\n",
      "Processing video 36/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod05_Sect02_part2.mp4\n",
      "Mod05_Sect02_part2.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod05_Sect02_part2.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod05_Sect02_part2.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod05_Sect02_part2.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod05_Sect02_part2.mp4\n",
      "Text:  Hi, welcome back. We'll continue exploring image analysis with a closer look at facial detection. Facial detection uses a model that was tuned to perform predictions specifically for detecting faces and facial features. Facial detection has many of the same features as standard object detection, such as a bounding box or the coordinates of the box surrounding the face that was detected. This will include a value representing the confidence that the bounding box contains a face. There will be a list of attributes if found, such as if the face has a beard or if it appears to be male or female. There will also be a confidence score for these attributes. It can also detect physical emotions, like whether the person is smiling or frowning. It's important to understand this classification is based only on visual clues, and so it might not represent the actual emotion of the person. Facial landmarks are components of the face such as eyes and mouth. Typical landmarks also include X and Y coordinates. Quality describes the brightness and the sharpness of the face. And Pose describes the rotation of the face inside the image. Again, confidence is a feature here, and it's provided for each detected feature. And remember, the feature prediction is based only on visual observations. With Amazon Recognition, you can compare two images to determine if they contain the same person. Comparisons require both a source and a target image. The results will include all the faces that were found, and they include information about matching and non-matching faces. Again, confidence scores indicate how likely each prediction is. Amazon Recognition can also search for known faces. To use this feature, you need to train the model by providing a collection of images to use. After you train the model, you can then detect those people and images you provide. To find known faces, first create a collection and add faces to the collection. Amazon Recognition will perform facial recognition on the images you provide. It will then return typical information like the bounding box coordinates or the confidence score. To associate faces with an image, specify an image ID in the externalImageId request parameter. This could be the file name of the image or another ID that you create. After you create your collection, you can then use the searchFacesByImage operation to search for faces from the collection. The returned data contains an array of all faces that matched. The information includes bounding boxes, confidence scores, and the external image ID value. You can then use this ID value to link back to the source image. Now that you've learned about the facial detection features of Amazon Recognition, here's a summary of the guidelines we've discussed so far. When Amazon Recognition detects a human face, it captures a bounding box that shows where the face was found in the video. It can also detect attributes such as the position of the eyes, nose, and mouth. It can detect emotion, the quality of the detection, and any landmarks that might appear. All these items will have an associated confidence score. A higher score means that the model has greater confidence about the detection. Gender is inferred from the image, not inferred from identity. Similarly, emotion is also determined from the image, and it might not reflect the subject's actual emotional state. How should I apply facial recognition responsibly? Facial recognition should never be used in a way that violates an individual's rights, including the right to privacy. It should also never be used to make autonomous decisions for scenarios that require a human being to analyze them. For example, suppose that a bank uses tools like Amazon Recognition in a financial application to verify their customers' identities. The bank should always clearly disclose the use of the technology and ask the customer to approve their terms and conditions. For more information about this topic, see the AWS webpage about the facts on facial recognition with artificial intelligence. We'll now explain how you can use Amazon Recognition to process videos. You can perform video processing on both stored videos and video streams. Stored videos should be uploaded and stored in an S3 bucket. Each type of detection has its own start operation. You can search for people, faces, labels, celebrities, text, and inappropriate content. Amazon Recognition publishes a completion status to a topic in Amazon Simple Notification Service, which is also known as Amazon SNS. Then, SNS can route these messages to subscribers. For durability, it's a best practice to route messages to a message queue in Amazon Simple Queue Service or Amazon SQS. Your application should monitor the SQS queue for completion. Each start operation has a corresponding get operation for retrieving the results. If you call getDetectionResults, it returns an array of labels that contain information about any labels found in the video. The label information includes the same labels as image detection, but it also includes a timestamp of where the label was detected in milliseconds from the start of the video. In addition to stored videos, you can also use Amazon Recognition Video to detect and recognize faces in streaming video. A typical use case for this is detecting a known face in a video stream. Amazon Recognition Video uses Amazon Kinesis video streams to receive and process a video stream. Amazon Recognition Video uses Amazon Kinesis video streams to receive and process a video stream. The analysis results are output from Amazon Recognition Video to a Kinesis data stream. They are then read by your client application. Amazon Recognition Video provides a stream processor that's called CreateStreamProcessor, and you can use it to start and manage the analysis of the streaming video. To use Amazon Recognition Video with your streaming video, your application must implement these resources. First, you need a Kinesis video stream to send streaming video to Amazon Recognition Video. Next, you need an Amazon Recognition Video stream processor to manage the streaming video analysis. And finally, you need a Kinesis data stream consumer to read the analysis results that Amazon Recognition Video sends to the data stream. If you want to find a face in a video, you sends to the data stream. If you want to find a face in a video, you need to create a collection. This process is the same as creating a collection for still images. Amazon Recognition Video places a JSON frame record for each analyzed frame into the Kinesis output stream. Amazon Recognition Video doesn't analyze every frame that's passed to it through the Kinesis output stream. Amazon Recognition Video doesn't analyze every frame that's passed to it through the Kinesis video stream. A frame record that's sent to a Kinesis data stream contains information about which video stream fragment the frame is in, where the frame is in the fragment, and faces that are recognized in the frame. It also includes status information for the stream processor. Before we wrap up, here's a quick summary. Amazon Recognition is a computer vision service that's based on deep learning. You can easily add image and video analysis to your applications. Amazon Recognition can detect faces, sentiment, text, unsafe content, and library search in both images and video. Amazon Recognition is integrated with other AWS services. Thanks for watching. We'll see you in the next video.\n",
      "Processing video 37/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod05_Sect03_part1.mp4\n",
      "Mod05_Sect03_part1.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod05_Sect03_part1.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod05_Sect03_part1.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod05_Sect03_part1.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod05_Sect03_part1.mp4\n",
      "Text:  In this section, we'll look at preparing custom datasets for computer vision, so you can detect custom objects. One challenge of using a pre-built model is that it will only find images it was trained to find. Though Amazon Rekognition was trained with tens of millions of images, it can't detect objects that it wasn't trained on. For example, consider the 8 of hearts playing card. If you run this card through Amazon Recognition, the results show various attributes. However, none of the labels are playing card or 8 of hearts. If you want Amazon Recognition to detect images in your problem domain, you must train the model with your images. So in this section, you'll learn how to train Amazon Recognition with images from your problem domain. Though you'll focus only on using Amazon Recognition here, you'll encounter a similar process if you use other pre-trained models. Training a computer vision algorithm to recognize images requires a large input dataset, which isn't practical for most organizations. Many machine learning problems today can be solved by training existing models, or you can use a managed service like Amazon Recognition Custom Labels. Like other machine learning processes, you need to train Amazon Recognition so it recognizes scenes and objects that are in a specific domain. You'll need both a training dataset and a test dataset that contain labeled images. If you have images that need labels, you can use Amazon Recognition Custom Labels to simplify your labeling tasks. For example, it provides a UI for labeling images, which includes a feature you can use to draw bounding boxes around images. It can also help find objects and scenes that are unique to your business needs. You can use it to classify images or detect objects within an image. Say you want to identify specific machine parts in images, such as turbochargers or torque converters. You could collect pictures of each kind of machine part and use them to train your model. Amazon Rekognition Custom Labels also includes automated machine learning capabilities that handle the machine learning process for you. When you provide training images, the service can automatically load and inspect the data, select the correct machine learning algorithms, train a model, and provide model performance metrics. When you finish training your model, you can then evaluate your custom model's performance on your test set. Each image in the test set has a side-by-side comparison of the model's prediction versus the label it assigned. There are also detailed performance metrics for you to review. You can start using your model immediately for image analysis, or you can iterate and retrain new versions with more images to refine the model. After you start using your model, you can track your predictions, correct any mistakes, and use the feedback data to retrain new model versions and improve their performance. So how do you label images? The diagram shows a typical process for training a computer vision model, which includes the Amazon recognition custom labels feature. We'll step through this in some detail. The process of developing a custom model to analyze images requires time, expertise, and resources. It often takes months to complete. and resources. It often takes months to complete. It can also require thousands or tens of thousands of hand-labeled images so the model has enough data to make accurate decisions. It can take months to generate and gather this data. And it can require large teams of labelers to prepare it for use in machine learning. Amazon Recognition Custom Labels builds on the existing capabilities of Amazon Recognition, which is already trained on tens of millions of images across many categories. Instead of thousands of images, you can upload a small set of training images that are specific to your use case. Typically, you'd use a few hundred images for this. You can use the AWS Management Console to upload training images. If your images are already labeled, Amazon Recognition Custom Labels can begin training your model. If they're not, you can label the images directly in the labeling interface. Or you can use Amazon SageMaker Ground Truth to label them for you. There'll be more on that shortly. Amazon Recognition Custom Labels works best when you use different models for different domains. For example, if you need to detect both machine parts and plant health, you'd use two different models. Images you select for training should be similar to the images that will be used for inference. Use images that use various lighting conditions, backgrounds, and resolutions. Ideally, your training images will mirror images you'd want to perform detection on. If you can use the same source, like you'd use in production, that works best. The documentation includes additional guidelines on image type, so whether they are JPEGs or PNGs, and other properties, like image size and resolution. That's it for part one of this section. We'll see you again for part two, where we'll review how to create the training dataset.\n",
      "Processing video 38/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod05_Sect03_part2.mp4\n",
      "Mod05_Sect03_part2.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod05_Sect03_part2.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod05_Sect03_part2.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod05_Sect03_part2.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod05_Sect03_part2.mp4\n",
      "Text:  Hi, welcome back. We'll continue exploring video analysis by reviewing how to create the training dataset. Datasets contain information that's needed to train and test an Amazon Recognition Custom Labels model, such as images, labels, and bounding boxes. such as images, labels, and bounding boxes. You can use images from Amazon S3, or you can upload them from your computer to S3 as part of the process. To train a model, your dataset should have at least two labels, with at least 10 images per label. Each image in your dataset must be labeled. As we mentioned earlier, you can use the Amazon Recognition Custom Labels console or Amazon SageMaker Ground Truth to label your images. Again, to train an Amazon Recognition Custom Labels model, your images must be labeled. A label indicates that an image contains an object, scene, or concept. As we mentioned earlier, a dataset needs at least two defined labels. Also, each image must have at least one assigned label that identifies the object, scene, or concept in the image. When you apply labels to an image as a whole, these labels are known as image-level labels. They're useful for identifying scenes or concepts that you want to detect. For example, one of the images shows a beach scene from Ko'olina. It's on the island of Oahu in the US state of Hawaii. To train a model to detect beaches, you'd add a beach label that applies to the entire image. You can also apply labels to specific areas of an image that contain an object you want to detect. For example, if you want your model to detect Amazon Echo devices, it must identify the different types of Echo devices in an image. The model needs information about where the devices are located in the image, and it needs a corresponding label that identifies the type of the device. This information is known as localization information. The location of the device is expressed as a bounding box. The example objects with bounding boxes image shows a bounding box that surrounds an Amazon Echo dot. The image also contains an Amazon Echo without a bounding box. The output of the labeling process will be a manifest file. The manifest file for an image-level label typically contains the label, or class name, along with some metadata about how the image was labeled. For object detection, the manifest contains information about each labeled image. The bounding box identifies where the object is in the image, along with the label that the bounding box belongs to. We've mentioned Amazon SageMaker Ground Truth a few times. We'll now look at what it is and how it might help you. With SageMaker Ground Truth, you can build high-quality training datasets for your machine learning models. To use it, create a dataset that needs labeling. You then provide detailed instructions on what needs to be labeled and submit the job. You can decide who processes the images to create a labeled dataset. You can use workers from the Amazon Mechanical Turk service, a vendor company, or an internal workforce with machine learning. You can use the labeled dataset output from SageMaker Ground Truth to train your own models, or you can also use it with Amazon Rekognition custom labels. SageMaker Ground Truth can use active learning to automate the labeling of your input data. Active learning is a machine learning technique that identifies data that should be labeled by your workers. In SageMaker Ground Truth this functionality is called automated data labeling. Automated data labeling can reduce the time and cost it takes to label your data set compared to using only human workers. When you use automated labeling, you incur Amazon SageMaker training and inference costs. Yes, we just said that you can use machine learning to label the images that you'll then use for machine learning. We'll talk through how this works. When SageMaker Ground Truth starts an automated data labeling job, it selects a random sample of input data, or objects, and sends it to human workers. When the labeled data is returned, SageMaker Ground Truth uses this data, which is the validation data, to validate the models that were trained for automated data labeling. SageMaker Ground Truth runs a batch transform job using the validated model for inference on the validation data. Batch inference produces a confidence score and quality metric for each object in the validation data. Automated labeling determines if the confidence score for each object, which was produced in step five, meets the required threshold, which was determined in step four. If the confidence score meets the threshold, the expected quality of automatic labeling exceeds the requested level of accuracy. The object is then considered to be automatically labeled. Step 6 produces a dataset of unlabeled data with confidence scores. SageMaker Ground Truth selects data points with low confidence scores from this dataset and sends them to human workers for additional labeling. SageMaker Ground Truth then uses the existing human-labeled data and the additional human-labeled data to train a new model. The process is repeated until the dataset is fully labeled or until another stopping condition is met. For example, automatic labeling can stop when you meet your budget for human annotation. We recommend using automated data labeling on large data sets. The minimum number of objects allowed for automated data labeling is 1,250. However, we strongly suggest providing a minimum of 5,000 objects. That's it for part 2 of this section. We'll see you again for part 3, where we'll review how to evaluate and improve your model.\n",
      "Processing video 39/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod05_Sect03_part3.mp4\n",
      "Mod05_Sect03_part3.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod05_Sect03_part3.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod05_Sect03_part3.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod05_Sect03_part3.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod05_Sect03_part3.mp4\n",
      "Text:  Hi, welcome back. We'll continue exploring video analysis by reviewing how to create the test dataset. The final step before you train your model is to identify a test dataset. You will use this test dataset to validate and evaluate the model's performance. You'll do this by performing an inference on the images in the test dataset. You'll then compare the results with the labeling information that's in the training dataset. You can create your own test dataset. Alternatively, you can use Amazon Recognition Custom Labels to split your training dataset into two datasets by using an 80-20 split. This split means that 80% of the data is used for training and 20% is used for testing. After you define the training and test datasets, Amazon Recognition Custom Labels can automatically train the model for you. The service automatically loads and inspects the data, selects the correct machine learning algorithms, trains a model, and provides model performance metrics. You're charged for the amount of time a model takes to train. A dataset that contains more images and labels will take longer to train. When training's complete, you evaluate the performance of the model. During testing, Amazon Recognition Custom Labels predicts if a test image contains a custom label. The confidence score is a value that quantifies the certainty of the model's prediction. Because this is a classification problem, the results can be mapped to a confusion matrix. With a true positive, the model correctly predicts the presence of the custom label in the test image. That is, the predicted label is also a ground truth label for that image. For example, Amazon Recognition Custom Labels correctly returns a cat label when a cat is present in an image. For a false positive, the model incorrectly predicts the presence of a custom label in a test image. That is, the predicted label isn't a ground truth label for the image. For example, Amazon Recognition Custom Labels returns a cat label, but there's no cat label in the ground truth for that image. For a false negative, the model doesn't predict that a custom label is present in the image, but the ground truth for that image includes this label. For example, Amazon Recognition Custom Labels doesn't return a cat custom label for an image that contains a cat. With a true negative, the model correctly predicts that a custom label isn't present in the test image. For example, Amazon Recognition Custom Labels doesn't return a cat label for an image that doesn't contain a cat. The console provides access to true positive, false positive, and false negative values for each image in your test dataset. These prediction results are used to calculate the various metrics for each label and an aggregate of metrics for your entire test set. The same definitions apply to predictions that the model makes at the bounding box level. With bounding boxes, all metrics are calculated over each bounding box in each test image, regardless of whether the boxes are prediction or ground truth. To help you, Amazon Recognition Custom Labels provides various metrics. For example, you can view summary metrics and evaluation metrics for each label. It also provides precision metrics for each label, and an average precision metric for the entire test data set. Precision is the proportion of positive results that were correctly classified. Amazon Recognition Custom Labels provides average recall metrics for each label and an average recall metric for the entire test data set. Recall is the fraction of your test set labels that were correctly classified. Using the previous example of cats, that would be how many cats were correctly classified. The service also provides an average model performance score for each label, and an average model performance score for the entire test dataset. The F1 score combines precision and recall together to give you just one number that quantifies the overall performance of a particular machine learning algorithm. You might use the F1 score when you have a class imbalance, but you also want to preserve the equality between precision and sensitivity. A higher value means better model performance for both recall and precision. If you're satisfied with the accuracy of your model, you can start using it. That's it for part 3 of this section. We'll see you again for part 4 where we'll review how to evaluate and improve your model.\n",
      "Processing video 40/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod05_Sect03_part4_ver2.mp4\n",
      "Mod05_Sect03_part4_ver2.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod05_Sect03_part4_ver2.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod05_Sect03_part4_ver2.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod05_Sect03_part4_ver2.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod05_Sect03_part4_ver2.mp4\n",
      "Text:  Hi, welcome back. We'll continue exploring video analysis by reviewing how to evaluate and improve your model. In general, you can improve the quality of your model with larger quantities of better quality data. Use training images that clearly show the object or scene, and don't include many things that you're not interested in. For bounding boxes around objects, use training images that show the object as fully visible and not hidden by other objects. Make sure that your training and test data sets match the type of images that you'll eventually run inference on. For objects where you have just a few training examples, like logos, you should provide bounding boxes around the logo in your test images. These images represent the scenarios you want to localize the object in. Reducing false positives often results in better precision. To reduce false positives, first, check if increasing the confidence threshold enables you to keep the correct predictions while eliminating false positives. Increasing the confidence threshold eventually results in diminishing gains because of the trade-off between precision and recall for a given model. Next, check to see if you need to add additional classes for training. For example, if you are detecting cats, but often dogs are being flagged as cats, add dog as a label to your training dataset, along with the images of dogs that you got the false positive on. Effectively, you are helping the model learn to predict dog and not cat through the new training images. You might find that the model is confused between two of your custom labels, cat and dog. The test image with label cat is predicted as having label dog and vice versa. In this case, first check for mislabeled images in your training and test sets. Also, adding more training images that reflect this confusion will help a retrained model learn to better discriminate between cat and dog. Reducing false negatives often results in better recall. To reduce false negatives, first lower the confidence threshold. This should improve recall. Also use better examples to model the variety of both the object and the images they appear in. Finally, split your label into two classes that are easier to learn. For example, instead of good cookies and bad cookies, you might want good cookies, burnt cookies, and broken cookies to help the model learn each unique concept better. If you're satisfied with the performance of your model, you can make it available for use by starting it from the console or by using code. After the model is running, you can perform an inference with the AWS CLI or the SDK. When you call the API, you specify the Amazon resource name of the Amazon Recognition Custom Labels model that you want to use. The Amazon resource name is also known as an ARN. You'll also specify the image you want the model to make a prediction with. You can provide an input image as an image byte array of base64 encoded image bytes, or as an S3 object. Custom labels are returned in an array of custom label objects. Each custom label represents a single object, scene, or concept that's found in the image. A custom label includes a label for the object, scene, or concept that was found in the image. It also includes a bounding box for objects that were found in the image. The bounding box coordinates show where the object is located on the source image. The coordinate values are a ratio of the overall image size. Finally, the custom label includes the confidence score. This represents how confident Amazon Recognition Custom Labels is in the accuracy of the label and bounding box. During training, a model calculates a threshold value that determines if a prediction for a label is true. By default, the Detect Custom Labels operation doesn't return labels with a confidence value that's less than the model's calculated threshold value. To filter the returned labels, specify a value for minconfidence that's greater than the model's calculated threshold. You can get the model's calculated threshold from the model's training results in the Amazon Recognition Custom Labels console. To get all the labels regardless of confidence, specify a min confidence value of 0. If you find that the confidence values returned by the detect custom labels operation are too low, consider retraining the model. You can restrict the number of custom labels that are returned from the detect custom labels operation by specifying the Max Results input parameter. The returned results are sorted from the highest confidence to the lowest confidence. Here are some key takeaways from this section of the module. Models must be trained for the specific domain that you want to analyze. If you're looking for turbochargers, you'll need many pictures of turbochargers to train your model. You can set custom labeling for the specific business case. We looked at the custom labeling process and some of the tools you can use. If you want objects to be detected, you need to label images and create bounding boxes for these objects. You can use Amazon SageMaker Ground Truth to build training datasets for your models, which can also use machine learning to label your images. Thanks for watching, and we'll see you in the next video.\n",
      "Processing video 41/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod05_WrapUp_ver2.mp4\n",
      "Mod05_WrapUp_ver2.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod05_WrapUp_ver2.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod05_WrapUp_ver2.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod05_WrapUp_ver2.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod05_WrapUp_ver2.mp4\n",
      "Text:  It's now time to summarize some of the main points in this module. In this module, you learned how to describe the use cases for computer vision, describe the Amazon Managed Machine Learning services available for image and video analysis, list the steps required to prepare a custom data set for object detection. Describe how Amazon SageMaker Ground Truth can be used to prepare a custom data set. And use Amazon Recognition to perform facial detection. That concludes this introduction to computer vision. Thanks for watching. We'll see you again in the next video.\n",
      "Processing video 42/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod06_Intro.mp4\n",
      "Mod06_Intro.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod06_Intro.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod06_Intro.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod06_Intro.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod06_Intro.mp4\n",
      "Text:  Introduction to Natural Language Processing Hi, and welcome to Module 6 of AWS Academy Machine Learning, Introduction to Natural Language Processing. In this module, we'll introduce Natural Language Processing, which is also known as NLP. This section includes a description of the major challenges faced by NLP and the overall development process for NLP applications. We'll then review five AWS services you can use to speed up the development of NLP-based applications. After completing this module, you should be able to describe the NLP use cases that are solved by using managed Amazon ML services, and describe the managed Amazon ML services available for NLP. Let's get started.\n",
      "Processing video 43/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod06_Sect01.mp4\n",
      "Mod06_Sect01.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod06_Sect01.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod06_Sect01.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod06_Sect01.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod06_Sect01.mp4\n",
      "Text:  We'll get started by reviewing what Natural Language Processing means. Natural Language Processing is also known as NLP. Before we explain what NLP is, we'll consider an example of NLP, Amazon Alexa. Alexa works by having a device, such as an Amazon Echo, record your words. The recording of your speech is sent to Amazon's servers to be analyzed more efficiently. Amazon breaks down your phrase into individual sounds. Then, it connects to a database containing the pronunciation of various words to find which words most closely correspond to the combination of individual sounds. Amazon identifies important words to make sense of the tasks and carry out corresponding functions. For instance, if Alexa notices words like outside or temperature, it will open the weather Alexa skill. Amazon servers then send the information back to your device and Alexa skill. Amazon servers then send the information back to your device, and Alexa speaks. NLP is a broad term for a general set of business or computational problems you can solve with machine learning, or ML. However, NLP systems predate machine learning. For example, speech-to-text on older pre-smartphone cell phones used NLP, and so did screen readers. Many NLP systems now use some form of machine learning. NLP considers the hierarchical structure of language. Words are at the lowest layer in a hierarchy. A group of words make a phrase. In the next level up, phrases make a sentence. And ultimately, sentences convey ideas. NLP systems face several significant challenges. We'll look at its challenges next. Language isn't precise. Words can have different meanings based on the other words that surround them. device. Words can have different meanings based on the other words that surround them. This is known as context. Often the same words or phrases can have multiple meanings. For example, consider the term weather. You could be under the weather, which has a colloquial meaning in English that you're sick. Or you could say, there's wonderful weather outside, which means the weather conditions outside are good. The phrase, oh really, could convey surprise, disagreement, and many other things. It depends on the context and inflection. Here are some of the main challenges for NLP. One challenge is discovering the structure of the text. One challenge is discovering the structure of the text. One of the first tasks of any NLP application is to break down the text into meaningful units, such as words, phrases, and sentences. Another challenge is labeling data. After the system converts the text to data, it must apply labels representing the various parts of speech. Every language will require a different labeling scheme to match the language's grammar. NLP also faces a challenge in representing context. Because word meaning can depend heavily on context, any NLP system needs a way to represent it. This is a large challenge because there are many contexts, and it's difficult to convert context into a form computers can understand. Finally, although grammar defines a structure for language, the application of grammar is indescribably large in scope. Handling the variation in how language is used by humans is a major challenge for NLP systems. That's where machine learning can have a large impact. You can apply NLP to a range of problems. Some of the more common applications include search applications such as Google or Bing, human-machine interactions like Alexa, sentiment analysis for marketing or political campaigns, social research based on media analysis, and chatbots to mimic human speech in applications. You can apply the machine learning development pipeline you've seen throughout this course when developing an NLP solution. The first task is to formulate a problem, then collect and label data. For NLP, collecting data consists of breaking down the text into meaningful subsets and labeling the sets. Feature engineering is a major part of NLP applications. This process gets complicated when you're dealing with highly irregular or unstructured text. For example, say you're building an application to classify documents. You'd need to be able to distinguish between the words with common terms but different meanings. Labeling data in the NLP domain is sometimes also called tagging. In the labeling process, you assign individual text strings to different parts of speech. There are specialized tools you can use for NLP labeling. The first task for an NLP application is to convert the text to data so it can be analyzed. You convert text by removing words that aren't needed for analysis from the input text. In the example, the words this and is are removed to leave the phrase sample text. After removing stop words, you can normalize text by converting similar words into a common form. For example, the words run, runner, ran, and running are all different forms of the word run. You can normalize all instances of these words in a block of text using the stemming and lemmatization processes. Lemmatization groups different forms of a word into a single term. Limitization of the versions of the word run would group all instances of those forms into a single term, run. Stemming, on the other hand, removes characters that the stemming algorithm considers unnecessary. Stemming might not work with the run example, as the form ran might not be recognized as a form of the word run. After you've normalized the text, you can standardize it by removing words that aren't in the dictionary you're using for analysis. For example, you could remove acronyms, slang, and special characters. The Natural Language Toolkit is also known as NLTK. Their Python library provides functions for removing stop words and normalizing text. Another first step in creating an NLP system is to convert the text into a data collection such as a data frame. All NLP libraries provide functions to assist with this process. The example shows using the word tokenize function from the NLTK library. After you've cleaned up your text and loaded it into a data frame, you can apply one of the NLP models to create features. Here are a couple of common models. The first model is known as bag of words. This is a simple model for capturing the frequency of words in a document. The model creates a key for each word. The value of the key is the number of times that word occurs in the document. The second model is term frequency and inverse document frequency, which is also known as TF-IDF. Term frequency is a count of how many times a word appears in a document. Inverse document frequency is the number of times a word occurs in a group of documents. These two values are used together to calculate a weight for the words. Words that frequently appear in many documents have a lower weight. There are many established models in the NLP field. The example shows a bag-of-words model. Bag-of-words is a vector model. Vector models convert each sentence or phrase into a vector, which is a mathematical object that records both directionality and magnitude. In the example, a simple sentence is converted into a vector, where the frequency of each word is recorded. The word is has a value of two because it appears twice in the sentence. Bag of words is often used to classify documents into different categories. It's also used to derive attributes that feed into NLP applications, such as in sentiment analysis. There are three broad categories of text analysis. First, the classification of text is similar to other classification systems you've seen in this course. Text provides the input to a process that extracts features. Then you send the features through a machine learning algorithm that interacts with a classifier model and infers the classification. There are many applications for text matching. For example, autocorrect spelling and grammar checking are based on text matching. The algorithm for edit distance, also known as the Levenstein distance, is frequently used. You can derive relationships between different words or phrases in the text using a process called co-reference resolution. Several NLP systems provide Python libraries for deriving relationships. One of the biggest challenges for NLP is how to describe the context for the text. Consider this example where a user is searching for the term tablet. Because the word tablet has at least two distinct meanings, the search engine needs to know which meaning the user has in mind. Most search engines rely on the most commonly used context if the term isn't qualified further. For example, by adding another term like medicine or computing to the search. The process of extracting entities is known as named entity recognition, or NER. An NER model has the following functions. First, it can identify noun phrases using dependency charts and part of speech tagging. It can classify phrases using a classification algorithm, such as Word2Vec. Finally, it can disambiguate entities using a knowledge graph. Here's an example of using NER to extract the entities Titanic and North Atlantic from the text. After the named entities are extracted, you can use a knowledge graph to extract meaning. A knowledge graph combines subject matter expertise with machine learning to extract meaning. A knowledge graph combines subject matter expertise with machine learning to derive meaning. The Amazon Recommendations Engine is an example of a knowledge graph. Here are the main points to remember from this section. First, NLP predates machine learning. You can use the same ML workflow that you've seen in other modules for NLP. Some of the main use cases for NLP are search query analysis, human-machine interaction, and marketing and social research. NLP is complicated because human language lacks precision. Thanks for watching, we'll see you in the next video.\n",
      "Processing video 44/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod06_Sect02.mp4\n",
      "Mod06_Sect02.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod06_Sect02.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod06_Sect02.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod06_Sect02.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod06_Sect02.mp4\n",
      "Text:  Welcome back. In this section, we'll review five managed machine learning services you can use for various use cases. These services simplify the process of creating a machine learning application. We'll start by looking at Amazon Transcribe. You can use Amazon Transcribe to recognize speech in audio files and produce a transcription. It can recognize specific voices in an audio file, and you can create a customized vocabulary for terms that are specialized for a particular domain. You can also add a transcription service to your applications by integrating with WebSockets, a transcription service to your applications by integrating with WebSockets, an internet protocol you can use for two-way communication between an application and Amazon Transcribe. Here are some of the more common use cases for Amazon Transcribe. First, medical professionals can record their notes, and Amazon Transcribe can capture their spoken notes as text. Also, video production organizations can generate subtitles automatically from video. This could also be done in real time for a live feed to add closed captioning. Media companies can use Amazon Transcribe to capture and label content. They can then feed the content into Amazon Comprehend for further analysis. Last, companies can record customer service or sales calls and transcribe them. They can analyze the results for training or for strategic opportunities. Amazon Polly can convert text into lifelike speech. You can input either plain text files or a file that's formatted in Speech Synthesis Markup Language, or SSML. SSML is a markup language used to provide special instructions for how speech should sound. For example, if you want to introduce a pause in the flow of speech, you can add an SSML tag that instructs Amazon Polly to pause between two words. You can also output speech from Amazon Polly to MP3, Vorbis, and PCM audio stream formats. Amazon Polly is eligible for use with certain regulated workloads. For example, it's eligible for use with the U.S. Health Insurance Portability and Accountability Act of 1996, or HIPAA. Amazon Polly is also eligible for use with Payment Card Industry Data Security Standard, or PCI DSS. Here are some of the more common use cases for Amazon Polly. As a first example, major news companies are using Amazon Polly to generate vocal content directly from their written stories. It's also been embedded in mapping APIs so developers can add voice to their geo-based applications. Language training companies have used Amazon Polly to create systems for learning a new language. Finally, animators have used it to add voices to their characters. With Amazon Translate, you can create multi-language experiences in your applications. You can create systems that read documents in one language and then render or store them in another language. You can also use it as part of a document analysis system. Amazon Translate is fully integrated with other machine learning services, such as Amazon Comprehend, Amazon Transcribe, and Amazon Polly. With this integration, you can extract named entities, sentiment and key phrases by integrating it with Amazon Comprehend, create multilingual subtitles with Amazon Transcribe, and speak translated content with Amazon Polly. Here are some of the more common use cases for Amazon Translate. The first use case is building international websites. You can use Amazon Translate to quickly globalize your websites. Amazon Translate can also be used to develop multilingual chatbots. Chatbots are used to create a more human-like interface to applications. With Amazon Translate, you can create a chatbot that speaks multiple languages. Another use case is software localization. Localization is a major cost for all software aimed at a global audience. Amazon Translate can decrease software development time and significantly reduce costs for localizing software. The final example use case is international media management. Companies that manage media for a global audience have used Amazon Translate to reduce their costs for localization. Amazon Comprehend implements many of the NLP techniques that we reviewed earlier in this module. You can extract key entities, perform sentiment analysis, and tag words with parts of speech. Here are some of the more common use cases for Amazon Comprehend. The first example is analyzing legal and medical documents. is analyzing legal and medical documents. Legal, insurance, and medical organizations have used Amazon Comprehend to perform many of the NLP functions we reviewed in this module. Another use is for large-scale mobile app analysis. Mobile app developers use Amazon Comprehend to look for patterns of usage with their apps so they can design improvements. Financial fraud detection is another use case for Amazon Comprehend. Banking, financial, and other institutions have used it to examine very large data sets of financial transactions to uncover fraud and look for patterns of illegal transactions. Finally, it can be used for content management. Media and other content companies can use Amazon Comprehend to tag content for analysis and management. With Amazon Lex, you can add a human language front-end to your applications. Amazon Lex lets you use the same conversational engine that powers Amazon Alexa. You can automatically increase capacity for your Amazon Lex solution by creating AWS Lambda functions that scale on demand. You can also store log files of the conversations for further analysis. Here are some of the more common use cases for Amazon Lex. The first use case is building front-end interfaces for inventory management and sales. Voice interfaces are becoming more common. Companies have used Amazon Lex to add chatbots to their inventory and sales applications. Another use for Amazon Lex is creating customer service interfaces. Human-like voice applications are quickly becoming the standard for many customer service applications. Amazon Lex can reduce the time it takes to develop these chatbots and increase their quality. Amazon Lex can also be used to develop interactive assistance. By combining Amazon Lex with other ML services, customers are creating more sophisticated assistance for many different industries. The final example use case is querying databases with a human-like language. Amazon Lex has been combined with other AWS database services to create sophisticated data analysis applications with a human-like language interface. Here are some of the main points you should take away from this module. First, Amazon Transcribe can automatically convert spoken language to text. Amazon Polly can convert written text to spoken language. Amazon Translate can create real-time translation between languages. Amazon Comprehend automates many of the NLP use cases reviewed in this module. And finally, Amazon Lex can create a human-like interface to your applications. Thanks for watching. We'll see you in the next video.\n",
      "Processing video 45/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod06_WrapUp.mp4\n",
      "Mod06_WrapUp.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod06_WrapUp.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod06_WrapUp.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod06_WrapUp.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod06_WrapUp.mp4\n",
      "Text:  Welcome back. It's now time to review the module and wrap it up. In summary, in this module, you learned how to describe the NLP use cases that are solved by using managed Amazon ML services and describe the managed ML services available for NLP. Good job. Thanks for watching. We'll see you in the next module. and describe the managed ML services available for NLP. Good job. Thanks for watching. We'll see you in the next module.\n",
      "Processing video 46/46\n",
      "Downloading video: CUR-TF-200-ACMNLP-1/video/Mod07_Sect01.mp4\n",
      "Mod07_Sect01.mp3\n",
      "Converting video to audio: CUR-TF-200-ACMNLP-1/video/Mod07_Sect01.mp4\n",
      "MoviePy - Writing audio in audio_output/Mod07_Sect01.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "audio_output/Mod07_Sect01.mp3\n",
      "Video: CUR-TF-200-ACMNLP-1/video/Mod07_Sect01.mp4\n",
      "Text:  Welcome to Module 7, Course Wrap-Up. Congratulations on completing the AWS Academy Machine Learning course. We'll take a few minutes to review what you've learned and where you can go from here. We're going to start with a review of what you've learned in this course. You learned how to describe machine learning, implement a machine learning pipeline, and use Amazon machine learning services for forecasting, computer vision, and natural language processing. Well done. Although this course isn't designed to prepare you to become certified for the AWS Certified Machine Learning specialty, we'll review how you can continue to work towards that certification. AWS Certification helps you build credibility and confidence by validating your cloud expertise with an industry-recognized credential. It also helps organizations identify skilled professionals who can lead cloud initiatives by using AWS. You must earn a passing score by taking a proctored exam to earn an AWS certification. After receiving a passing score, you'll receive your certification credentials. AWS certification doesn't publish a list of all services or features that are covered in a certification exam. However, there's an exam guide for each exam, and it lists the current topic areas and objectives covered in the exam. Exam guides can be found on the Prepare for Your AWS Certification Exam webpage. You'll be required to update your certification or recertify every three years. View the AWS Certification Recertification page for more details. The information on this slide is current as of June 2020. However, exams are frequently updated. Also, the details regarding which exams are available and what topics are tested by each exam are subject to change. The AWS Certified Machine Learning Specialty means you can select and justify the appropriate machine learning approach for a given business problem. You can also identify appropriate AWS services to implement machine learning solutions. And finally, you can design and implement scalable, cost-optimized, reliable, and secure machine learning solutions. Before sitting for the AWS Certified Machine Learning Specialty exam, we recommend that you have the following knowledge and experience. First, you should have one to two years of experience developing, architecting, or running ML or deep learning workloads on the AWS Cloud. Your experience should include performing basic hyperparameter optimization and working with machine learning and deep learning frameworks. You should also be able to express the intuition behind basic ML algorithms. Finally, you should be able to follow best practices for model training, in addition to best practices for deployment and operations. Thanks for watching and congratulations on completing the AWS Academy Machine Learning course.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def download_from_s3(s3, bucket_name, key, output_path):\n",
    "    s3.download_file(bucket_name, key, output_path)\n",
    "\n",
    "# Following code convert into the audio format\n",
    "def convert_video_to_audio(video_route, audio_route):\n",
    "   \n",
    "    video = VideoFileClip(video_route)\n",
    "    audio = video.audio\n",
    "    audio.write_audiofile(audio_route)\n",
    "\n",
    "# Following code will be processing the video\n",
    "def process_video(video_index, output_directory):\n",
    "    \n",
    "    # Following code will download the video and than send it to convert into the audio format\n",
    "    print(\"Downloading video:\", video_index)\n",
    "    video_route = os.path.join(output_directory, 'video.mp4')\n",
    "    audio_file_name = os.path.splitext(os.path.basename(video_index))[0] + '.mp3'\n",
    "    print(audio_file_name)\n",
    "    audio_route = os.path.join(output_directory, audio_file_name)  \n",
    "\n",
    "    download_from_s3(s3, bucket_name, video_index, video_route)\n",
    "\n",
    "    print(\"Converting video to audio:\", video_index)\n",
    "    \n",
    "    convert_video_to_audio(video_route, audio_route)\n",
    "    print(f\"audio_output/{audio_file_name}\")\n",
    "    result = pipelin(f\"audio_output/{audio_file_name}\")\n",
    "    print(\"Video:\", video_index)\n",
    "    print(\"Text:\", result[\"text\"])\n",
    "    if not os.path.exists(\"text_data\"):\n",
    "        os.makedirs(\"text_data\")\n",
    "    text_file_name = audio_file_name.split(\".\")\n",
    "    with open(f\"text_data/{text_file_name[0]}.txt\",\"w\",encoding=\"UTF-8\") as file:\n",
    "        file.write(result[\"text\"])\n",
    "\n",
    "    # remove the all temporary file\n",
    "    os.remove(video_route)\n",
    "    os.remove(audio_route)\n",
    "\n",
    "# AWS S3 configuration\n",
    "bucket_name = \"aws-tc-largeobjects\"\n",
    "output_prefix = \"CUR-TF-200-ACMNLP-1/video/\"\n",
    "output_directory = \"audio_output\"  \n",
    "\n",
    "\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "processors = AutoProcessor.from_pretrained(model_id)\n",
    "pipelin = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processors.tokenizer,\n",
    "    feature_extractor=processors.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,\n",
    "    batch_size=16,\n",
    "    return_timestamps=True,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# print the list of videos in the S3 bucket\n",
    "response = s3.list_objects_v2(Bucket=bucket_name, Prefix=output_prefix)\n",
    "video_indexs = [obj['Key'] for obj in response.get('Contents', [])]\n",
    "\n",
    "for i, video_index in enumerate(video_indexs):\n",
    "    print(f\"Processing video {i+1}/{len(video_indexs)}\")\n",
    "    process_video(video_index, output_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalizing the text\n",
    "([Go to top](#Capstone-8:-Bringing-It-All-Together))\n",
    "\n",
    "Use this section to perform any text normalization steps that are necessary for your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: Mod04_Sect02_part2.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi, welcome back. well continue exploring wrangling time series data. seasonality in data is any kind of repeating observation where the frequency of the observation is stable. for example, in sales, you typically see higher sales at the end of a quarter and into the fourth quarter. consumer retail sees even higher sales in the fourth quarter. be aware that data can have multiple types of seasonality in the same data set. there are many times when you should incorporate seasonality information into your forecast. for instance, localized holidays are a good example for sales. the chart shows that the total revenue generated by arcades has a strong correlation with the number of computer science doctorates awarded in the us. but correlations do not mean causation. if you disagree, see the source for the chart. there are many other correlations plotted on the site, and none of them make any sense. with your own data, be careful that youre not seeing and acting on correlations that dont have meaning in the real world. heres an experiment. if you generate two random time series datasets of numbers between 0 and 1, youll find that they have a very low correlation. but if you introduce the same slope to both datasets, youll see a very strong correlation. you need to know how stable a system is. the level of stability, or stationarity, can inform how much you should expect the systems past behavior to inform future behavior. a system with low stability wont be successful at predicting the future. youll often want to determine the trend for a time series. want to determine the trend for a time series. but if you adjust the series for the trend, it can be difficult to compare it with another series that was also adjusted for the trend. this is because the trends might dominate the values in the series. this could then lead to overestimates in correlation between the two series like we discussed previously. autocorrelation is one of the special problems you face with time series data. as youve seen in other machine learning problems, the goal of building an ml model is to make sure youre separating the signal from the noise. autocorrelation is a form of noise because separate observations arent independent of each other. noise because separate observations arent independent of each other. a time series with autocorrelation might overstate the accuracy of the model thats produced. some of the algorithms youll look at in this module can help correct for autocorrelation. these factors, along with seasonality, will influence the model youll select to produce your forecast. some algorithms handle seasonality and autocorrelation, but others do not. pendus was developed with financial data analysis in mind. as such, its good at handling time series data. first, you can set the index for your pendas data frame to be a date time. you can then use date and time to select your data. you can use ranges that contain partial dates. you can also extract date parts such as year, month, weekday name, and more. for grouping and resampling tasks, pendus has builtin functions to do both. finally, pendus can give you insights into autocorrelation. for more information about pendus and the time series, refer to the pendus documentation. one of the tasks in building a forecasting application is to choose an appropriate algorithm. your choice of algorithm should be determined by the type of dataset youre using and the features of that dataset. amazon forecast supports these five algorithms, but there are others. each algorithm can handle data with slightly different characteristics. for example, take autoregressive integrated moving average, which is also known as arima. it removes autocorrelations that could influence the pattern of observations. or take exponential smoothing, which is also known as ets. this algorithm is useful for datasets with seasonality. you can find out more about these algorithms in the amazon forecast documentation. some key takeaways from this section of the module include time series data is sequence data that includes a time element which makes it different from regular datasets. some of the time challenges include dealing with different time formats, handling missing data through downsampling, upsampling, and smoothing, dealing with seasonality such as weekdays and yearly cycles, avoiding bad correlations. appendix has excellent time series support with functions for dealing with time. appendix has excellent time series support with functions for dealing with time. there are five algorithms used by amazon forecast, arima, deepar, ets, npts, and profit. thats it for this section, well see you in the next video.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod05_Sect03_part4_ver2.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi, welcome back. well continue exploring video analysis by reviewing how to evaluate and improve your model. in general, you can improve the quality of your model with larger quantities of better quality data. use training images that clearly show the object or scene, and dont include many things that youre not interested in. for bounding boxes around objects, use training images that show the object as fully visible and not hidden by other objects. make sure that your training and test data sets match the type of images that youll eventually run inference on. for objects where you have just a few training examples, like logos, you should provide bounding boxes around the logo in your test images. these images represent the scenarios you want to localize the object in. reducing false positives often results in better precision. to reduce false positives, first, check if increasing the confidence threshold enables you to keep the correct predictions while eliminating false positives. increasing the confidence threshold eventually results in diminishing gains because of the tradeoff between precision and recall for a given model. next, check to see if you need to add additional classes for training. for example, if you are detecting cats, but often dogs are being flagged as cats, add dog as a label to your training dataset, along with the images of dogs that you got the false positive on. effectively, you are helping the model learn to predict dog and not cat through the new training images. you might find that the model is confused between two of your custom labels, cat and dog. the test image with label cat is predicted as having label dog and vice versa. in this case, first check for mislabeled images in your training and test sets. also, adding more training images that reflect this confusion will help a retrained model learn to better discriminate between cat and dog. reducing false negatives often results in better recall. to reduce false negatives, first lower the confidence threshold. this should improve recall. also use better examples to model the variety of both the object and the images they appear in. finally, split your label into two classes that are easier to learn. for example, instead of good cookies and bad cookies, you might want good cookies, burnt cookies, and broken cookies to help the model learn each unique concept better. if youre satisfied with the performance of your model, you can make it available for use by starting it from the console or by using code. after the model is running, you can perform an inference with the aws cli or the sdk. when you call the api, you specify the amazon resource name of the amazon recognition custom labels model that you want to use. the amazon resource name is also known as an arn. youll also specify the image you want the model to make a prediction with. you can provide an input image as an image byte array of base64 encoded image bytes, or as an s3 object. custom labels are returned in an array of custom label objects. each custom label represents a single object, scene, or concept thats found in the image. a custom label includes a label for the object, scene, or concept that was found in the image. it also includes a bounding box for objects that were found in the image. the bounding box coordinates show where the object is located on the source image. the coordinate values are a ratio of the overall image size. finally, the custom label includes the confidence score. this represents how confident amazon recognition custom labels is in the accuracy of the label and bounding box. during training, a model calculates a threshold value that determines if a prediction for a label is true. by default, the detect custom labels operation doesnt return labels with a confidence value thats less than the models calculated threshold value. to filter the returned labels, specify a value for minconfidence thats greater than the models calculated threshold. you can get the models calculated threshold from the models training results in the amazon recognition custom labels console. to get all the labels regardless of confidence, specify a min confidence value of 0. if you find that the confidence values returned by the detect custom labels operation are too low, consider retraining the model. you can restrict the number of custom labels that are returned from the detect custom labels operation by specifying the max results input parameter. the returned results are sorted from the highest confidence to the lowest confidence. here are some key takeaways from this section of the module. models must be trained for the specific domain that you want to analyze. if youre looking for turbochargers, youll need many pictures of turbochargers to train your model. you can set custom labeling for the specific business case. we looked at the custom labeling process and some of the tools you can use. if you want objects to be detected, you need to label images and create bounding boxes for these objects. you can use amazon sagemaker ground truth to build training datasets for your models, which can also use machine learning to label your images. thanks for watching, and well see you in the next video.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod05_Intro.txt\n",
      "\n",
      "\n",
      "Normalized Text: welcome back to aws academy machine learning. this is module 5, and we have a great topic for you today, computer vision. in this module, well start with an overview of the computer vision space, and youll learn about some of the use cases and terminology. next, well explore details about analyzing image and video with managed services from amazon web services or aws. finally, well look at how you can use your own customized data sets for performing object detection. at the end of this module, youll be able to describe the use cases for computer vision, describe the amazon managed machine learning services available for image and video analysis, list the steps required to prepare a custom dataset for object detection, describe how amazon sagemaker ground truth can be used to prepare a custom dataset. and finally, use amazon recognition to perform facial detection. thanks for watching, well see you in the next video.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod02_Sect04.txt\n",
      "\n",
      "\n",
      "Normalized Text: welcome back. in this section, well look at some of the tools youll be using throughout the rest of this course. before we start, this list isnt an exhaustive list of all the tools available today. were only going to cover them at a high level, but its a good place to get started. first, theres the jupyter notebook. the jupyter notebook is an opensource web application you can use to create and share documents that contain live code, equations, visualizations, and narrative text. uses include data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more. jupyterlab is a webbased interactive development environment for jupyter notebooks, code, and data. jupyterlab is flexible. you can use it to configure and arrange the user interface to support a wide range of workflows in data science, scientific computing, and machine learning. jupyterlab is extensible and modular. you can write plugins that add new components and integrate with existing ones. later in this course, youll use amazon sagemaker, which hosts both jupyter notebooks and jupyterlab. pandas is an opensource python library. its used for data handling and analysis. pandas represents data in a table similar to a spreadsheet. this table is known as a pandas data frame. matplotlib is a python library for creating scientific, static, animated, and interactive visualizations in python. youll use it to generate plots of your data later in this course. seaborn is another data visualization library for python thats built on matplotlib. it provides a highlevel interface for drawing attractive and informative statistical graphs. numpy is one of the fundamental scientific computing packages in python. it contains functions for ndimensional array objects. it also has useful math functions such as linear algebra, fourier transform, and random number capabilities. scikitlearn is an opensource machine learning library that supports supervised and unsupervised learning. it also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities. scikitlearn is built on numpy, scipy, and matplotlib. its a good tool for exploring machine learning. although youll only use it to borrow a few functions in this course, you might want to consider exploring it after you complete this course. moving up from individual libraries and packages, there are also tools that contain productionready frameworks. we already mentioned scikitlearn, which is a good library for machine learning. the frameworks supported on aws, such as tensorflow and keras, also include libraries you can use for machine learning. all the frameworks listed here are supported on aws and can be used from amazon sagemaker. aws also provides compute instances that are tuned for machine learning in both the cloud and at the edge. compute instances can be optimized for learning and inference. another aws resource you can use are certain amazon machine images or amis. we offer prepackaged amis that contain many of the popular frameworks. finally, theres amazon sagemaker, which is an aws service with many capabilities. first, sagemaker can deploy machine learning instances running jupyter notebooks and jupyter lab. it manages the deployment of these compute resources, so you only need to connect to the jupyter environment. sagemaker also provides tools for labeling data, training models, and hosting trained models. aws marketplace also provides a selection of readytouse model packages and algorithms from thirdparty machine learning developers. aws also provides a set of managed machine learning services and you can integrate them into your applications even if you dont have substantial machine learning experience. for computer vision, amazon recognition provides object and facial recognition for both image and video. also, amazon textract can extract text from images. speech services include amazon polly, which can speak text. another speech service is amazon transcribe, which converts spoken audio to text. for language, amazon comprehend uses nlp to find insights and relationships in text. also, amazon translate can translate text into different languages. if you want to work with chatbots, amazon lex helps you build interactive conversational applications that use voice or text. for forecasting, amazon forecast uses machine learning to combine time series data with additional variables so you can build forecasts. and finally, if youd like to work with recommendations, amazon personalize can help you create individual personalized recommendations for customers. these managed services have already been trained in many aspects of the problem domain. you only need to provide your specific data to get started. were going to look at many of these managed services in the second half of this course, after you learn how to do things on your own. the key takeaways for this section include these points. first, python is the most popular language for performing machine learning tasks. jupyter notebooks provide you with a webbased, hosted development environment for machine learning. youll use jupyter notebooks frequently in machine learning. there are a large number of opensource tools, such as pendus, that youll use often as a machine learning practitioner. finally, depending upon your requirements, you might start with lowlevel frameworks to create your own solution. you might also use tools such as amazon sagemaker to help with some of the heavy lifting. or you could simply use and adapt one of the managed amazon ml services for your specific problem domain. thats it for this video. well see you in the next one.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod05_Sect02_part2.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi, welcome back. well continue exploring image analysis with a closer look at facial detection. facial detection uses a model that was tuned to perform predictions specifically for detecting faces and facial features. facial detection has many of the same features as standard object detection, such as a bounding box or the coordinates of the box surrounding the face that was detected. this will include a value representing the confidence that the bounding box contains a face. there will be a list of attributes if found, such as if the face has a beard or if it appears to be male or female. there will also be a confidence score for these attributes. it can also detect physical emotions, like whether the person is smiling or frowning. its important to understand this classification is based only on visual clues, and so it might not represent the actual emotion of the person. facial landmarks are components of the face such as eyes and mouth. typical landmarks also include x and y coordinates. quality describes the brightness and the sharpness of the face. and pose describes the rotation of the face inside the image. again, confidence is a feature here, and its provided for each detected feature. and remember, the feature prediction is based only on visual observations. with amazon recognition, you can compare two images to determine if they contain the same person. comparisons require both a source and a target image. the results will include all the faces that were found, and they include information about matching and nonmatching faces. again, confidence scores indicate how likely each prediction is. amazon recognition can also search for known faces. to use this feature, you need to train the model by providing a collection of images to use. after you train the model, you can then detect those people and images you provide. to find known faces, first create a collection and add faces to the collection. amazon recognition will perform facial recognition on the images you provide. it will then return typical information like the bounding box coordinates or the confidence score. to associate faces with an image, specify an image id in the externalimageid request parameter. this could be the file name of the image or another id that you create. after you create your collection, you can then use the searchfacesbyimage operation to search for faces from the collection. the returned data contains an array of all faces that matched. the information includes bounding boxes, confidence scores, and the external image id value. you can then use this id value to link back to the source image. now that youve learned about the facial detection features of amazon recognition, heres a summary of the guidelines weve discussed so far. when amazon recognition detects a human face, it captures a bounding box that shows where the face was found in the video. it can also detect attributes such as the position of the eyes, nose, and mouth. it can detect emotion, the quality of the detection, and any landmarks that might appear. all these items will have an associated confidence score. a higher score means that the model has greater confidence about the detection. gender is inferred from the image, not inferred from identity. similarly, emotion is also determined from the image, and it might not reflect the subjects actual emotional state. how should i apply facial recognition responsibly? facial recognition should never be used in a way that violates an individuals rights, including the right to privacy. it should also never be used to make autonomous decisions for scenarios that require a human being to analyze them. for example, suppose that a bank uses tools like amazon recognition in a financial application to verify their customers identities. the bank should always clearly disclose the use of the technology and ask the customer to approve their terms and conditions. for more information about this topic, see the aws webpage about the facts on facial recognition with artificial intelligence. well now explain how you can use amazon recognition to process videos. you can perform video processing on both stored videos and video streams. stored videos should be uploaded and stored in an s3 bucket. each type of detection has its own start operation. you can search for people, faces, labels, celebrities, text, and inappropriate content. amazon recognition publishes a completion status to a topic in amazon simple notification service, which is also known as amazon sns. then, sns can route these messages to subscribers. for durability, its a best practice to route messages to a message queue in amazon simple queue service or amazon sqs. your application should monitor the sqs queue for completion. each start operation has a corresponding get operation for retrieving the results. if you call getdetectionresults, it returns an array of labels that contain information about any labels found in the video. the label information includes the same labels as image detection, but it also includes a timestamp of where the label was detected in milliseconds from the start of the video. in addition to stored videos, you can also use amazon recognition video to detect and recognize faces in streaming video. a typical use case for this is detecting a known face in a video stream. amazon recognition video uses amazon kinesis video streams to receive and process a video stream. amazon recognition video uses amazon kinesis video streams to receive and process a video stream. the analysis results are output from amazon recognition video to a kinesis data stream. they are then read by your client application. amazon recognition video provides a stream processor thats called createstreamprocessor, and you can use it to start and manage the analysis of the streaming video. to use amazon recognition video with your streaming video, your application must implement these resources. first, you need a kinesis video stream to send streaming video to amazon recognition video. next, you need an amazon recognition video stream processor to manage the streaming video analysis. and finally, you need a kinesis data stream consumer to read the analysis results that amazon recognition video sends to the data stream. if you want to find a face in a video, you sends to the data stream. if you want to find a face in a video, you need to create a collection. this process is the same as creating a collection for still images. amazon recognition video places a json frame record for each analyzed frame into the kinesis output stream. amazon recognition video doesnt analyze every frame thats passed to it through the kinesis output stream. amazon recognition video doesnt analyze every frame thats passed to it through the kinesis video stream. a frame record thats sent to a kinesis data stream contains information about which video stream fragment the frame is in, where the frame is in the fragment, and faces that are recognized in the frame. it also includes status information for the stream processor. before we wrap up, heres a quick summary. amazon recognition is a computer vision service thats based on deep learning. you can easily add image and video analysis to your applications. amazon recognition can detect faces, sentiment, text, unsafe content, and library search in both images and video. amazon recognition is integrated with other aws services. thanks for watching. well see you in the next video.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod03_Sect04_part2.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi, welcome back. well continue exploring feature engineering by reviewing how to clean your dataset. in addition to converting string data to numerical data, youll need to clean your dataset for several other potential problem areas. before encoding the string data, make sure the strings are all consistent. youll also need to make sure variables use a consistent scale. for example, if one variable describes the number of doors in a car, the scale will probably be between 2 and 8. but if another variable describes the number of cars of a particular type sold in the state of california, the scale will type sold in the state of california, the scale will probably be in the thousands. some data items might also capture more than one variable in a single value. for instance, suppose the dataset includes variables that combine safety and maintenance into a single variable, such as safe high maintenance. single variable, such as safe high maintenance. youll need to train your machine learning system for both variables and also split that single variable into two separate variables. you might also encounter data sets that are missing data for some variables, and some data sets will include outliers. well cover techniques for dealing with these situations in this section. you might find that data is missing. for example, some columns in your data set could be missing data because of a data collection error, or maybe data wasnt collected on a particular feature until the data collection process was underway. missing data can make it difficult to accurately interpret the relationship between the related feature and the target variable. so regardless of how the data ended up being missed, its important for you to deal with this issue. unfortunately, most machine learning algorithms cant handle missing values automatically. youll need to use human intelligence to update missing values with data thats meaningful and relevant to the problem. most python libraries for data manipulation include functions for finding missing data. so how do you decide if you should drop or impute missing values. this question is answered in part by better understanding how those values came to be missing in the first place and how much data the missing values represent within your larger data set. for instance, say the missing values are randomly spread throughout your data set and dont represent a larger portion of its respective row or column. in this case, imputation is most likely the better option. in contrast, say that you have a column or row that has a large percentage of missing values. in this case, dropping the entire row or column would be preferred over imputation. if you decide to drop rows with missing data, you can use builtin functions to do this. for example, pandas dropna function can drop all rows with missing data, or you can drop specific data values by using a subset. as an alternative to dropping missing values, you can impute values for those missing values. there are different ways to impute a missing value. for categorical values, the missing value is usually replaced with the mean, the median, or the most frequent values. for numerical or continuous variables, the missing value is usually replaced with the mean or the median. you can impute a single row of missing data, which is known as univariate. you can also do this for multiple rows, which is known as multivariate. well now look at a univariate example. here the scikitlearn imputer function is being used to impute some missing values. its a fairly small dataset, but there are two missing values. the missing value was imputed by the strategy of the mean. to do this, you first calculate the mean. here its the mean of 3 and 2, which is 2.5. then youll impute the mean value for the missing value. some data libraries include an impute package that provides more complex ways to impute data. examples include knearestneighbor, soft impute, multiple imputation by chain equations, and others. thats it for part 2 of this section. well see you again for part 3 where well review how to work with outliers in your data.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod02_Sect05.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi, welcome back. this is section 5 and were going to discuss challenges with machine learning. youll come across many challenges in machine learning. there are a lot of poor quality and inconsistent data available. a significant portion of your job will be getting access to or generating enough good data thats representative of the problem you want to solve. a key issue to watch out for is under or overfitting the model. its not all about the data, although it mostly is. do you have data science experience? is staffing a team of data scientists costeffective? does management support using machine learning? what does the business landscape look like? are the problems too complex to formulate into a machine learning problem? can the resulting model be explained to the business? if it cant be explained, it might not get adopted. whats the cost of building, updating, and operating a machine learning solution? finally, how does the technology map? does the business unit have access to the data thats needed? can the data be secured to meet any regulatory requirements? what tools and frameworks will be used? how will the solution integrate with other systems? these are important questions. to be successful, youll need to be able to answer and address them. many machine learning problems can be solved today by using existing models and without substantial machine learning knowledge. weve already talked about the aws managed services for machine learning. you can add sophisticated machine learning capabilities to your applications with only some basic developer skills for calling apis. there are other prebuilt models you can use or adapt. one example is yolo, which means you only look once. yolo is a popular computer vision model. in addition to these scenarios, you can use the aws marketplace if youd prefer to buy models and services from independent software vendors instead of developing your own. here are the key takeaways for this section. first, youll face many machine learning challenges. the biggest ones that you can directly influence are related to data. you should consider managed services to solve machine learning problems within the domains they support, such as using amazon recognition for computer vision problems. thats it for this section. well see you in the next video.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod03_Sect06.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi and welcome back. this is section 6 and were going to look at hosting and using the model. in this section, well look at how you can deploy your trained model so it can be consumed by applications. after youve trained, tuned, and tested your model youll learn more about testing in the next section, youre now ready to deploy your model. if youre thinking that were looking at the phases out of order, heres why were discussing deployment now. if you want to test your model and get performance metrics from it, you first need to make an inference or prediction from the model, and this typically requires deployment. deployment for testing is different from production, although the mechanics are the same. amazon sagemaker provides everything you need to host your model for simple testing and evaluation, from a few requests to deployments handling tens of thousands of requests. there are two ways you can deploy your model. for single predictions, you can deploy your model with amazon sagemaker hosting services. sagemaker will deploy multiple compute instances, which run your model behind a loadbalanced endpoint. applications can call the api at the endpoint to make predictions. with this model, you can scale the number of instances up or down based on demand. to get predictions for an entire dataset, use amazon sagemaker batch transform. instead of deploying and maintaining a permanent endpoint, sagemaker will spin up your model and perform the predictions for the entire dataset you provide. it will then store the results in amazon s3 before it shuts down and terminates the compute instances. its useful for performing batch predictions when you test the model. you can quickly run your entire validation set against the model without writing any code to process and collate the individual results. the goal of the deployment phase is to provide a managed environment to host models for providing inference securely and with low latency. after your model is deployed into production, you should monitor your production data and retrain your model if necessary. newly deployed models need to reflect the current production data. new data is accumulated over time and it could potentially identify alternative or new outcomes. and so deploying a model is not a onetime exercise. instead, its a continuous process. with one click, you can deploy your model on amazon ml instances that can automatically scale across multiple availability zones for higher redundancy. just specify the type of instance and the maximum and minimum number of instances desired. sagemaker will take care of the rest. it will launch the instances, deploy your model, and set up the secure https endpoint for your application. secure https endpoint for your application. your application only needs to include an api call to this endpoint to achieve inference with low latency and high throughput. with this architecture, you can integrate your new models into your application in minutes because changes to the model no longer need changes to the application code. sagemaker manages your production compute infrastructure on your behalf. it can perform health checks, apply security patches, and conduct other routine maintenance, all with builtin amazon cloudwatch monitoring and logging. after youve trained the model, you can create the endpoint either in code or by using the sagemaker console. if youre planning to host only a single model, you can create an endpoint for that model. but if youre planning to host multiple models, you need to create a multimodel endpoint. multimodel endpoints provide a scalable and costeffective solution for deploying large numbers of models. they use a shared serving container thats enabled to host multiple models. this reduces hosting costs by improving endpoint utilization compared to using single model endpoints. it also reduces deployment overhead because sagemaker manages loading models in memory and scaling the models based on the traffic patterns to them. when you deploy machine learning models into production to make predictions on new data, you need to make sure you apply the same data processing steps that were used in training to each inference request. otherwise, you can get incorrect prediction results. by using inference pipelines, you can reuse the data processing steps from model training during inference without maintaining two separate copies of the same code. this helps ensure the accuracy of your predictions and reduces development overhead. because sagemaker is a managed service, inference pipelines are completely managed. when you deploy the pipeline model, the service installs and runs the sequence of containers on each ec2 instance in the endpoint or each batch transform job. additionally, the sequence of feature processing and inference runs with low latency because the containers are collated on the same ec2 instances. some key takeaways from this section of the module include these points. you can deploy your trained model by using sagemaker to handle api calls from applications or to perform predictions using a batch transformation. the goal of your model is to generate predictions to answer the business problem. be sure that your model can generate good results before you deploy to production. finally, use multimodel endpoint support to save resources when you have multiple models to deploy. thats it for this section. well see you in the next video.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod03_WrapUp.txt\n",
      "\n",
      "\n",
      "Normalized Text: its now time to review the module and wrap up with a knowledge check. in this module, you learned how to formulate a problem from a business request, obtain and secure data for machine learning, build a jupyter notebook by using amazon sagemaker. outline the process for evaluating data. explain why data needs to be preprocessed. use open source tools to examine and preprocess data. use amazon sagemaker to train and host a machine learning model. use crossvalidation to test the performance of an ml model, use a hosted model for inference, and create an amazon sagemaker hyperparameter tuning job to optimize a models effectiveness. that concludes this module. thanks for watching. well see you again in the next video.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod04_Sect02_part1.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi and welcome back. this is section 2 and were going to focus on processing time series data because it can be different from other types of data youve been using so far. time series data is data that is captured in chronological sequence over a defined period of time. introducing time into a machine learning model has a positive impact because the model can derive meaning from changes in the data points over time. time series data tends to be correlated. this means that theres a dependency between data points. this has mixed results for forecasting. this is because youre dealing with a regression problem, and regression assumes that data points are independent. you need to develop a method for dealing with data dependence so you can increase the validity of the predictions. in addition to the time series data, you can add related data to augment a forecasting model. for example, suppose you want to make a prediction about retail sales. you could include information about the product being sold, such as item identification or sales price, along with the number of units sold per time period. the third type of data is metadata about the dataset. for instance, say that you have a retail dataset. you might want to include metadata, like a brand name or a genre for music or videos, so you can group results. its better to have more data. when you work with multiple data sources, youll face the challenge of handling the timestamp of the data. youll observe differences in the timestamp format and other challenges such as incomplete data. however, you might be able to infer missing data in some cases. for example, say you have some data that contains both the month and the day, but no year. observe whether the data seems to sequence through the month numbers in the database, repeating after 12. if it does so, you could add the year if you knew when the data started. you could then infer future years based on the order of the data. much timestamp data is stored in utc format, but not all data is. you should check if the timestamp is in local or universal time. sometimes the timestamp doesnt represent the time you think it does. for example, suppose you have a database of cars that were serviced at a garage. does the timestamp indicate the time the car arrived, was completed, or picked up? or does it indicate when the final entry was entered into the system? say youre trying to model the hourly caloric intake of patients. however, you only have daily data. then youll need to adjust your target timescale. also, your data might not have any timestamps. there could be other ways to extrapolate a time series depending on the data and domain. for example, you might have wavelength measurements or vectors within an image. as a final note, remember that daylight savings is different around the world. also because of daylight savings, time might even occur twice a year in their time zones. a common occurrence in realworld forecasting problems is missing values in the raw data. missing values makes it harder for a model to generate a forecast. the primary example in retail is an outofstock situation in demand forecasting. if an item goes out of stock, the sales for the day will zero. if the forecast is generated based on those zero sales values, the forecast will be incorrect. there are many reasons why values can be marked as missing. missing values can occur because of no transaction. they can also occur because of possible measurement errors. for example, a service that monitored certain data wasnt working correctly. or as another example, the measurement couldnt happen correctly. in retail, the primary example for an inability to take correct measurements is an outofstock situation in demand forecasting. this means that demand doesnt equal sales on that day. there are several ways you can calculate the missing data. the first method is forward fill. this uses the last known value for the missing value. building on that idea, moving average uses the average of the last known values to calculate the missing value. backward fill uses the next known value after the missing value. the danger here is that youre using the future to calculate the past, which is bad in forecasting. this method is also known as lookahead and should be avoided. interpolation uses an equation to calculate the missing value. you can also use a zero fill. this is often used in retail because missing sales data shouldnt be calculated. the missing data represents that there were no orders on that day. it would be wise to investigate why this happened, but in this case, you dont want to fill in the missing value. you might get data at different frequencies. for example, you might have sales data that includes the exact timestamp the sale was recorded, but have inventory data that only contains the year, month, and day of the inventory level. when you have data thats at a different frequency than other datasets, or data thats not compatible with your question, you might need to downsample. downsampling is moving from a more finely grained time to a less finely grained time. as the example shows, this could be converting an hourly dataset to a daily dataset. when downsampling, you need to decide how to combine the values. in the previous case of sales data, summing the quantity makes the most sense. if the data is temperature, you might want to find the average. understanding your data helps you decide whats the best course of action. the opposite of downsampling is upsampling, when you move from a less finely grained time to a more finely grained time. the problem with upsampling is that its extremely difficult to achieve in most cases. suppose you want to upsample your sales data from daily sales to hourly sales. unless you have some other data source to reference, you wouldnt be able to do this. there are cases when you need to do something, perhaps to match the frequency of another time series. or you might have an irregular time series or specific domain knowledge that would help. in those cases, you need to be careful how you make the conversion. for the retail example, the best you could do is create a single order for the day at a specified hour. for temperature, you could copy the daily temperature into each hourly slot, or use a formula to calculate a curve. in data science, outliers have a mix of positive and negative attributes. the same is true of time series data. suppose you were examining sales data, and you had an order that has an unusually high number of items. you might not want to include that in your forecast calculations because the order size might never be repeated. removing these outliers and anomalies is known as smoothing. smoothing your data can help you deal with outliers and other anomalies. there are a few reasons why you might consider smoothing. first, during data preparation, you remove error values and could also remove outliers. you might also want to smooth your data to generate features. for visualization, you could smooth your data to reduce the noise in a plot. its important to understand why you are smoothing the data and the impact that it might have. the outcome might be to reduce noise and create a better model. but an equally important question is, could your smoothing compromise the model? is the model expecting noisy data? will you also be able to smooth the data in production? thats it for part one of this section. well see you again for part two, where well review more time series specific challenges and the tools and algorithms that can help us wrangle your data.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod02_Sect01.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi, and welcome to section 1. in this section, were going to talk about what machine learning is. this course is an introduction to machine learning, which is also known as ml. but first, well discuss where machine learning fits into the larger picture. machine learning is a subset of artificial intelligence, or ai. this is a broad branch of computer science thats focused on building machines that can do human tasks. deep learning is a subdomain of machine learning. to understand where these all fit together, well discuss each one. as we just mentioned, machine learning is a subset of a broader computer science field known as artificial intelligence. ai focuses on building machines that can perform tasks a human would typically perform. in contemporary popular culture, youve probably seen ais in movies, television, or works of fiction. for example, you might have seen ais that control the world around them, or that start acting on their own initiative. these ais started as computer agents that perceive their environments and take actions to achieve a specific goal, though maybe not the outcome their creators originally wished for. other fictional ais interact extensively with humans as helpers or workers, and they generally do a better job working with humanity, but theyre more general in purpose. these kinds of ais are examples of artificial general intelligence, or agi. they have the capacity to learn or understand any task that a human can. ai problems typically span many fields of research, such as natural language processing, reasoning, knowledge representation, learning, perception, and physical environment interaction. ai isnt yet a reality for us, unless we are all truly living in a simulation. but every year, we move closer to it in each of those areas. you might have also read or seen commentary on the ethics of creating ai. not all views are positive, perhaps partly in fear of the malicious fictional ais that want to destroy humanity or use them as power sources. or perhaps theyre concerned about the risk of mass unemployment because an intelligent machine could work 247 and not need any breaks. dont worry, though. were not going to build the next rogue ai in this course. maybe in the next one. if you do a search, youll probably find many definitions of machine learning. there isnt a universally agreedupon definition, so well start by looking at a couple of definitions. for example, we could say, machine learning is the scientific study of algorithms and statistical models to perform a task by using inference instead of instructions. this isnt a bad starting point. the key point here is, using algorithms and statistical models instead of instructions. to help you better understand this, well apply this idea to a concrete example. suppose you need to write an application that determines if an email message is spam or not. without machine learning, youd need to write a complex series of decision statements using if and else statements. youd also need to use words in the subject or body, the number of links, and the length of the message to determine if an email message is spam. it would be hard and laborintensive to build a large set of rules covering every possibility. with machine learning, however, you could use a list of email messages that were marked as spam or not spam and train a machine learning model. the model would then learn which patterns of words, length, and other attributes are good indicators of spam messages. if you presented the model with an email message it hadnt seen before, the model would perform a prediction to say whether the message was spam or not spam. deep learning represents a significant leap forward in the capabilities for artificial intelligence and machine learning. the theory behind deep learning was created from how the human brain works. an artificial neural network, or ann, is inspired by the biological neurons found in the brain, although the implementation has become very different. artificial neurons have one or more inputs and a single output. these neurons fire, or activate, their outputs based on a transformation of the inputs. a neural network is composed of layers of these artificial neurons with connections between the layers. there are typically input, output, and hidden layers in the network. the output of a single neuron connects to the inputs of all the neurons in the next layer. the network is then asked to solve a problem. the input layer is populated from the training data, and the neurons activate throughout the layers until an answer is presented in the output layer. the accuracy of the output is then measured. if the output doesnt meet your threshold, the training is repeated, but with slight changes to the weights of the connections between neurons. the neural network will do this repeatedly. each time it strengthens the connections that lead to success and diminishes the connections that lead to failure. as youll see in this course, machine learning practitioners spend a lot of time optimizing the ml models, selecting the best data features to train with, and selecting the models with the best results. in contrast, deep learning practitioners spend almost no time on those tasks. instead, they spend their time modeling data with different ann architectures. though the theory for deep learning goes back decades, the hardware needed to run deep learning problems wasnt generally accessible until recently, but now that its available, you can use deep learning to address problems that are more complex than the problems you could have worked on before. mainstream machine learning is a recent occurrence. rapid advancements in machine and deep learning only started around the mid2000s. this is partly because moores law and the rise of cloud computing resulted in easier access to larger, faster, and cheaper compute and storage capabilities. you can now rent computing power for a few hours for pennies. before this, you needed substantial investments to buy and operate largescale compute clusters on your own. in 2012, neural networks started to be used in the imagenet largescale visual recognition challenge, a machine learning competition for image recognition. the accuracy rate jumped up to about 82 and has been steadily climbing ever since. in fact, it exceeded human performance in 2015. here are some of the key takeaways for this section. first, artificial intelligence is the broad field of building machines to perform human tasks. also, machine learning is a subset of ai. it focuses on using data to train machine learning models so they can make predictions. deep learning is a technique inspired from human biology. it uses layers of artificial neurons to build networks that solve problems. and last, advancements in technology, cloud computing, and algorithm development have led to a corresponding advance in machine learning capabilities and applications. thats it for this section. well see you in the next video.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod05_Sect03_part1.txt\n",
      "\n",
      "\n",
      "Normalized Text: in this section, well look at preparing custom datasets for computer vision, so you can detect custom objects. one challenge of using a prebuilt model is that it will only find images it was trained to find. though amazon rekognition was trained with tens of millions of images, it cant detect objects that it wasnt trained on. for example, consider the 8 of hearts playing card. if you run this card through amazon recognition, the results show various attributes. however, none of the labels are playing card or 8 of hearts. if you want amazon recognition to detect images in your problem domain, you must train the model with your images. so in this section, youll learn how to train amazon recognition with images from your problem domain. though youll focus only on using amazon recognition here, youll encounter a similar process if you use other pretrained models. training a computer vision algorithm to recognize images requires a large input dataset, which isnt practical for most organizations. many machine learning problems today can be solved by training existing models, or you can use a managed service like amazon recognition custom labels. like other machine learning processes, you need to train amazon recognition so it recognizes scenes and objects that are in a specific domain. youll need both a training dataset and a test dataset that contain labeled images. if you have images that need labels, you can use amazon recognition custom labels to simplify your labeling tasks. for example, it provides a ui for labeling images, which includes a feature you can use to draw bounding boxes around images. it can also help find objects and scenes that are unique to your business needs. you can use it to classify images or detect objects within an image. say you want to identify specific machine parts in images, such as turbochargers or torque converters. you could collect pictures of each kind of machine part and use them to train your model. amazon rekognition custom labels also includes automated machine learning capabilities that handle the machine learning process for you. when you provide training images, the service can automatically load and inspect the data, select the correct machine learning algorithms, train a model, and provide model performance metrics. when you finish training your model, you can then evaluate your custom models performance on your test set. each image in the test set has a sidebyside comparison of the models prediction versus the label it assigned. there are also detailed performance metrics for you to review. you can start using your model immediately for image analysis, or you can iterate and retrain new versions with more images to refine the model. after you start using your model, you can track your predictions, correct any mistakes, and use the feedback data to retrain new model versions and improve their performance. so how do you label images? the diagram shows a typical process for training a computer vision model, which includes the amazon recognition custom labels feature. well step through this in some detail. the process of developing a custom model to analyze images requires time, expertise, and resources. it often takes months to complete. and resources. it often takes months to complete. it can also require thousands or tens of thousands of handlabeled images so the model has enough data to make accurate decisions. it can take months to generate and gather this data. and it can require large teams of labelers to prepare it for use in machine learning. amazon recognition custom labels builds on the existing capabilities of amazon recognition, which is already trained on tens of millions of images across many categories. instead of thousands of images, you can upload a small set of training images that are specific to your use case. typically, youd use a few hundred images for this. you can use the aws management console to upload training images. if your images are already labeled, amazon recognition custom labels can begin training your model. if theyre not, you can label the images directly in the labeling interface. or you can use amazon sagemaker ground truth to label them for you. therell be more on that shortly. amazon recognition custom labels works best when you use different models for different domains. for example, if you need to detect both machine parts and plant health, youd use two different models. images you select for training should be similar to the images that will be used for inference. use images that use various lighting conditions, backgrounds, and resolutions. ideally, your training images will mirror images youd want to perform detection on. if you can use the same source, like youd use in production, that works best. the documentation includes additional guidelines on image type, so whether they are jpegs or pngs, and other properties, like image size and resolution. thats it for part one of this section. well see you again for part two, where well review how to create the training dataset.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod06_WrapUp.txt\n",
      "\n",
      "\n",
      "Normalized Text: welcome back. its now time to review the module and wrap it up. in summary, in this module, you learned how to describe the nlp use cases that are solved by using managed amazon ml services and describe the managed ml services available for nlp. good job. thanks for watching. well see you in the next module. and describe the managed ml services available for nlp. good job. thanks for watching. well see you in the next module.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod01_Course Overview.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi, and welcome to amazon academy machine learning foundations. in this module, youll learn about the course objectives, various job roles in the machine learning domain, and where you can go to learn more about machine learning. after completing this module, you should be able to identify course prerequisites and objectives, indicate the role of the data scientist in business, and identify resources for further learning. were now going to look at the prerequisites for taking this course. before you take this course, we recommend that you first complete aws academy cloud foundations. you should also have some general technical knowledge of it, including foundational computer literacy skills like basic computer concepts, email, file management, and a good understanding of the internet. we also recommend that you have intermediate skills with python programming and a general knowledge of applied statistics. finally, general business knowledge is important for this course. this includes insight into how information technology is used in business. its also important to have businessrelated skill sets, such as communication skills, leadership skills, and an orientation towards customer service. leadership skills, and an orientation towards customer service. in this course, youll be introduced to the key concepts of machine learning, its tools, and its uses. youll also be introduced to, and work with, some of the aws services for machine learning. youll learn how to recognize how machine learning and deep learning are part of artificial intelligence, describe artificial intelligence and machine learning and deep learning are part of artificial intelligence, describe artificial intelligence and machine learning terminology, identify how machine learning can be used to solve a business problem, describe the machine learning process, list the tools available to data scientists, and identify when to use machine learning instead of traditional software development methods. as part of this course, youll also learn how to implement a machine learning pipeline. this includes how to formulate a problem from a business request, obtain and secure data for machine learning, build a jupyter notebook by using amazon sagemaker, outline the process for evaluating data. explain why data needs to be preprocessed. and use open source tools to examine and preprocess data. you will also use amazon sagemaker to train and host a machine learning model. use crossvalidation to test the performance host a machine learning model. use crossvalidation to test the performance of a machine learning model. use a hosted model for inference. create an amazon sagemaker hyperparameter tuning job to optimize a models effectiveness. and finally, how to use managed amazon machine learning services to solve specific machine learning problems in forecasting, computer vision, and natural language processing. well now review the course outline. to achieve the course objectives, youll complete the following modules. to start, in module 2 youll get an introduction to machine learning. in module 3, youll learn how to implement a machine learning pipeline with amazon sagemaker. modules 4, 5, and 6 describe how to apply managed amazon machine learning services for problems in forecasting, computer vision, and natural language processing. finally, module 7 is a summary of the course. it also includes an overview of steps you can take to work towards the aws certified machine learning specialty. the next five slides provide more detail about the subtopics covered in each module. the purpose of module 2 is to introduce you to major concepts for understanding machine learning. section 1 describes the overall field of machine learning and how machine learning relates to artificial intelligence and deep learning. in section 2, youll learn about some of the most common business problems you can solve with machine learning. section 3 describes the general workflow for solving machine learning problems. youll also learn some of the more common machine learning terms. in section 4, youll review some of the commonly used tools by machine learning professionals. and lastly, in section 5, youll get an overview of some of the common challenges youll face when working with machine learning problems. in module 3, youll get an introduction to amazon sagemaker and how you can use it to implement a machine learning pipeline. the module focuses on the application of machine learning to solve problems with several public domain datasets as examples of the machine learning pipeline. section 1 introduces you to defining business problems and the datasets we will use during this module. section 2 through 8 describe the phases of the machine learning pipeline by using computer vision as an example application. in section 2, youll learn how to collect and secure data. section 3 describes different techniques for evaluating data. in section 4, youll learn about the process of feature engineering. section 5 describes the steps youll take to train a model with sagemaker. in section 6, youll get an overview of the options in sagemaker for hosting and using a model. finally, sections 7 and 8 cover how to evaluate and tune your model with sagemaker. in this module, youll be introduced to using machine learning to create forecasts based on a time series data. in section 1, youll be introduced to forecasting and some of its common applications. section 2 outlines some of the pitfalls of using time series data to make forecasts. finally, in section 3, youll get an overview of how to use amazon forecast. in this module, youll learn about using machine learning for computer vision. section 1 describes the general problems you can solve with computer vision. in section 2, youll learn about the process for analyzing images and videos. and in section 3, youll learn the steps youll need to take to prepare datasets for computer vision. in this module, youll be introduced to natural language processing with machine learning. in section 1, youll learn about the general set of problems you can solve with natural language processing. section 2 reviews some of the managed amazon machine learning services you can use to address natural language processing problems. these services include amazon transcribe, amazon translate, amazon lex, amazon comprehend, and amazon polly. module 7 is the final module of the course. in this module, youll review what youve learned throughout this course. youll also be introduced to the next steps you should take if you want to achieve the aws certified machine learning specialty. section 1 of this module summarizes the topics youve covered in this course. in section 2, youll learn more about the aws documentation. youll also review two common frameworks for applying aws services. and finally, section three describes the steps you should take if you want to continue working towards the aws certified machine learning specialty. in this section, youll learn about some of the more common job roles for machine learning professionals. if youre interested in a data scientist role, focus on developing analytical, statistical, and programming skills. as a data scientist, youll use those skills to collect, analyze, and interpret large data sets. some universities now offer degrees in data science, but data scientists often have degrees in related fields like statistics, math, computer science, or economics. as a data scientist, youll need technical competencies in statistics, machine learning, programming languages, and data analytics. if youd like to have a career as a machine learning engineer, the skills youll need will be similar to a data scientists skill set. like data scientists, machine learning engineers also require technical competencies in statistics and machine learning. however, youll focus more on programming skills and software architecture than analysis and interpretation. as a machine learning engineer, youll apply those programming and architecture skills to design and develop machine learning systems. machine learning engineers often have previous experience with software development, and they rely more heavily on programming and software engineering than other machine learning roles. you might also be interested in a career in science where you can apply machine learning technology to your field. machine learning is having an impact in everything from astronomy to zoology, so there are many different paths open to you. as an applied science researcher, your primary focus will be on the type of science youre working on. youll need some of the same skills as a data scientist, but youll also need to know how to apply those skills to your chosen domain. thus, applied science roles also require technical competencies in statistics and machine learning. many software developers are now integrating machine learning into their applications. if youre interested in a career as a software developer, you should also include machine learning technology in your studies. as a machine learning developer, your primary focus will be software development skills. but youll also need some of the same skills as a data scientist, so make sure you take coursework in statistics and applied mathematics. and heres a final note for this module. we recommend reviewing your student guide. in your student guide, youll find links to documentation and other resources youll use throughout the course. thats it for this introduction. thanks for watching. well see you in the next video.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod02_WrapUp.txt\n",
      "\n",
      "\n",
      "Normalized Text: its now time to review the module. here are the main takeaways for this module. first, we looked at defining machine learning and how it fits into the broader ai landscape. we also looked at the types of problems machine learning can help us solve and how machine learning applies learning algorithms to develop models from large datasets. we then looked at the machine learning pipeline and the different stages for developing a machine learning application. finally, we introduced some of the tools and services you can use, before discussing some of the challenges with machine learning. in summary, in this module you learned how to recognize how machine learning. in summary, in this module, you learned how to recognize how machine learning and deep learning are part of artificial intelligence, describe artificial intelligence and machine learning terminology, identify how machine learning can be used to solve a business problem, describe the machine learning process, list the tools available to data scientists. identify when to use machine learning instead of traditional software development methods. thanks for watching, well see you in the next video.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod05_Sect02_part1_ver2.txt\n",
      "\n",
      "\n",
      "Normalized Text: welcome back. in this section, well explore image analysis in more detail. and in part two, well take a closer look into video analysis. to start, well introduce the main amazon service well be using, amazon recognition. amazon recognition is a computer vision service thats based on deep learning. you can use it to add image and video analysis to your applications. there are many uses for amazon recognition, including creating searchable image and video libraries. amazon recognition makes both images and stored videos searchable, so you can discover the objects and scenes that appear in them. you can use amazon recognition to build a facebased user verification system, so your applications can confirm user identities by comparing their live image with a reference image. amazon recognition interprets emotional expressions, such as happy, sad, or surprise. it can also interpret demographic information from facial images, such as gender. amazon recognition can also detect inappropriate content in both images and stored videos. and finally, amazon recognition can recognize and extract text content from images. before we go further, heres a quick note on security. you need to check if the applications you build using amazon recognition fall under any regulatory restrictions as defined in your field or country. security and compliance for amazon recognition is a shared responsibility between aws and the customer. for more information about this topic, see the aws compliance page. amazon recognition is an awsmanaged service. with a managed service, amazon hosts the machine learning models, maintains an api, and scales out to meet demand for you. you can benefit from a set of models that constantly learn and improve. also, you can focus on building applications that use the api and optionally training the service to understand your unique business needs. there are various resources you can use to access and interact with amazon recognition, such as apis, sdks, and commands for the aws command line interface, which is also known as the aws cli. the languages supported by the sdks include javascript, python, php, .net, ruby, python, php, .net, ruby, java, go, node.js, and c. finally, amazon recognition integrates with other aws services. for example, if you need storage, you can use amazons simple storage service, or s3. for authentication and authorization, you can use aws identity and access management, which is also known as iam. this diagram illustrates an image search feature where users can take pictures and get information about the real estate properties theyre viewing. first, the user takes a picture with their mobile device. the user then initiates a search, which causes the application to upload to amazon s3. s3 is configured to call other services when a write event occurs. in this case, the bucket passes the s3 path of the new object to aws lambda. when the lambda function is called, it uses the amazon recognition sdk to call the service. amazon recognition analyzes the image, detects aspects of the property, creates labels, and passes the information back to lambda as an object formatted in javascript object notation or json. lambda then stores the labels and confidence score in amazon elastic search service, which is also known as amazon es. application users can now identify aspects of a property using the objects that were detected in the image. in this example architecture, the system checks uploaded images for inappropriate content. like the previous example, processing begins when the user uploads content. first, the user uploads an image to amazon s3. second, the s3 bucket is configured to call a lambda function when an object is written to the bucket. third, lambda calls amazon recognition via the sdk. amazon recognition then analyzes the images for inappropriate content and sends the response back to lambda. fourth, if the content is appropriate, the content is approved. fifth, if the content isnt appropriate, the content can be sent for manual inspection. and finally, if the content isnt approved, a notification is sent to the user. in this final use case, the system analyzes a video feed for sentiment analysis. first, an instore camera captures video thats then sent to a back office or a cloudbased application. typically, an application like this uses amazon kinesis to stream the video. second, the application uses the sdk to send the video to amazon recognition for further analysis. visual sentiment is extracted along with other attributes such as age. third, the discovered attributes are sent to amazon kinesis. fourth, a lambda function extracts the data from the stream. fifth, the data is then written to s3. next, the data is loaded into amazon redshift on a regular basis. and finally, tools like amazon quicksight can be used to generate reports from the data. amazon recognition is designed to integrate into your applications through the api and sdks. api operations are provided for detecting labels, faces, recognizing celebrities, and detecting unsafe images. to perform a prediction, provide the service with an image object in amazon s3 or upload a byte stream of an image. images can be in jpeg or png formats. amazon recognition processes the image, performs the prediction, and returns a json object with the results. when amazon recognition performs predictions, it often returns multiple labels. each label has a confidence level. this confidence level indicates how likely the label was found in the image. like this example shows, labels can also have hierarchies. when you find instances of objects, you need to understand where the detected object is in the image. for each instance, the results from amazon recognition include a bounding box that contains the starting coordinate of top, left, and box dimensions of width, height. like the example, you can use this information to determine the location of the detected object in the image. its important to note that all findings contain a confidence score. you can use the confidence score in your applications to tune your response to predictions. with a higher score, its more likely that the object was correctly labeled. thats it for part 1 of this section. well see you again for part 2, where well explore facial detection.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod07_Sect01.txt\n",
      "\n",
      "\n",
      "Normalized Text: welcome to module 7, course wrapup. congratulations on completing the aws academy machine learning course. well take a few minutes to review what youve learned and where you can go from here. were going to start with a review of what youve learned in this course. you learned how to describe machine learning, implement a machine learning pipeline, and use amazon machine learning services for forecasting, computer vision, and natural language processing. well done. although this course isnt designed to prepare you to become certified for the aws certified machine learning specialty, well review how you can continue to work towards that certification. aws certification helps you build credibility and confidence by validating your cloud expertise with an industryrecognized credential. it also helps organizations identify skilled professionals who can lead cloud initiatives by using aws. you must earn a passing score by taking a proctored exam to earn an aws certification. after receiving a passing score, youll receive your certification credentials. aws certification doesnt publish a list of all services or features that are covered in a certification exam. however, theres an exam guide for each exam, and it lists the current topic areas and objectives covered in the exam. exam guides can be found on the prepare for your aws certification exam webpage. youll be required to update your certification or recertify every three years. view the aws certification recertification page for more details. the information on this slide is current as of june 2020. however, exams are frequently updated. also, the details regarding which exams are available and what topics are tested by each exam are subject to change. the aws certified machine learning specialty means you can select and justify the appropriate machine learning approach for a given business problem. you can also identify appropriate aws services to implement machine learning solutions. and finally, you can design and implement scalable, costoptimized, reliable, and secure machine learning solutions. before sitting for the aws certified machine learning specialty exam, we recommend that you have the following knowledge and experience. first, you should have one to two years of experience developing, architecting, or running ml or deep learning workloads on the aws cloud. your experience should include performing basic hyperparameter optimization and working with machine learning and deep learning frameworks. you should also be able to express the intuition behind basic ml algorithms. finally, you should be able to follow best practices for model training, in addition to best practices for deployment and operations. thanks for watching and congratulations on completing the aws academy machine learning course.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod05_Sect03_part2.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi, welcome back. well continue exploring video analysis by reviewing how to create the training dataset. datasets contain information thats needed to train and test an amazon recognition custom labels model, such as images, labels, and bounding boxes. such as images, labels, and bounding boxes. you can use images from amazon s3, or you can upload them from your computer to s3 as part of the process. to train a model, your dataset should have at least two labels, with at least 10 images per label. each image in your dataset must be labeled. as we mentioned earlier, you can use the amazon recognition custom labels console or amazon sagemaker ground truth to label your images. again, to train an amazon recognition custom labels model, your images must be labeled. a label indicates that an image contains an object, scene, or concept. as we mentioned earlier, a dataset needs at least two defined labels. also, each image must have at least one assigned label that identifies the object, scene, or concept in the image. when you apply labels to an image as a whole, these labels are known as imagelevel labels. theyre useful for identifying scenes or concepts that you want to detect. for example, one of the images shows a beach scene from koolina. its on the island of oahu in the us state of hawaii. to train a model to detect beaches, youd add a beach label that applies to the entire image. you can also apply labels to specific areas of an image that contain an object you want to detect. for example, if you want your model to detect amazon echo devices, it must identify the different types of echo devices in an image. the model needs information about where the devices are located in the image, and it needs a corresponding label that identifies the type of the device. this information is known as localization information. the location of the device is expressed as a bounding box. the example objects with bounding boxes image shows a bounding box that surrounds an amazon echo dot. the image also contains an amazon echo without a bounding box. the output of the labeling process will be a manifest file. the manifest file for an imagelevel label typically contains the label, or class name, along with some metadata about how the image was labeled. for object detection, the manifest contains information about each labeled image. the bounding box identifies where the object is in the image, along with the label that the bounding box belongs to. weve mentioned amazon sagemaker ground truth a few times. well now look at what it is and how it might help you. with sagemaker ground truth, you can build highquality training datasets for your machine learning models. to use it, create a dataset that needs labeling. you then provide detailed instructions on what needs to be labeled and submit the job. you can decide who processes the images to create a labeled dataset. you can use workers from the amazon mechanical turk service, a vendor company, or an internal workforce with machine learning. you can use the labeled dataset output from sagemaker ground truth to train your own models, or you can also use it with amazon rekognition custom labels. sagemaker ground truth can use active learning to automate the labeling of your input data. active learning is a machine learning technique that identifies data that should be labeled by your workers. in sagemaker ground truth this functionality is called automated data labeling. automated data labeling can reduce the time and cost it takes to label your data set compared to using only human workers. when you use automated labeling, you incur amazon sagemaker training and inference costs. yes, we just said that you can use machine learning to label the images that youll then use for machine learning. well talk through how this works. when sagemaker ground truth starts an automated data labeling job, it selects a random sample of input data, or objects, and sends it to human workers. when the labeled data is returned, sagemaker ground truth uses this data, which is the validation data, to validate the models that were trained for automated data labeling. sagemaker ground truth runs a batch transform job using the validated model for inference on the validation data. batch inference produces a confidence score and quality metric for each object in the validation data. automated labeling determines if the confidence score for each object, which was produced in step five, meets the required threshold, which was determined in step four. if the confidence score meets the threshold, the expected quality of automatic labeling exceeds the requested level of accuracy. the object is then considered to be automatically labeled. step 6 produces a dataset of unlabeled data with confidence scores. sagemaker ground truth selects data points with low confidence scores from this dataset and sends them to human workers for additional labeling. sagemaker ground truth then uses the existing humanlabeled data and the additional humanlabeled data to train a new model. the process is repeated until the dataset is fully labeled or until another stopping condition is met. for example, automatic labeling can stop when you meet your budget for human annotation. we recommend using automated data labeling on large data sets. the minimum number of objects allowed for automated data labeling is 1,250. however, we strongly suggest providing a minimum of 5,000 objects. thats it for part 2 of this section. well see you again for part 3, where well review how to evaluate and improve your model.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod02_Sect03.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi and welcome back. this is section 3 and were going to give you a quick, highlevel overview of machine learning terminology and a typical workflow. we will cover these topics in more detail later in this course, but for now well focus on the larger picture. so to begin, you should always start with the business problem you or your team believe could benefit from machine learning. from there, you want to do some problem formulation. in this phase, one task is to articulate your business problem and convert it to an ml problem. after youve formulated the problem, you move on to the data preparation and preprocessing phase. youll pull data from one or more data sources. these data sources might have differences in data or types that need to be reconciled so you can form a single, cohesive view of your data. youll need to visualize your data and use statistics to determine if the data is consistent and can be used for machine learning. well look at some of the data sources later in the course. this example data has four columns containing data from three different data sources. the sources had slightly different ways of representing data and the results are shown in the table. in ml problems, columns represent features and rows represent instances. there are some issues here with the data in some of the instances. in some cases, youll need a subject matter expert or a functional expert to understand the authenticity of the data. for example, the date thats represented as 1121969 could be november 2nd or february 11th in the year 1969. someone who owns or manages the data pool would be able to clarify this ambiguity. also, the word mail can probably be attributed to an import issue where cells shifted position. but there could be an outside chance where its the actual location, mali, a city thats the capital of the republic of maldives. at times, this error identification isnt as simple, and youll need an sme to review the data. youll learn about the role of experts later in this course. remember that one of the largest impacts you can have on the success of experts later in this course. remember that one of the largest impacts you can have on the success of a machine learning project is to have consistent and correct data. after your data is in good shape, its time to train your model. this is where the process gets very iterative and fluid. youll likely go through many multiple passes of feature engineering, training, evaluating, and tuning before you find a model that meets your business goals. feature engineering is the process of selecting or creating the features your model will be trained with. features are the columns of data you have in your dataset. the goal of the model is to correctly estimate the target value for the new data. the ml algorithm will use the features to predict the target. in this example, the target data is the average number of steps taken in a week. selecting the correct features can involve adding, removing, or calculating new features. you might want to make the data formats consistent. the consistent formats could be later used in the model, or you can make these changes for cosmetic reasons. depending on the problem you want to solve with this data, you might not even need to include the name feature in the example data. what about the country feature? if this were a traditional database, you might want to move country to a lookup table, then reference it. most ml algorithms want the data for an instance in a single row. ml algorithms need numerical data to process. you could consider turning the country text into the countrys iso code. however, the model might interpret the numerical value as having meaning, so the uks iso code value of 44 would be more significant than the iso code value of the us, which is 01. in this case, splitting the data into multiple columns is fine. this is known as categorical encoding, and youll learn about this later in the course. for other types of data, you could convert the text value into a numerical value. for example, you could use 0 or 1 to represent male or female. these numeric values can be used more easily by the model. what about the remaining features, like age, birth month, which is shown as bm in the table, or day of week, which is shown as dow. extracting the age, birth month, and day of week might be appropriate depending on the problem youre trying to solve. does age impact the target variable? what about which day of the week they were born on? dont worry if this sounds complicated. youll learn more about feature engineering later in this course. after your data is cleaned and youve identified the features you want to use, its time to train a model. you wont use all the data to train your model. in fact, you need to hold some of the data so you have some data to test with. typically, youll use about 80 of the data to train with, and youll save the rest of the data for testing. next, youll train a model with training data. in the diagram, the model uses the xgboost algorithm. the model itself has some parameters you can set. these parameters will alter how the algorithm works and theyre known as hyperparameters. the output of the training job will be a trained model. with the trained model, you can use some of the test data to see how well the model performs. youll take an instance the model hasnt seen and use it to perform a prediction. because you already know the target in your test data, you can compare the two values. from these comparisons, you can calculate metrics, which give you data on how well the model is performing. youll then make changes to the models data, features or hyperparameters, until you find the model that yields the best results. when training your model, theres a real danger of overfitting or underfitting the model. your model is overfitting your training data when you see the model performs well on the training data but doesnt perform well on the evaluation data. this is because the model is memorizing the data it saw and cant generalize to unseen examples. your model is underfitting the training data when the model performs poorly on the training data. this is because the model cant capture the relationship between the input examples, which are often called x, and the target values, which are often called y. understanding model fit is important for understanding the root cause of poor model accuracy. this understanding will guide you to take corrective steps. you can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction error on the training data and the evaluation data. well show you steps you can take to avoid this later in this course. after youve retrained the model and youre satisfied with the results, you deploy your model to deliver the best possible predictions. later in this course, well walk you through these different phases and give you handson experience with each of them. but knowing the process is also useful when using the managed services well also explore later, where certain amazon ml services will do the bulk of the work for you. here are some of the key takeaways for this section. first, we looked at how the machine learning pipeline process can guide you through the process of training and evaluating a model. the iterative process can be broken into three broad steps, data processing, model training, and model evaluation. thats it for this video. well see you in the next one.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod03_Sect08.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi, and welcome back to module 3. this is section 8. in this section, were going to take a look at how you can tune the models hyperparameters to improve model performance. recall from an earlier module that hyperparameters can be thought of as the knobs that tune the machine learning algorithm to improve its performance. now that were looking more explicitly at tuning models, its time to look more specifically at the different types of hyperparameters and how to perform hyperparameter optimization. there are a couple of different categories of hyperparameters. the first kind are model hyperparameters. the first kind are model hyperparameters. they help define the model itself. as an example, consider a neural network for a computer vision problem. for this case, additional attributes of the architecture need to be defined, like filter size, pooling, and the stride or padding. the second kind are optimizer hyperparameters. they relate to how the model learns patterns based on data, and theyre used for a neural network model. these types of hyperparameters include optimizers like gradient descent and stochastic gradient descent. they can also include optimizers that use momentum, like atom, or that initialize the parameter weights with methods like xavier initialization or he initialization. the third kind are data hyperparameters. they relate to the attributes of the data itself. these include attributes that define different data augmentation techniques, like cropping or resizing for imagerelated problems. theyre often used when you dont have enough data or enough variation in your data. tuning hyperparameters can be very laborintensive. used when you dont have enough data or enough variation in your data. tuning hyperparameters can be very labor intensive. traditionally this was done manually by someone who had domain experience related to the hyperparameter and the use case. this person would manually select the hyperparameters based on their intuition and experience. then they would train the model and score it on the validation data. this process would be repeated over and over again until they achieved satisfactory results. this manual process isnt always the most thorough and efficient way of tuning hyperparameters. with sagemaker, you can perform automated hyperparameter tuning with amazon sagemaker automatic model tuning. it finds the best version of a model by running multiple training jobs on your dataset by using the algorithm and hyperparameter ranges you specify. it then chooses the hyperparameter values that results in a model that performs the best as measured by a metric you choose. it uses gaussian process regression to predict which hyperparameter values might be most effective at improving fit. it also uses bayesian optimization to balance exploring the hyperparameter space and exploiting specific hyperparameter values when appropriate. and importantly, automatic model tuning can be used with builtin algorithms from sagemaker, prebuilt deep learning frameworks, and bring your own algorithm containers. suppose that you want to solve a binary classification problem on a fraud data set. your goal is to maximize the area under the auc curve metric of the algorithm by training a linear learner algorithm model. you dont know which values of the learning rate, beta 1, beta 2, and epochs you should use to train the best model. to find the best values for these hyperparameters, you can specify ranges of values that sagemaker hyperparameter tuning will then search. it will find the combination of values that results in the training job that performs the best, as measured by the objective metric that you chose. in the example, sagemaker hyperparameter tuning launches training jobs that use hyperparameter values in the ranges you specified, and then returns the training job with the highest auc. hyperparameter tuning might not necessarily improve your model. its an advanced tool for building machine solutions. as such, it should be considered part of the scientific method process. when you build complex machine learning systems, like deep learning neural networks, exploring all possible combinations is impractical. to improve optimization, use the following guidelines when you create hyperparameters. first, instead of using all hyperparameters, limit the number of hyperparameters to the ones you think would give you good results. the range of values for the hyperparameters you choose to search can significantly affect the success of hyperparameter optimization. although you might want to specify a large range that covers every possible value for a hyperparameter, youll get better results by limiting your search to a small range of values. if you get the best metric values within a part of a range, consider limiting the range to only that part. during hyperparameter tuning, sagemaker attempts to figure out if your hyperparameters are logscaled or linearscaled. initially, it assumes that hyperparameters are linearscaled. if they should be logscaled, it might take time for sagemaker to discover that on its own. if you know that a hyperparameter should be log scaled and you can convert it yourself, doing so can improve hyperparameter optimization. running more hyperparameter tuning jobs concurrently gets more work done quickly, but a tuning job improves only through successive rounds of experiments. typically, running one training job at a time achieves the best results with the least amount of compute time. say that you have a distributed training job that runs on multiple instances. in this case, hyperparameter tuning uses the last reported objective metric from all instances of that training job as the value of the objective metric for that training job. design distributed training jobs so that they report the objective metric you want. now that youve gone through the endtoend process of training and tuning a machine learning model, its worth talking about amazon sagemaker autopilot. this service can help you find a good model with little effort or input on your part. with autopilot, you create a job that supplies the test, training, and target. autopilot will analyze the data, select appropriate features, and then train and tune the models. it will document the metrics and find the best model based on the provided data. the results include the winning model and metrics and a jupyter notebook you can use to investigate the results. although using autopilot doesnt remove your need to preprocess the data, it can save you time during feature selection and model tuning. some key takeaways from this section of the module include these points. first, model tuning is important for finding the best solution to your business problem. hyperparameters can be tuned for the model, optimizer, and data. sagemaker can perform automatic hyperparameter tuning. and finally, overall model development can be accelerated by using autopilot. thats it for this video, see you in the next one.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod03_Sect04_part1.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi, and welcome to section 4. in this section, were going to look at feature engineering. feature engineering is one of the most impactful things you can do to improve your machine learning model. well now look at what it is. there are two things that can help make your models more successful. the first is feature selection and the second is feature extraction or the process of creating features. in feature selection you select the most relevant features and discard the rest. you can apply feature selection to prevent redundancy or irrelevance in the existing features. you can also use it to limit the number of features to help prevent overfitting. feature extraction builds valuable information from raw data by reformatting, combining, and transforming primary features into new ones. this process continues until it yields a new data set that can be consumed by the model to achieve your goals. as the diagram shows, feature extraction covers a range of activities, from dealing with missing data to converting text data into numerical data. although the list isnt exhaustive, it should give you some idea of the data handling thats needed to get data into a useful state. many of the tasks are no different than any other job working with data. youll want to make sure data is in the correct format, that its consistently represented, correctly spelled, among other tasks. for example, you might combine data or extract data into multiple columns. or you could also remove columns altogether. specific to machine learning, youll need to convert text columns to numerical values. youll also need to decide how to handle outliers and potentially rescale your data. next, well look at some of the more common tasks in this section. most machine learning algorithms work best with numerical data. youll need to make sure that all columns in your dataset contain numeric data by converting or encoding it. you might need to make several passes through the data sheet before you can encode it. for example, you might have variability in the text values, such as rows that contain both medium and med as values. if the categorical data has order to it, youll want to encode the text into numerical values that capture this ordinal relationship. say you have data showing maintenance costs. you might encode low to 1, medium to 2, high to 3, and very high to 4. to 4. after youve made sure your categorical data is all uniform, you can use tools like skykit learn and pandas to encode your data. if the categorical data doesnt have any order to it, then youll need to break the data into multiple columns. this will help make sure you dont introduce an ordinal relationship to the data that isnt there. for example, suppose you assigned a value of 1 to the first color, such as red, and you then assigned 2 to the next value, say blue. the model could interpret blue as being more important than red because blue has a higher numeric value. encoding nonordinal data into multiple columns or features is a better way. think of the new features like a checkbox. consider the example. there are three features that were generated. the value 1 indicates that the instance has that feature, like its color. thats it for this section. well see you again in the next video.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod03_Sect01.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi, and welcome back to module 3. this is section 1, and were going to take a look at some of the data sets well use in this module. well also look at guidance for how to formulate a business problem. before we get started, heres a reminder of the machine learning pipeline we looked at in the previous module, and how that maps to the sections in this module. this section, section 1, will cover how to formulate a problem. it will also cover the datasets well use throughout this module. section 2 will discuss how to obtain and secure data for your machine learning activities. in section 3, well show you tools and techniques for gaining an understanding of your data. then in section 4, well look at preprocessing your data so its ready to train a model. section 5 will cover selecting and training an appropriate machine learning model. section 6 will show you how to deploy a model so you can make a prediction. section 7 will examine the process of evaluating the performance of a machine learning model. and finally, in section 8 well look at tuning the model. the machine learning pipeline is an iterative process. when you work on a realworld problem, you might find yourself iterating many times before you arrive at a solution that meets your businesss needs. in this first section, well examine how to think about turning a business requirement into a machine learning problem. the first step in this phase is to simply define the problem you want to solve and the goal you want to reach. understanding the business goal is key because youll use it to measure the performance of your solution. its not unusual to solidify the business problem before you can begin targeting a solution. there are a lot of other questions you could ask to develop a good understanding of the problem. with more information about the problem, you can begin framing an approach. first, can the problem even be solved by machine learning? or would a traditional approach make more sense? is this a supervised or unsupervised machine learning problem? do you have labeled data to train a supervised model? there are many questions you could ask yourself and the business. ultimately, you should try to validate the use of machine learning, and you should make sure you have access to the right people and data. you should also try to come up with the simplest solution to the problem. heres an example. you want to identify fraudulent credit card transactions so you can stop the transaction before it processes. thats your problem. now whats the business goal or outcome driving this problem statement? in this case, say that the intended outcome is a reduction in the number of customers who end their membership to the credit card as a result of a fraudulent transaction. from a business perspective, how do you define success given this problem and the desired outcome? this is the stage where you need to move from qualitative statements to quantitative statements that can be easily measured. continuing with the example, a metric you could use to define success for this problem might be a 10 reduction in the number of customers who file claims for fraudulent transactions within a sixmonth period. so now youve defined the business side of your problem. its time to start thinking about this in terms of your machine learning model itself. whats the actual output you want to see from your model? you want to be specific here. it should be a statement that reflects what an ml model could actually output. an example might be, the model will output whether or not a credit card transaction is fraudulent or not fraudulent. now that you know what you want your ml model to actually achieve, you can use this information to determine the type of ml youre working with. you can use this information to determine the type of ml youre working with. if you have historical data where customers filed reports for fraud transactions, you can use this data for your machine learning purpose. this historical data falls under the supervised learning approach, where the labels are already defined. recall from earlier in this course that supervised ml types are categorized into two groups, classification and regression. in the credit card example, the desired output of categorizing a transaction is fraud or not fraud. so you can see that youre dealing with a binary classification problem. throughout this module, youll see several datasets being used. you can access these datasets and many more from the uc irvine machine learning repository. the first dataset contains numerical information about the composition of wine along with the quality of the wine. the question you might want to ask on this dataset is, based on the composition of the wine, could we predict the quality and therefore the price? in addition to that question, well also use this dataset to view statistics, deal with outliers, and scale numerical data. the second dataset is a car evaluation database. this dataset is heavily textbased. this enables you to explore the encoding categorical data, which converts the text values into numbers that can be processed by machine learning. the third dataset is a biomedical dataset, which youll also use in the labs. the question to answer for this dataset is, based on the biomechanical features, can you predict if a patient has an abnormality? this dataset will take you through the entire endtoend process. youll end with a trained model thats been tuned and that you can use to make a prediction. in this section, we looked at how business problems need to be converted into an ml problem. we also looked at some of the key questions to ask, which are, what is defining success? can you measure the outcome or impact if your solution is implemented? most business problems fall into one of two categories. the first category is classification, which can be binary or multiclass. ask yourself, does the target belong to a class? and the second category is regression. for this, ask yourself, can you predict a numerical value? thats it for section one. well see you in the next video.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod04_Intro.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi, and welcome to module 4 of aws academy machine learning. in this module, were going to look at forecasting. well start with an introduction to forecasting and look at how time series data is different from other kinds of data. then, were going to look at amazon forecast, a service that helps you simplify building forecasts. at the end of this module, youll be able to describe the business problem solved with amazon forecast, describe the challenges of working with time series data, list the steps required to create a forecast by using amazon forecast. and use amazon forecast to make a prediction. see you in the next video!\n",
      "\n",
      "\n",
      "\n",
      "File: Mod06_Sect02.txt\n",
      "\n",
      "\n",
      "Normalized Text: welcome back. in this section, well review five managed machine learning services you can use for various use cases. these services simplify the process of creating a machine learning application. well start by looking at amazon transcribe. you can use amazon transcribe to recognize speech in audio files and produce a transcription. it can recognize specific voices in an audio file, and you can create a customized vocabulary for terms that are specialized for a particular domain. you can also add a transcription service to your applications by integrating with websockets, a transcription service to your applications by integrating with websockets, an internet protocol you can use for twoway communication between an application and amazon transcribe. here are some of the more common use cases for amazon transcribe. first, medical professionals can record their notes, and amazon transcribe can capture their spoken notes as text. also, video production organizations can generate subtitles automatically from video. this could also be done in real time for a live feed to add closed captioning. media companies can use amazon transcribe to capture and label content. they can then feed the content into amazon comprehend for further analysis. last, companies can record customer service or sales calls and transcribe them. they can analyze the results for training or for strategic opportunities. amazon polly can convert text into lifelike speech. you can input either plain text files or a file thats formatted in speech synthesis markup language, or ssml. ssml is a markup language used to provide special instructions for how speech should sound. for example, if you want to introduce a pause in the flow of speech, you can add an ssml tag that instructs amazon polly to pause between two words. you can also output speech from amazon polly to mp3, vorbis, and pcm audio stream formats. amazon polly is eligible for use with certain regulated workloads. for example, its eligible for use with the u.s. health insurance portability and accountability act of 1996, or hipaa. amazon polly is also eligible for use with payment card industry data security standard, or pci dss. here are some of the more common use cases for amazon polly. as a first example, major news companies are using amazon polly to generate vocal content directly from their written stories. its also been embedded in mapping apis so developers can add voice to their geobased applications. language training companies have used amazon polly to create systems for learning a new language. finally, animators have used it to add voices to their characters. with amazon translate, you can create multilanguage experiences in your applications. you can create systems that read documents in one language and then render or store them in another language. you can also use it as part of a document analysis system. amazon translate is fully integrated with other machine learning services, such as amazon comprehend, amazon transcribe, and amazon polly. with this integration, you can extract named entities, sentiment and key phrases by integrating it with amazon comprehend, create multilingual subtitles with amazon transcribe, and speak translated content with amazon polly. here are some of the more common use cases for amazon translate. the first use case is building international websites. you can use amazon translate to quickly globalize your websites. amazon translate can also be used to develop multilingual chatbots. chatbots are used to create a more humanlike interface to applications. with amazon translate, you can create a chatbot that speaks multiple languages. another use case is software localization. localization is a major cost for all software aimed at a global audience. amazon translate can decrease software development time and significantly reduce costs for localizing software. the final example use case is international media management. companies that manage media for a global audience have used amazon translate to reduce their costs for localization. amazon comprehend implements many of the nlp techniques that we reviewed earlier in this module. you can extract key entities, perform sentiment analysis, and tag words with parts of speech. here are some of the more common use cases for amazon comprehend. the first example is analyzing legal and medical documents. is analyzing legal and medical documents. legal, insurance, and medical organizations have used amazon comprehend to perform many of the nlp functions we reviewed in this module. another use is for largescale mobile app analysis. mobile app developers use amazon comprehend to look for patterns of usage with their apps so they can design improvements. financial fraud detection is another use case for amazon comprehend. banking, financial, and other institutions have used it to examine very large data sets of financial transactions to uncover fraud and look for patterns of illegal transactions. finally, it can be used for content management. media and other content companies can use amazon comprehend to tag content for analysis and management. with amazon lex, you can add a human language frontend to your applications. amazon lex lets you use the same conversational engine that powers amazon alexa. you can automatically increase capacity for your amazon lex solution by creating aws lambda functions that scale on demand. you can also store log files of the conversations for further analysis. here are some of the more common use cases for amazon lex. the first use case is building frontend interfaces for inventory management and sales. voice interfaces are becoming more common. companies have used amazon lex to add chatbots to their inventory and sales applications. another use for amazon lex is creating customer service interfaces. humanlike voice applications are quickly becoming the standard for many customer service applications. amazon lex can reduce the time it takes to develop these chatbots and increase their quality. amazon lex can also be used to develop interactive assistance. by combining amazon lex with other ml services, customers are creating more sophisticated assistance for many different industries. the final example use case is querying databases with a humanlike language. amazon lex has been combined with other aws database services to create sophisticated data analysis applications with a humanlike language interface. here are some of the main points you should take away from this module. first, amazon transcribe can automatically convert spoken language to text. amazon polly can convert written text to spoken language. amazon translate can create realtime translation between languages. amazon comprehend automates many of the nlp use cases reviewed in this module. and finally, amazon lex can create a humanlike interface to your applications. thanks for watching. well see you in the next video.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod03_Sect02_part1.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi, welcome back. were now going to look at a few ways you can collect and secure data. in this section, well explore some of the techniques and challenges associated with collecting and securing the data thats needed for machine learning. consider again the original example about predicting credit card fraud. youve further formulated the problem. but what data do you need to actually train your model so you can get the desired output and subsequently achieve your intended business outcome? do you have access to the data? if so, how much data do you have and where is it? what solution can you use to bring all this data into one centralized repository? the answers to these questions are essential at this stage. the good news for a budding data scientist is that there are many places where you can obtain data. private data from you or your existing customer already exists, including everything from log files to customer invoice databases. private data can be useful depending on the problem youre trying to solve. in many cases, private data is found in many different systems. well look at how to bring these sources together shortly. sometimes, you want to use data that was collected and made available by a commercial organization. you want to use data that was collected and made available by a commercial organization. companies such as reuters, change healthcare, dun bradstreet, and foursquare maintain databases you can subscribe to. they include curated news stories, anonymized healthcare transactions, global business records, and location data. if you supplement your own data with commercial data, you can get useful insights you wouldnt have gotten otherwise. there are also many open source data sets, ranging from wine quality to movie reviews. these data sets are made available for use in research or for teaching purposes. aws, kaggle, and the uci machine learning repositories are good places to find open source data sets. government and health organizations are other sources of data that could be useful. supervised machine learning problems need a lot of data. these are also called observations, and you already need to know the target answer or prediction for that data. these are also called observations, and you already need to know the target answer or prediction for that data. this kind of data, where you already know the target answer or prediction, is called labeled data. each observation in your data is made up of two elements, the target and the features. the target is the answer you want to predict. so in the credit card transaction example, the target of any given observation is either fraud or not fraud. a feature is an attribute of the example that you can use to identify patterns for predicting the target answer. a feature in the credit card example could be the date of the transaction, the vendor, or the amount in dollars of the transaction. you might wonder if the source of the target is fraud or not fraud. typically, this information is discovered only after the transaction is complete and the actual card owner notices a fraudulent transaction on their statement. this information would be recorded with the transaction for exactly the purpose of using it to train a future model. so given what you know about the elements of an ml dataset, well return to one of the original questions. what data do you need to actually train your model to reach the desired output and subsequently your intended business outcome? this is an example of a stage in the ml pipeline when its crucial to get domain expertise to help you answer this question. with domain knowledge, you can start determining the features and target data your model will need to make accurate predictions. your data should be representative of the data youll have when youre using the model to make a prediction. for example, if you want to predict credit card fraud, you need to collect data for positive or fraudulent transactions. you also need to collect data for negative or nonfraudulent transactions. you need both types of data so the machine learning algorithm can find patterns that will distinguish between the two types. suppose your average amount of fraudulent transactions is actually 3, but your training dataset only includes a very small fraction of fraudulent observations, say 0.4. in this case, itll be difficult for your model to truly learn patterns related to fraudulent transactions that it might encounter in production. there are many different services in aws where you could find or store your data. here are some key services you might use. amazon simple storage service is also known as amazon s3. it provides objectlevel storage. with s3, you can store as much data as you want in the form of objects, which you can think of as files. they could be csv files or files of other formats you need. s3 can be accessed through the webbased aws management console. you can also access s3 programmatically through the api and sdks or with thirdparty solutions, which also use the api and sdks or with thirdparty solutions, which also use the api and sdks. if your training data is already in s3 and youre planning to run training jobs several times with different algorithms and parameters, you could use amazon fsx for lustre. its a file system service that speeds up your training jobs by serving your s3 data to amazon sagemaker at high speeds. the first time you run a training job, fsx for lustre automatically copies data from s3 and makes it available to sagemaker. you can use the same amazon fsx file system for subsequent iterations of training jobs, which prevents repeated downloads of common s3 objects. alternatively, your training data might already be in amazon elastic file system or amazon efs. if so, we recommend using efs as your data source for training data. it can launch your training jobs directly from the service without needing data movement, which results in faster training start times. this is often the case in environments where data scientists have home directories in amazon efs. they can quickly iterate on their models by bringing in new data, sharing data with colleagues, and experimenting with different fields or labels in their data set. for example, a data scientist can use a jupyter notebook to do an initial cleansing on a training set and launch a training job from amazon sagemaker. they could then use their jupyter notebook to drop a column and relaunch the training job and finally compare the resulting models to see which one works better. there are many other aws services and resources where you might find data. for example, you could use amazon relational database service or amazon rds, a managed relational database service. you could also use amazon redshift, which is a managed data warehouse service. another option is amazon timestream, a managed time series database designed specifically to handle large amounts of data from the internet of things, or iot. you could even spin up your own instances on amazon elastic compute cloud, which is also known as amazon ec2, and host your own database on these instances. when you have data sources, youll need to extract useful data from these sources when assembling your data for machine learning. well look at this next. thats it for part one of this section. well see you again for part two where well review how to extract, transform, and load data.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod03_Sect03_part3.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi, welcome back. now well review how to find correlations in your dataset. how can you quantify the linear relationship among the variables youre seeing in a scatterplot? a correlation matrix is a good tool in this situation. it conveys both the strong and weak linear relationships among numerical variables. correlation can go as high as 1 or as low as minus 1. when the correlation is 1, this means those two numerical features are perfectly correlated with each other. its like saying y is proportional to x. when the correlation of those two variables is minus one, its like saying y is proportional to minus x. any linear relationship in between can be quantified by the correlation. so if the correlation is zero, this means theres no linear relationship. but it doesnt mean that theres no relationship, its just an indication that theres no linear relationship between those two variables. however, looking at a number isnt always straightforward. often its easier to view the numbers when theyre represented by colors. now well look at the heat map. the highest number, 1, in dark green, and minus 1 is in dark brown. the color gives you both the positive and negative directions, and it also shows how strong the correlations are. we can use the seaborne heat map function to show the correlation matrix. looking at the chart, theres some correlation between citric acid and fixed acidity. that would be expected in wine because citric acid contributes to the acidity of the wine. however, there isnt much correlation between fixed acidity and ph. ph is a measurement of the strength of those acids present, but fixed acidity is a measure of the quantity. in this particular dataset, there doesnt appear to be a correlation here. some key takeaways from this section of the module include these points. the first step is to get your data into a format that can be used easily. pendis is a popular python library for working with data. descriptive statistics will help you gain insights into the data. you can use visualizations to examine the dataset in more detail. thats it for this section, well see you again in the next video.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod04_Sect01.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi, and welcome to section 1. well get started by reviewing what forecasting is and some use cases for it. forecasting is an important area of machine learning. its important because there are so many opportunities for predicting future outcomes based on historical data. many of these opportunities involve a time component. however, while the time component adds additional information, it also makes time series problems more difficult to handle compared to other types of predictions. you can think of time series data as falling into two broad categories. the first type is univariate data, which means theres just one variable. the second one is multivariate data, which means theres more than one variable. there are several common patterns in time series data. the first pattern is a trend. with a trend, you get a pattern with the values increasing, decreasing, or staying the same over time. there are seasonal patterns. these reflect times of the year, month, day, or other patterns. cyclical patterns are similar to seasonal patterns. these are patterns that repeat, like a large retail sale event that happens the same time each year. finally, there are changes in the data over time that appear to be random or that have no discernible pattern. there are many uses for forecasting. you can use forecasting in marketing applications, such as for sales forecasting or demand projections. it could also be used in inventory management systems that anticipate required inventory levels. forecasting energy consumption can help predict when and where energy is needed. and weather forecasting systems can be used for governments and commercial applications, such as agriculture. thats it for this section. see you in the next video.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod02_Intro.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi, and welcome to module 2 of aws academy machine learning. in this module, were going to introduce machine learning. well first look at the business problems that can be solved by machine learning. well then talk about terminology, process, tools, and some of the challenges youll face. process, tools, and some of the challenges youll face. after completing this module, you should be able to recognize how machine learning and deep learning are part of artificial intelligence, describe artificial intelligence and machine learning terminology, identify how machine learning can be used to solve a business problem, describe the machine learning process, list the tools available to data scientists, and identify when to use machine learning instead of traditional software development methods. youre now ready to get started with section 1. see you in the next video.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod02_Sect02.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi and welcome back. in this section, were going to look at the types of business problems machine learning can help you solve. machine learning is used all across your digital lives. your email spam filter is the result of a machine learning program that was trained with examples of spam and regular email messages. based on books youre reading or products you bought, machine learning programs can predict other books or products youre likely to be interested in. again, the machine learning program was trained with data from other readers habits and purchases. when detecting credit card fraud, the machine learning program was trained on examples of transactions that turned out to be fraud, along with normal transactions. you can probably think of many more examples, from social media applications, using facial detection to group your photos, to detecting brain tumors in brain scans or finding anomalies in xrays. there are three main types of machine learning. theres supervised learning, where a model uses known inputs and outputs to generalize future outputs. theres unsupervised learning, where the model doesnt know inputs or outputs, so it finds patterns in the data without help. and theres reinforcement learning, where the model interacts with its environment and learns to take actions that will maximize rewards. its important to know the different types of ml because the type will guide you towards selecting algorithms that make sense for solving your business problem. lets look more into each of these types. supervised learning is a popular type of ml because its widely applicable. its called supervised learning because there needs to be a supervisor, a teacher, who can show the right answers, so to speak. like any student, a supervised algorithm needs to learn by example. essentially, it needs a teacher who uses training data to help it determine the patterns and relationships between the inputs and outputs. if you want to build an application to detect credit card fraud, youd need training data that includes examples of fraud and examples of normal transactions. within supervised learning, there are different types of problems, within supervised learning, there are different types of problems, classification and regression. there are two subtypes of classification problems. the first is binary classification. think back to the example with identifying fraudulent transactions. the target variable in this example is limited to two options, fraudulent or not fraudulent. this is a binary classification problem. there are also multiclass classification problems. these ml problems classify an observation into one of three or more categories. say that you have an ml model that predicts why a customer is calling your store so you can reduce the number of transfers needed before the customer gets to the correct customer support department. in this case, the different customer support departments represent the variety of potential target variables, which could be many different departments, much more than just two. there are also regression problems. in a regression problem, youre no longer mapping an input to a defined number of categories. instead, youre mapping an input to a continuous value, like an integer. one example of an ml regression problem is predicting the price of a companys stock. computer vision is a good example of supervised learning. is this a cat or a dog? is there a tumor in this xray? computer vision is often built with deep learning models. it automates the extraction, analysis, classification, and understanding of useful information from a single image or a sequence of images. from a single image or a sequence of images. computer vision enables machines to identify people, places, and things in images with accuracy at or above human levels, and with greater speed and efficiency. the image data can take many forms, such as single images, video sequences, views from multiple cameras, or threedimensional data. youll learn more about computer vision later in this course. well now discuss unsupervised machine learning. sometimes all you have is the data. theres no supervisor in the room. in unsupervised learning, labels arent provided like they are with supervised learning. you dont know all the variables and patterns. in these instances, the machine has to uncover and create the labels itself. these models use the data theyre presented with to detect emerging properties of the entire dataset. then they construct patterns from those properties. clustering is a common subcategory of unsupervised learning. this kind of algorithm groups data into different clusters based on similar features. it does this to better understand the attributes of a specific cluster. for example, by analyzing customer purchasing habits, unsupervised algorithms can identify groups of customers that are associated with the size tier of a company. the advantage of unsupervised algorithms is that they enable you to see patterns in the data that you werent aware of before. natural language processing is also known as nlp. this is another area of machine learning thats experiencing growth. this is another area of machine learning thats experiencing growth. if youve ever used alexa or any other voice assistant, theyll use nlp to try and answer your question. nlp isnt just about speech. its also about written text. nlp shows up in many applications. for example, nlp is used with chat or call center bots, which are automated systems that help you get your bank balance or order food from a restaurant. you can use nlp in translation tools, which convert text between languages. for example, you might use applications that translate menus in real time. nlp is also used in voicetotext translations, which convert spoken words into text. used in voicetotext translations, which convert spoken words into text. and finally, nlp can be used in sentiment analysis, which you can use to analyze the sentiment of comments and reviews of products, music, and movies. these sentiments could be used to give the movie an audience rating. youll learn more about nlp later in this course. another kind of machine learning thats been gaining popularity recently is reinforcement learning. unlike other machine learning, reinforcement learning continuously improves its model by mining feedback from previous iterations. in reinforcement learning, an agent continuously learns, through trial and error, as it interacts in an environment. reinforcement learning is broadly useful when the reward of a desired outcome is known, but the path to achieving it isnt. and that path requires a lot of trial and error to discover. take the example of aws deepracer. in the aws deepracer simulator, the agent is the virtual car. the environment is a virtual racetrack. the actions are throttle and steering inputs to the car. and the goal is completing the racetrack as quickly as possible without deviating from the track. the car needs to learn the desired driving behavior to reach the goal of completing the track. for the car to learn this, aws deepracer teams use rewards to incentivize their model to learn the desired driving behavior. in reinforcement learning, the thing driving the learning is called the agent. in this case, its the aws deepracer car. the environment is the place where the agent learns, which in this example would be the marked racetrack. when the agent does something in the environment that provokes a response, such as crossing a boundary it shouldnt cross, thats called an action. that response is called a reward or penalty depending on whether the agent did something to be reinforced or discouraged in the model. as the agent moves within the environment, its action should start receiving more rewards and fewer penalties until it meets the desired business outcome. selfdriving vehicles bring together many machine and deep learning algorithms and models to solve the problem of driving from point a to point b. two of its main tasks are the continuous detection of the environment and forecasting changes. these involve detecting objects and localizing and predicting the movement of the detected objects. the outputs of these findings act as inputs to other systems that make decisions on what they should do with the vehicles various controls. there are use cases in selfdriving vehicles that require realtime responses to the environment. for example, if a previously hidden pedestrian walks out from behind an obstacle, the vehicle brakes need to be applied immediately. there can be no latency or room for error with these actions. not every problem should be solved with machine learning. sometimes regular programming will work well for your needs. if youre interested in exploring a potential machine learning solution, look for the existence of large datasets and a large number of variables. machine learning is often the best choice if youre uncertain of the business logic or procedures needed to obtain an answer or accomplish a task. machine learning systems can be complex. the supporting infrastructure, management support, and technical expertise need to be in place to help ensure the projects success. here are the key takeaways for this section, where we explored some machine learning applications that are already part of everyday life. first, machine learning problems can be grouped into three categories. supervised learning is where you have training data where you already know the answer. unsupervised learning is where you have data but are looking for insights within the data. reinforcement learning is where the model learns based on experience and feedback. most business problems are supervised learning problems. thats it for this section, well see you in the next video.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod03_Sect05.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi, welcome back to module 3. this is section 5 on training. in this section, were going to look at how to select a model and train it with the data we have preprocessed. at this point, youve done a lot to clean and prepare your data, but that doesnt mean your data is completely ready to train the algorithm. some algorithms may not be able to work with training data in a data frame format. some file formats, like csv, are commonly used by various algorithms, but they do not make use of that optimization that some of the file formats, like recordio protobuf, can use. many amazon sagemaker algorithms support training with data in a csv format. amazon sagemaker requires that a csv file doesnt have a header record and that the target variable is in the first column. most amazon sagemaker algorithms work best when you use the optimized protobuf record io format for the training data. using this format allows you to take advantage of pipe mode when training the algorithms that support it. in pipe mode, your training job streams data directly from amazon s3. when using the csv format, the target variable in your training dataset should be the first column on the left and your features should be to the right of the target variable column. evaluating a model with the same data that it trained on will lead to overfitting. recall overfitting is where your model learns the particulars of a dataset too well. its essentially memorizing the training data, rather than learning the relationships between features and labels. this means the model isnt learning from those relationships and patterns to apply them to new data in the future. holdout is when you split your data into multiple sets, commonly sets for training data, validation data, and testing data. training data, which includes both features and labels, feeds into the algorithm youve selected to produce your model. you then use the model to make predictions over the validation dataset, which is where youll likely notice things youll want to tweak, and tune, and change. then, when youre ready, you run the test dataset, which only includes features since you want the labels to actually be predicted. the performance you get here with the test dataset is what you can reasonably expect to see in production. a common split when using the holdout method is using 80 of the data for a training set, 10 for validation, and 10 for test. or if you have a lot of data, you can split it into 70 training, 15 validation, and 15 test. so for a small dataset, we can use kfold crossvalidation to utilize as much of the data as possible, while still having relatively good metrics, in order to choose which model is better. kfold crossvalidation randomly partitions the data into k different segments. for each segment, well use the rest of the data outside of it for training in order to do a validation on that particular segment. lets look at an example. here we have a 5fold crossvalidation. the available training data is separated into five different chunks. for the training of the first model, we are using all those chunks as the training data, and then we are going to calculate the metrics on this test piece. for the second model, we are going to use these pieces as training. after the model is trained, you apply it to this test piece. we do the same thing five times. we use all the training data and we test it on five different models on different chunks of the test data, eventually testing it on all data points. one other thing to note about splitting your data, data in a specific order can lead to biases on your model. this is especially true if youre working with structured data. for example, the wine data is ordered by the quality column. when you run your model against your test data, this ordered pattern will be applied, biasing the model. it might also mean that some targets are missing from the training data. typically, randomizing your data set prior to splitting is sufficient, and many libraries will provide functions for this. with smaller sets, it is sometimes useful to use stratified sampling. stratified sampling ensures that the training and test sets have approximately the same percentage of samples of each target class as the complete set. an internet search will give you many ways to shuffle and split the data. one of the easiest is to use the train test split function from sklearn. amazon sagemaker provides four different ways you can train models. the builtin algorithms available can be easily deployed from the aws console, cli, or a jupyter notebook. containers are used behind the scenes when you use one of the amazon sagemaker builtin algorithms, but you do not have to deal with them directly. amazon sagemaker supported frameworks provide prebuilt containers to support deep learning frameworks such as apache mxnet, tensorflow, pytorch, and chainer. it also supports machine learning libraries such as skykit learn and sparkml by providing prebuilt docker images. if you use the amazon sagemaker python sdk, they are deployed using their respective amazon sagemaker sdk estimator class. if there is no prebuilt amazon sagemaker container image that you can use or modify for an advanced scenario, you can package your own script or algorithm to use with amazon sagemaker. you can use any programming language or framework to develop your container. for an example, if your team works and builds ml models in r, you can build your own containers to train and host an algorithm in r as well. someone else may have already developed and tuned a model. it is worth looking in the aws marketplace to find available models. amazon sagemaker provides highperformance, scalable machine learning algorithms optimized for speed, scale, and accuracy. algorithms optimized for speed, scale, and accuracy. for supervised learning, amazon sagemaker includes xgboost and linear learner algorithms for classification and quantitative problems. there is also a factorization machine to address recommendation and time series prediction problems. amazon sagemaker includes support for unsupervised learning, such as with kmeans clustering, and principal component analysis, pca, to solve problems like identifying customer groupings based on purchasing behavior. finally, there are a selection of specialized algorithms for processing images and other deep learning tasks. lets look a little closer at three of the most commonly used builtin algorithms and their use cases. xgboost, or extreme gradient boosting, is a popular and efficient opensource implementation of the gradient boosted trees algorithm. gradient boosting is a supervised learning algorithm that attempts to accurately predict a target variable by combining an ensemble of estimates from a set of simpler, weaker models. xgboost has done remarkably well in machine learning competitions because it robustly handles a variety of data types, relationships, and distributions. the large number of hyperparameters can be tweaked and tuned for improved fit. this flexibility makes xgboost a solid choice for problems in regression, classification and ranking. the amazon sagemaker linear learner algorithm provides a solution for both classification and regression problems. with the amazon sagemaker algorithm, you can simultaneously explore different training objectives and choose the best solution from your validation set. you can also explore a large number of models and choose the best one for your needs. compared with methods that provide a solution for only continuous objectives, the amazon sagemaker linear learner algorithm provides a significant increase in speed over naive hyperparameter optimization techniques. kmeans is an unsupervised learning algorithm. it attempts to find discrete groupings within data where members of a group are as similar as possible to one another and as different as possible from members of other groups. you define the attributes that you want the algorithm to use to determine similarity. to train a model in amazon sagemaker, you create a training job. the training job includes the url of the amazon s3 bucket where you stored the training data, the url of the s3 bucket where you want to store the output of the job. the amazon elastic container registry path where the training code is stored. the compute resources that you want amazon sagemaker to use for model training. compute resources are ml compute instances managed by amazon sagemaker. amazon sagemaker provides a selection of instance types optimized to fit different machine learning use cases. instance types comprise varying combinations of cpu, gpu, memory, and networking capacity, and give you the flexibility to choose the appropriate mix of resources for building, training, and deploying your ml models. each instance type includes one or more instance sizes, allowing you to scale your resources to the requirements of your target workload. some key takeaways from this section of the module include split data into training and testing sets helps you validate the models accuracy. kfold crossvalidation can help with smaller data sets. two key algorithms for supervised learning are xgboost and linearlearner. use kmeans for unsupervised learning. and use amazon sagemaker to train models. thats it for section 5, i hope to see you in the next video.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod03_Sect07_part1.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi, welcome back to module 3. in this section, well look at how you can evaluate your models success in predicting results. at this point, youve trained your models. its now time to evaluate that model to determine if it will do a good job predicting the target on new and future data. because future instances have unknown target values, you need to assess how the model will perform on data where you already know the target answer. youll then use this assessment as a proxy for performance on future data. this is the reason why you hold out a sample of your data for evaluating or testing. an important part of this phase involves choosing the most appropriate metric for your business situation. think back to the earlier section on problem formulation. during that phase, you define your business problem and outcome, and then you craft a business metric to evaluate success. the model metric you choose at this phase should be linked to that business metric as much as possible. theres often a high correlation between the two metrics. in addition to considering your business problem and success metric, the type of ml problem youre working with will influence the model metric you choose. throughout the rest of this module, well look at examples of common metrics used in classification problems. well also look at common metrics used in regression problems. were going to start by considering a simple binary classification problem. heres a specific example. imagine that you have a simple image recognition model thats labeling data as either cat or not cat. after the models been trained, you can use the test dataset you held back to perform predictions. to help examine the performance of the model, you can compare the predicted values with the actual values. if you plot the values into a table like the example, you can start getting some insights into how well the model performed. in a confusion matrix, you can get a highlevel comparison of how the predicted class is matched up against the actual classes. if the actual label or class is cat, which is identified as p for positive, and the predicted label or class is also cat, then you have a true positive. this is a good outcome for your model. similarly, if you have an actual label of not cat, which is identified as n for negative, and the predicted label or class is also not cat, then you have a true negative. this is also a good outcome for your model. in both these cases, your model predicted the correct outcome when it used the testing data. there are two other possible outcomes, and both arent considered good outcomes. the first one is when the actual class is negative, so you got not cat, but the predicted class is positive, or cat. this is called a false positive because the prediction is positive but incorrect. finally, there are false negatives. these happen when the actual class is positive, so you got cat, but the predicted class is negative, or not cat. thats it for part one of this section. well see you again for part two where well review calculating classification metrics.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod03_Sect02_part3.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi, welcome back. well continue exploring data collection by reviewing how to secure your data. its important to consider the security of your data. though the data sets used in this course are all public, real data about customer transactions or health records need to be kept secure. you can use aws identity and access management, which is also known as iam. its a service that controls access to resources. make sure youre securing your data within aws correctly so you can avoid data breaches. the diagram shows a simple iam policy that allows only read access to a specific s3 bucket for the listed role. in addition to controlling access to data, you need to make sure your data is secure. its a good practice and it might also be legally required for certain data types, such as financial data or healthcare records. aws provides encryption features for storage services, typically for data thats at rest or in transit. you can often meet these encryption requirements by enabling encryption on the object or service you want to protect. for data in transit, you must use secure transports like secure sockets layer, transport layer security, or ssl tls. another aspect to consider is compliance audits. when dealing with data from regulated industries, youll often need to audit access to the data. aws cloudtrail is a service that enables governance, compliance, operational auditing, and risk auditing of your aws account. with cloudtrail, you can log, continuously monitor, and retain account activity related to actions across your entire aws infrastructure. cloudtrail provides an event history of your aws account activity, including actions taken through the aws management console, aws sdks, command line tools, and other aws services. this event history simplifies security analysis, resource change tracking, and troubleshooting. you can also use cloudtrail to detect unusual activity in your aws accounts. all these features can help you simplify operational analysis and troubleshooting. here are the key takeaways for this section. we looked at the first step in solving machine learning problems, obtaining the data required to train your machine learning model. we also reviewed how etl can be used to obtain data from multiple sources. services like aws glue can make it used to obtain data from multiple sources. services like aws glue can make it easy to obtain data from multiple data stores. finally, make sure you understand your security requirements. these are based on both business need and any regulatory requirements. also make sure your data is secure. only authorized users should be able to access your data and it should be encrypted where possible. thats it for section 2, well see you in the next video.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod03_Sect03_part2.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi, welcome back. well continue exploring how to describe your data. now that your data is in a readable format, you can perform descriptive statistics on the data to better understand it. descriptive statistics help you gain valuable insights into your data so that you can effectively preprocess the data and prepare it for your ml model. well look at how you can do that and discuss why its so important. first, descriptive statistics can be organized into a few different categories. overall statistics include the number of rows and the number of columns in your dataset. this information, which relates to the dimensions of your data, is very important. for example, it can indicate that you have too many features, which can lead to high dimensionality and poor model performance. attribute statistics are another type of descriptive statistic, specifically for numeric attributes. theyre used to get a better sense of the shape of your attributes. this includes properties like the mean, standard deviation, variance, and minimum and maximum values. if you need to look at relationships between more than one variable, you can consider multivariate statistics. they mostly relate to the correlations and relationships between your attributes. for cases when you have multiple variables or features, you might want to look at the correlations between them. its important to identify correlations between attributes because a high correlation between two attributes can sometimes lead to poor model performance. when features are closely correlated and theyre all used in the same model to predict the response variable, there could be problems. for example, the model loss might not converge to a minimum state. so be aware of highly correlated features in your dataset. mean and median are two different measures describing the extent that your data is clustered around some value or position. mean can be a useful method for understanding your data when the data is symmetrical. however, if your data is skewed or contains outliers, then median tends to provide the better metric for understanding your data as it relates to central tenancy. for instance, if you have outliers with large values, the mean can be skewed one way, and it wouldnt serve as an accurate representation of where your values are truly centered. median isnt affected by outliers in the same way. well talk more about outliers soon. statistics are available, and they can be viewed on numerical data by using methods such as describe. there are also other methods to calculate the mean, median, and others. you can also view statistics on single or multiple columns. you can even group data by specific values. for categorical attributes, you can look at the frequency of attribute values in your dataset. that information will give you some idea about what is inside that categorical variable. the diagram here shows the car dataset, which is made up of several categorical values, buying, maint, lug boot, safety, and class. safety can be either low, medium, or high. from the describe function, you can see that there are three unique values, with low being the most frequent. values, with low being the most frequent. looking at the class column, it appears that the top value of the four is unacc, which stands for unaccounted. this accounts for 1,210 of the 1,728 values, or 70. this might suggest an imbalance. for a target variable thats also of a categorical type, you can look at the class distribution to see whether theres a class imbalance in your dataset. imbalanced data can mark a disproportionate ratio for your classes. for instance, your dataset is made up of credit card transactions, but only a tenth of a percent is labeled as fraud. in this case, your algorithm might not learn well enough to predict examples of credit card fraud. visualization could help you gain insights into your data that you might not be aware of otherwise. a histogram is often a good visualization technique for seeing the overall behavior of a particular feature. with a histogram, you can answer questions like, is the feature data normally distributed? how many peaks are there in the data? is there any skewness for that particular feature? when using histograms for your data visualization, values are binned. the taller peaks of the histogram indicate the most common values. for numerical features, you can use density plots and box plots, in addition to histograms, to get an idea of whats inside that particular feature. like a histogram, these visualizations will help you answer questions like, whats the range of the data? the peak of the data? are there any outliers? are there any special features? answering these questions helps you understand your data better and can also help you decide if you need to do more specialized data preprocessing. a box plot is a method for graphically depicting groups of numerical data through their quartiles. when you have more than two numerical variables in a feature dataset, you might want to look at their relationship. a scatter plot is a good way to identify any special relationships among those variables. in this case, the left diagram has sulfates and alcohol. they are two numerical variables. in this case, the left diagram has sulfates and alcohol. they are two numerical variables. suppose you want to show the relationship between these variables. you can use a scatter plot to help you visualize that. there are plots scattered around, and the correlation among them might not be that high because the data is scattered. however, you might find some relatively positive relationships between the two variables. scatter plot matrices help you look at the relationship between multiple different features. in pendis, you can easily create scatter plot matrices based on the columns you want to look at. this example has three columns, and it will give the pairwise scatter plot for any two columns. with a scatter plot, you might want to identify special regions that a particular subset of data could fit into. in the example, is there a relationship between alcohol sulfates and quality? you could plot those values against good and poor quality wines like the example. plotting gives you an idea of how useful particular variables can be if youre using them for a classification problem. thats it for part 2 of this section. well see you again for part 3 where well review correlations and the takeaways for this section.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod03_Sect03_part1.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi and welcome back. this is section 3 and were going to cover how to evaluate your data. in this section, well look at different data formats and types. well also look at how you can visualize and analyze the data before feature engineering. before you can start running statistics on your data to better understand what youre working with, you need to ensure its in the right format for analysis. for amazon sagemaker, algorithms support training with data in csv format. many of the tools youll use to explore, visualize, and analyze the data can also read it in csv format. generally speaking, youll need to have at least some domain knowledge for the problem youre trying to solve with machine learning. for example, if youre developing a model to predict if a set of symptoms indicates a disease, youd need to know the relationship between the symptoms and the disease. data typically needs to be in numeric form, so machine learning algorithms can use the data to make predictions. well look at ways you can convert text data in the next section. for now, well just explore the data and try to gain some insights into the overall data set. one popular open source python library is pandas. it can take data in various formats, reformat it, and load it into a tabular representation of your data, presenting it in rows and columns. some of the formats that pandas can reformat and load include csv, excel, pickle, and javascript object notation, or json. pandas also has data analysis and manipulation features and well use them throughout this module loading data can be as simple as the example which pulls in the csv file from the specified url when you load data into pandas its stored as a pandas dataframe. in the pandas documentation, a dataframe is described as a general 2d labeled, sizemutable tabular structure with potentially heterogeneously typed column. a more helpful way to think of a dataframe is to think of it as a spreadsheet or a sql table. like a table or spreadsheet, a data frame will have rows which are also known as instances, and it will have columns which are also known as attributes. the shape property of a data frame describes the number of rows and columns it has. each column in a data frame is a series. a series is a onedimensional labeled array. a series can store data of any type. to learn more about data structures in pandas, see the pandas documentation. along with data, you can load a data frame with row labels and column labels. the row labels are known as an index, and the column labels are known as columns. if you loaded your data from a csv file with a header row, the columns will be created from the first line of the file. you can change that behavior, however. if you dont have column names in the source file, you can pass them as a parameter. when performing data analysis, its important to make sure youre using the correct data types. in many cases, pandas will correctly infer the correct data types when it loads data, and you can move on. if you have domain knowledge or access to a domain expert, they can often identify data type issues. you can use either d types or the info function to obtain information on the column types as shown in the example. if you dont have the correct data types, you need to figure out why this is the case. often a numeric column could have been missing data or it could be a single text value. for example, in the car dataset, the number of doors can be 2, 3, 4, or 5 more. after youve analyzed the data, you can convert a column to the correct data type using pandas. thats it for part 1 of this section. well see you again for part 2, where well review how to describe your data.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod03_Sect02_part2.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi, welcome back. well continue exploring data collection by reviewing how to extract, transform, and load data. data is typically spread across many different systems and data providers. this presents a challenge. youll need to bring all these data sources together into something that can be consumed by a machine learning model. you can do this through extract, transform, and load, which is also known as etl. the steps in etl are defined this way. in the extract step, you pull the data from the sources to a single location. during extraction, you might need to modify the data, combine matching records, or do other tasks that transform the data. finally, in the load step, the data is loaded into a repository such as amazon s3. a typical etl framework has several components. as an example, consider the diagram. first, the crawler a program connects to a data store, which can be a source or a target. it progresses through a ranked list of classifiers to determine the schema for your data. then, it creates metadata tables in the aws glue data catalog. a job defines the business logic thats needed to perform etl work. to run the job, youll need to use a schedule or event. as a final note, the services we just discussed exist in the transform partition of the etl process. aws glue is a fully managed etl service that makes it simple and costeffective to categorize your data, clean it, enrich it, and move it reliably between various data stores. aws glue consists of a central metadata repository known as the aws glue data catalog. this is an etl engine that automatically generates python or scala code. it also provides a flexible scheduler that handles dependency resolution, job monitoring, and retries. aws glue is serverless, so you dont need to set up or manage any infrastructure. you can use the aws glue console to discover data, transform it, and make it available for search and queries. the console calls the underlying services to orchestrate the work needed to transform your data. you can also use the aws glue api operations to interface with the aws glue services. this way, you can edit, debug, and test your python or scala apache spark etl code using a familiar development environment. aws glue is wellsuited to machine learning because it can receive labeled data that can be used for training. heres an example. say that you provide aws glue with training data that teaches the model what duplicate records in the data source look like. then, aws glue can identify the duplicates and present them for further analysis by a data engineer. aws glue enables the orchestration of complex etl jobs. in the example, aws glue crawls the data sources and presents the information to clients as a data catalog. aws glue can run your etl jobs based on an event, such as getting a new data set. for example, you can use an aws lambda function to trigger your etl jobs to run as soon as new data becomes available in amazon s3. you can also register this new data set in the aws glue data catalog as part of your etl jobs. although managed tools are available in aws to manipulate data, a data scientist will also write scripts in their jupyter notebook to handle data. a very simple extract and load script is shown here. the imports and variables section imports the libraries that are used. note that bodo3 is the library for aws. variables are also set here for the zip files web location and a local folder for extraction. the download and extract section makes a web request, saving the bytes from the url as a stream. this stream is passed to the zip file function, which is then used to extract the data. with the extracted files in a folder, the upload to s3 section enumerates the folders files and uploads each file to amazon s3. if you discover that this script is used often, it should be migrated to a standalone function that can be imported by other python applications. thats it for part two of this section. well see you again for part three where well review how to secure your data.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod03_Intro.txt\n",
      "\n",
      "\n",
      "Normalized Text: welcome back to aws academy machine learning. this is module three, and were going to work through the entire machine learning pipeline by using amazon sagemaker. this module will discuss a typical process for handling a machine learning problem. the machine learning pipeline can be applied to many machine learning problem. the machine learning pipeline can be applied to many machine learning problems. the focus is on supervised learning, but the process you learn in this module can be adapted to other types of machine learning as well. this is a large module and well be covering a lot of material. at the end of this module, youll be able to formulate a problem from a business request, obtain and secure data for machine learning, build a jupyter notebook by using amazon sagemaker, outline the process for evaluating data, explain why data needs to be preprocessed, use opensource tools to examine and preprocess data. use amazon sagemaker to train and host a machine learning model. use crossvalidation to test the performance of a machine learning model. use a hosted model for inference. and finally, create an amazon sagemaker hyperparameter tuning job to optimize a models effectiveness. were ready to get started. see you in the next video.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod03_Sect04_part3.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi, welcome back. well continue exploring feature engineering by describing how to work with outliers. you might also need to clean your data based on any outliers that exist. outliers are points in your dataset that lie at an abnormal distance from other values. theyre not always something you want to clean up because they can add richness to your dataset. but they can also make it harder to make accurate predictions because they skew values away from the other, more normal values related to that feature. an outlier might also indicate that the data point actually belongs to another column. you can think of outliers as falling into two broad categories. might also indicate that the data point actually belongs to another column. you can think of outliers as falling into two broad categories. the first is a single variation for just a single variable, or a univariate outlier. the second is a variation of two or more variables, or a multivariate outlier. one of the more common ways to find univariate outliers is with a box plot. a box plot shows how far a data point is to the mean for that variable. the box in the plot shows the data values within two quartiles of the mean. values outside that range are represented by the lines extending from the box, which are sometimes called whiskers. a scatter plot can be an effective way to see multivariate outliers. for example, this diagram shows the amount of sulfates and alcohol in a collection of wines. with the scatter plot, you can quickly visualize whether there are multivariate outliers for the two variables. the origin of your outlier will most likely inform how you deal with it during this preprocessing phase of the pipeline, or possibly later during feature engineering. there are several different approaches to dealing with outliers. you could delete the outlier if your outlier is based on an artificial error. this means the outlier isnt natural and was introduced because of some failure like incorrectly entered data. you could also transform the outlier by taking the natural log of a value. this, in turn, reduces the variation caused by the extreme outlier value, which would then reduce the outliers influence on the overall dataset. finally, you could use the mean of the feature and impute that value to replace the outlier value. again, this would be a good approach if the outlier was caused by artificial error. this isnt an exhaustive list, but it describes the most common options. after youve extracted features, youll need to select the most appropriate features for training your model. there are three main feature selection methods. filter methods use statistical methods to measure the relevance of features by their correlation with the target variable. wrapper methods measure how useful a subset of a feature is. they do this by training a model on the feature and then measuring how successful the model is. filters are faster and cheaper than wrapper methods because they dont involve training the models repeatedly. wrappers typically find the best subset of features, but theres a risk of overfitting compared to using subsets of features from filter methods. embedded methods are algorithmspecific, and they might use a combination of both filters and wrappers. filter methods use a proxy measure instead of the actual models performance. theyre fast to compute, but they can still capture how useful the feature set is. here are some common measures. the first is pearsons correlation coefficient, which measures the statistical relationship or association between two continuous variables. the second is linear discriminant analysis, or lda. this is used to find a linear combination of features that separates two or more classes. the third is analysis of variance, or anova. this is used to analyze the differences among group means in a sample. and finally, chisquare is a single number that tells you how much difference exists between your observed counts and the counts youd expect if there were absolutely no relationships in the population. filters are usually less computationally intensive than wrappers, but they produce a feature set that isnt tuned to a specific type of predictive model. this lack of tuning means a feature set from a filter is more general than one from a wrapper. the filter also usually has a lower prediction performance than a wrapper. lower prediction performance than a wrapper. however, the filters feature set doesnt contain the assumptions of a prediction model, so its more useful for exposing relationships between features. many filters provide feature ranking instead of an explicit best feature subset, and the cutoff point in the ranking is chosen through crossvalidation. filters have also been used as a preprocessing step for wrappers, which enables a wrapper to be used on larger problems. wrapper methods use a predictive model to score feature subsets. each new subset is used to train a model, which is then tested on a holdout set. the score for that subset is calculated by counting the number of mistakes made on that holdout set, the score for that subset is calculated by counting the number of mistakes made on that holdout set, or the error rate of the model. because wrappers train a new model for each subset, theyre computationally intensive. however, they usually provide the best performing feature set for that particular type of model or problem. forward selection starts with no features and adds them until the best model is found. backward selection starts with all features, drops them one at a time, and then selects the best model. embedded methods combine the qualities of filter and wrapper methods. theyre implemented by algorithms that have their own builtin feature selection methods. some of the most popular examples of these methods are lasso and ridge regression. they have builtin penalization functions to reduce overfitting. here are some key takeaways from this section of the module. first, feature engineering involves selecting the best features for machine learning. preprocessing gives you better data to work with, and better data typically provides better results. two categories for preprocessing are converting data to numerical values and cleaning up dirty data by removing missing data and cleaning outliers. finally, how you handle dirty data impacts your model. thats it for section 4, well see you in the next video!\n",
      "\n",
      "\n",
      "\n",
      "File: Mod06_Intro.txt\n",
      "\n",
      "\n",
      "Normalized Text: introduction to natural language processing hi, and welcome to module 6 of aws academy machine learning, introduction to natural language processing. in this module, well introduce natural language processing, which is also known as nlp. this section includes a description of the major challenges faced by nlp and the overall development process for nlp applications. well then review five aws services you can use to speed up the development of nlpbased applications. after completing this module, you should be able to describe the nlp use cases that are solved by using managed amazon ml services, and describe the managed amazon ml services available for nlp. lets get started.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod03_Sect07_part2.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi, welcome back. well continue exploring how to evaluate your model. the diagram shows the confusion matrix of how two different models performed on the same data. can you tell which ones better? which is better isnt a good question to ask. what do you mean by better? does better mean making sure you find all the cats? even if it means youll get many false positives? or does better mean making sure the model is the most accurate? its difficult to see just by looking at the two charts. what if youre trying several models, using multiple folds, and have hundreds of data points to compare? to do that, youll need to calculate more metrics. the first metric is sensitivity. this is sometimes referred to as recall, hit rate, or true positive rate. sensitivity is the percentage of positive identifications. in the cat example, it represents what percentage of cats were correctly identified. to calculate sensitivity, take the number of true positives, or the number of positive identifications of cats, and divide that by the total number of actual cats. in this example, 60 of cats that were cats were correctly identified as cats. specificity is sometimes referred to as selectivity or true negative rate. specificity is the percentage of negatives correctly identified. in the cat example, this is the number of images that were not cats that were correctly identified as not cats. to calculate specificity, take the number of true negatives and divide that by the total number of actual negatives. so for the example, thats the number of not cats that were correctly identified divided by the total number of actual not cats. this means that in the example 64 of notcats were identified as notcats. now that you have these metrics for each model, knowing what your business goal is makes it easier to decide which model to use. which model would you choose if you wanted to make sure youll identify as many cats as possible? model b would be a good answer, if youre not concerned about having many false positives, that is. if youre not concerned about having incorrectly identified notcats. which model would you choose if you wanted to make sure you identified animals that were not cats? model a might work for this scenario. again, it would depend on how many false negatives you can tolerate. if this was a classification of patients who had heart disease or not, which model would be best? this is where it gets interesting. a fun website might get a bad reputation if it cant identify cats correctly, but if youre trying to diagnose patients, your focus will probably be very different. its important to understand the tradeoffs youre making when you decide which model to use. there are also other metrics that can help you make your decisions. thats it for part 2 of this section. well see you again for part 3 where well start looking at thresholds.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod06_Sect01.txt\n",
      "\n",
      "\n",
      "Normalized Text: well get started by reviewing what natural language processing means. natural language processing is also known as nlp. before we explain what nlp is, well consider an example of nlp, amazon alexa. alexa works by having a device, such as an amazon echo, record your words. the recording of your speech is sent to amazons servers to be analyzed more efficiently. amazon breaks down your phrase into individual sounds. then, it connects to a database containing the pronunciation of various words to find which words most closely correspond to the combination of individual sounds. amazon identifies important words to make sense of the tasks and carry out corresponding functions. for instance, if alexa notices words like outside or temperature, it will open the weather alexa skill. amazon servers then send the information back to your device and alexa skill. amazon servers then send the information back to your device, and alexa speaks. nlp is a broad term for a general set of business or computational problems you can solve with machine learning, or ml. however, nlp systems predate machine learning. for example, speechtotext on older presmartphone cell phones used nlp, and so did screen readers. many nlp systems now use some form of machine learning. nlp considers the hierarchical structure of language. words are at the lowest layer in a hierarchy. a group of words make a phrase. in the next level up, phrases make a sentence. and ultimately, sentences convey ideas. nlp systems face several significant challenges. well look at its challenges next. language isnt precise. words can have different meanings based on the other words that surround them. device. words can have different meanings based on the other words that surround them. this is known as context. often the same words or phrases can have multiple meanings. for example, consider the term weather. you could be under the weather, which has a colloquial meaning in english that youre sick. or you could say, theres wonderful weather outside, which means the weather conditions outside are good. the phrase, oh really, could convey surprise, disagreement, and many other things. it depends on the context and inflection. here are some of the main challenges for nlp. one challenge is discovering the structure of the text. one challenge is discovering the structure of the text. one of the first tasks of any nlp application is to break down the text into meaningful units, such as words, phrases, and sentences. another challenge is labeling data. after the system converts the text to data, it must apply labels representing the various parts of speech. every language will require a different labeling scheme to match the languages grammar. nlp also faces a challenge in representing context. because word meaning can depend heavily on context, any nlp system needs a way to represent it. this is a large challenge because there are many contexts, and its difficult to convert context into a form computers can understand. finally, although grammar defines a structure for language, the application of grammar is indescribably large in scope. handling the variation in how language is used by humans is a major challenge for nlp systems. thats where machine learning can have a large impact. you can apply nlp to a range of problems. some of the more common applications include search applications such as google or bing, humanmachine interactions like alexa, sentiment analysis for marketing or political campaigns, social research based on media analysis, and chatbots to mimic human speech in applications. you can apply the machine learning development pipeline youve seen throughout this course when developing an nlp solution. the first task is to formulate a problem, then collect and label data. for nlp, collecting data consists of breaking down the text into meaningful subsets and labeling the sets. feature engineering is a major part of nlp applications. this process gets complicated when youre dealing with highly irregular or unstructured text. for example, say youre building an application to classify documents. youd need to be able to distinguish between the words with common terms but different meanings. labeling data in the nlp domain is sometimes also called tagging. in the labeling process, you assign individual text strings to different parts of speech. there are specialized tools you can use for nlp labeling. the first task for an nlp application is to convert the text to data so it can be analyzed. you convert text by removing words that arent needed for analysis from the input text. in the example, the words this and is are removed to leave the phrase sample text. after removing stop words, you can normalize text by converting similar words into a common form. for example, the words run, runner, ran, and running are all different forms of the word run. you can normalize all instances of these words in a block of text using the stemming and lemmatization processes. lemmatization groups different forms of a word into a single term. limitization of the versions of the word run would group all instances of those forms into a single term, run. stemming, on the other hand, removes characters that the stemming algorithm considers unnecessary. stemming might not work with the run example, as the form ran might not be recognized as a form of the word run. after youve normalized the text, you can standardize it by removing words that arent in the dictionary youre using for analysis. for example, you could remove acronyms, slang, and special characters. the natural language toolkit is also known as nltk. their python library provides functions for removing stop words and normalizing text. another first step in creating an nlp system is to convert the text into a data collection such as a data frame. all nlp libraries provide functions to assist with this process. the example shows using the word tokenize function from the nltk library. after youve cleaned up your text and loaded it into a data frame, you can apply one of the nlp models to create features. here are a couple of common models. the first model is known as bag of words. this is a simple model for capturing the frequency of words in a document. the model creates a key for each word. the value of the key is the number of times that word occurs in the document. the second model is term frequency and inverse document frequency, which is also known as tfidf. term frequency is a count of how many times a word appears in a document. inverse document frequency is the number of times a word occurs in a group of documents. these two values are used together to calculate a weight for the words. words that frequently appear in many documents have a lower weight. there are many established models in the nlp field. the example shows a bagofwords model. bagofwords is a vector model. vector models convert each sentence or phrase into a vector, which is a mathematical object that records both directionality and magnitude. in the example, a simple sentence is converted into a vector, where the frequency of each word is recorded. the word is has a value of two because it appears twice in the sentence. bag of words is often used to classify documents into different categories. its also used to derive attributes that feed into nlp applications, such as in sentiment analysis. there are three broad categories of text analysis. first, the classification of text is similar to other classification systems youve seen in this course. text provides the input to a process that extracts features. then you send the features through a machine learning algorithm that interacts with a classifier model and infers the classification. there are many applications for text matching. for example, autocorrect spelling and grammar checking are based on text matching. the algorithm for edit distance, also known as the levenstein distance, is frequently used. you can derive relationships between different words or phrases in the text using a process called coreference resolution. several nlp systems provide python libraries for deriving relationships. one of the biggest challenges for nlp is how to describe the context for the text. consider this example where a user is searching for the term tablet. because the word tablet has at least two distinct meanings, the search engine needs to know which meaning the user has in mind. most search engines rely on the most commonly used context if the term isnt qualified further. for example, by adding another term like medicine or computing to the search. the process of extracting entities is known as named entity recognition, or ner. an ner model has the following functions. first, it can identify noun phrases using dependency charts and part of speech tagging. it can classify phrases using a classification algorithm, such as word2vec. finally, it can disambiguate entities using a knowledge graph. heres an example of using ner to extract the entities titanic and north atlantic from the text. after the named entities are extracted, you can use a knowledge graph to extract meaning. a knowledge graph combines subject matter expertise with machine learning to extract meaning. a knowledge graph combines subject matter expertise with machine learning to derive meaning. the amazon recommendations engine is an example of a knowledge graph. here are the main points to remember from this section. first, nlp predates machine learning. you can use the same ml workflow that youve seen in other modules for nlp. some of the main use cases for nlp are search query analysis, humanmachine interaction, and marketing and social research. nlp is complicated because human language lacks precision. thanks for watching, well see you in the next video.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod05_WrapUp_ver2.txt\n",
      "\n",
      "\n",
      "Normalized Text: its now time to summarize some of the main points in this module. in this module, you learned how to describe the use cases for computer vision, describe the amazon managed machine learning services available for image and video analysis, list the steps required to prepare a custom data set for object detection. describe how amazon sagemaker ground truth can be used to prepare a custom data set. and use amazon recognition to perform facial detection. that concludes this introduction to computer vision. thanks for watching. well see you again in the next video.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod05_Sect01_ver2.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi, welcome back. this is section one, and were going to introduce computer vision. computer vision is an exciting space in machine learning. you can think of computer vision as the automated extraction of information from digital images. using computer vision, machines can identify people, places, and things in images with an accuracy thats at or above human levels, and with greater speed and efficiency. computer vision is often built with deep learning models. it automates the extraction, analysis, classification, and understanding of useful information from a single image or a sequence of images. the image data can take many forms, such as single images, video sequences, views from multiple cameras, or threedimensional data. computing power and algorithms have advanced over the last 10 years. this has led to an increase in capabilities and easier access to computer vision technologies. so how is computer vision being used? here are some of the primary use cases for computer vision. you can use image and facial recognition to improve public safety and home security, or as a way to authenticate access to personal devices. you can also use it to automatically classify images for content management and analysis. autonomous driving is partly enabled by computer vision technologies, and so are safety features of cars, such as lane detection or collision avoidance. medical image analysis with computer vision can improve the accuracy and speed of a patients medical diagnosis. this can result in better treatment outcomes and life expectancy for the patient. and finally, in manufacturing, welltrained computer vision is incorporated into robotics. this can improve quality assurance and operational efficiencies. these are just a few examples, and you can probably think of more. computer vision problems can be broken down into a few areas. content recognition is about identifying things in images. its a classification problem, but its a complex one with several layers. in the picture here, whats represented? is it breakfast, lunch, or dinner? would the classification only be food? the answer depends on what model you use to perform the classification. models must be trained, and the training data provides the algorithm with data for it to learn from. say that you have a model that was trained with pictures of different types of food. you might expect the image to output categories such as milk, peaches, mashed potato, chicken nuggets, and salad. if you trained the model with different images, it could classify objects as tray, cutlery, and napkin instead. when you work with images, you might want to know what kinds of objects are in the image and the location of those objects. object detection provides the image categories and where the objects are located in the image. theres a set of coordinates defining the location of a box surrounding the image, which is known as the bounding box. bounding boxes for detection are typically top, left, width, and height coordinates surrounding the images. you can use these coordinates in your applications. the images. you can use these coordinates in your applications. when objects are detected in an image, theres a confidence number usually associated with that object. this percentage indicates the probability that the object belongs to a specific class. this confidence level is important when you want to determine an action thats based on object detection, especially in facial detection applications or cases where the action has significance. object segmentation is also known as semantic segmentation. its like object detection, but you go into more detail to get fine boundaries for each detected object. basically its a finegrained inference for predicting each pixel in the image. some applications that require object segmentation include autonomous vehicles and advanced computerhuman interactions. though object segmentation is a key problem in the field of computer vision, we wont be covering it in this course. video adds another dimension to computer vision. with video, you get more data to work with, so you can capture the movement of people or objects, which are referred to as instances. for example, you can detect people who enter and leave frames, and also deal with moving cameras. heres a use case for computer vision. building on detection and tracking, you can analyze shopper behavior in your retail store by studying the path each person follows. if you use face analysis, you can understand other details about shoppers, such as average age ranges, gender distribution, and expressed emotions without identifying them. heres another computer vision use case. you can also analyze images to identify actions using the motion in the video. for example, activities such as delivering a package or dancing. looking at this image of a baseball player, some examples from the image could include capturing the batters accuracy, the pitchers pitching style, the type of pitch slow ball, slider and others, the inning, and the batters performance versus the specific pitcher. managers could use all that data to coach players on how to improve their performance, and they do. coaches can also use the data during the game to make gametime decisions. say you want to initiate various actions based on the speed of the baseball leaving the bat and its trajectory. a hit thats calculated by an ml model could lead to an audio or visual warning about a possible foul ball into the crowd. or it could result in a preemptive alarm that a hit has a high probability of being a home run. this means that events following a home run could be both welltimed and automated, such as playing music or setting off fireworks when the home run is hit by the home team. to wrap up this section, here are some key takeaways for this section. first, we covered how computer vision is the automated extraction of information from images. you can divide computer vision into two distinct areas, image analysis and video analysis. image analysis includes object classification, detection, and segmentation. video analysis includes instance tracking, action recognition, and motion estimation. thanks for watching. well see you in the next video.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod03_Sect07_part3.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi, welcome back. well continue exploring how to evaluate your model. classification models are going to return a probability for the target. this is a value of the input belonging to the target class, and it will be between 0 and 1. to convert the value to a class, you need to determine the threshold to use. you might think its 50, but you could change it to be lower or higher to improve your results. as youve seen with sensitivity and specificity, theres a tradeoff between correctly and incorrectly identifying classes. changing the threshold can impact that outcome. were going to take a look at how you can visualize this. a receiver operating characteristic graph is also known as an roc graph. it summarizes all the confusion matrices that each threshold produced. to build one, you calculate and plot the sensitivity, or true positive rate, against the false positive rate on a graph for each threshold value. you can calculate the false positive rate by subtracting the specificity from 1. after you plot those points, you can draw a line between them. the dotted black line from 0, 0 to 1, 1 means that the sensitivity or true positive rate is equal to the false positive rate. the point at 1, 1 means that youve correctly identified all the cats, but youve also incorrectly identified all the notcats. this is bad. any point on this line means that the proportion of correctly classified samples is the same as the proportion of incorrectly classified samples. the point at represents that there are zero true positives and zero false positives. a model that has high sensitivity and low false positive rate is usually the goal, so its considered to be better when the line between the threshold recordings is closer towards the top left corner. if you had the data from two models, you could plot out the roc curve for each model and compare them. however, that can be tedious. theres another graph you can use for this which well look at next. another evaluation metric you can use is the area under the curve receiver operator curve, which is also known as an auc roc. the auc part is the area under the plotted line. when the auc is higher, it means the model will be better at predicting cats as cats and not cats as not cats. you can use the auc to quickly compare models with each other. with the four numbers from our confusion matrix, you can calculate the models accuracy. this is also known as its score. you can do this by adding up the correct predictions and then dividing that number by the total number of predictions. though accuracy is a widely used metric for classification problems, it has limitations. this metric isnt as effective when there are a lot of true negative cases in your dataset. think about the catnotcat example. if most of your accuracy is based on true negatives, it says that your model is good at predicting what isnt a cat. in this case, you might not feel confident in your models ability to predict cats after you roll it out into production. this leads to an example of why its important to make sure that the metric you choose for model evaluation aligns to your business goal. think about the credit card fraud example. in this case, using accuracy as your main metric probably isnt a good idea because you have a lot of true negatives. your high true negative number might hide the fact that your models ability to identify cases of fraudthat is, to identify true positivesisnt ideal. as a credit card company, its probably unacceptable to have less than almost perfect performance identifying fraud cases. that would drive customers away, which would be the opposite of what youd want to achieve from a business standpoint. this is why two other metrics are often used in these situations. the first one is precision, which essentially removes the negative predictions. precision is the proportion of positive predictions that are actually correct. you can calculate it by taking the true positive and dividing it by true positive plus false positive. when the cost of false positives is high in your particular business situation, precision might be a good metric. think about a classification model that identifies email messages as spam or not. in this case, you dont want your model to label an email message as spam and thus prevent your users from seeing that message when its actually legitimate. or consider an example of a model that needs to predict whether a patient has a terminal illness. in this case, using precision as your evaluation metric doesnt account for false negatives in your model. here, for the model to be successful, its crucial that it doesnt falsely predict the absence of illness in a patient who actually has that illness. sensitivity would be a better metric to use for this situation. but it doesnt always need to be one or the other. the f1 score combines precision and sensitivity together. it gives you one number that quantifies the overall performance of a particular ml algorithm. you should consider using an f1 score when you have a class imbalance but want to preserve the equality between precision and sensitivity. but what do you do if youre dealing with a regression problem? in that case, there are other common metrics you can use to evaluate your model, including the mean squared error. the mean squared error is frequently used. its general purpose is the same as what you saw with classification metrics. you determine the prediction from the model, and you compare the difference between the prediction and the actual outcome. more specifically, you take the difference between the prediction and actual value, square that difference, and then sum up all the squared differences for all the observations. in skykit learn, you can use the mean squared error function directly from the metrics library. there are other metrics you can use for linear models, such as r squared. so youve trained your model, performed a batch transformation on your test data, and calculated your metrics. now what will you do? youll use these metrics to help you tune the model. you could select a different set of features and train the model again. after you retrain the model, ask yourself, which was the better model? the metrics will help inform you. you could also use different data and retrain the model with the same features. remember kfold crossvalidation from earlier in this module? finally, you could tune the parameters of the model itself, which is the subject of the next section. here are key takeaways from this section of the module. to evaluate the model, you need to have data that the model hasnt seen. this could be either a holdout set or you could use kfold crossvalidation. different machine learning models use different metrics. classification can use the confusion matrix and the aucroc that you can generate from it. regression can use mean squared. thats it for section 7. see you in the next video.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod04_WrapUp.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi, welcome back. its now time to review the module and wrap it up. in this module, you learned how to describe the business problem solved by amazon forecast, describe the challenges of working with time series data, list the steps required to create forecast by using amazon forecast, and use amazon forecast to make a prediction. thanks for participating. see you in the next module.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod04_Sect02_part3.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi and welcome back. in this section, well look at how you can use amazon forecast to create a predictor and generate forecasts. when you generate forecasts, you can apply the machine learning development pipeline youve seen throughout this course. but you still need data. you need to import as much data as you have, both historical data and related data. youll want to do some basic evaluation and feature engineering before you use the data to train a model so you can meet the requirements of amazon forecast. to train a predictor, you need to choose an algorithm. if youre not sure which algorithm is the best for your data, amazon forecast can choose for you. to do this, select automl as your algorithm. you also need to select a domain for your data. if youre not sure what the best fit is, you can also select a custom domain. domains have specific types of data they require. when you have a trained model, you can then use the model to make a forecast using an input data set group. after youve generated a forecast, you can query the forecast. you can also export it to a bucket in amazon s3. and finally, you can encrypt the data in the forecast before exporting it. the overall process for working with amazon forecast is to import historical and related data. amazon forecast inspects the data, identifies key data, and selects an appropriate algorithm. it uses the algorithm to train and optimize a custom model and produce a predictor. you create forecasts by applying the predictor to your dataset. you can then retrieve these forecasts in the aws management console, or you can export the forecasts as commadelimited files. you can also use an api and aws cli commands to create and retrieve forecasts. when you work with amazon forecast, you can select the domain youre working in. there are domains ranging from retail to web traffic, and theres also a custom option for everything else. by selecting a domain, you improve the efficiency of the predictor. each domain has specific types of data that youll supply when you build the predictor. for example, the retail domain expects data for the item identifiers, a timestamp for the observation, the number of sales for that item, and the specified timestamp. heres an example of the data youd need to provide for a retail demand forecast. for the time series, you need the time when the transaction took place, ideally in utc format, the item id of the item, and how many items were sold. the metadata for the item might include the category, the item color, and other attributes. the link back to the time series data will be only the item id, because item metadata typically doesnt change. related data for creating a more useful forecast could include the sales price or other promotion data. to link this back to the item, you must include the timestamp and the item id. heres an example of the data youd need to provide for a web traffic forecast. for the time series, you need the web page id, the number of page views per month, and the timestamp. related data for creating a more useful forecast could include the page category, such as navigation or content category. youll also need the geographic identifier for the web client. for metadata, you might also need to provide the region and the sales promotion information. amazon forecast predictors use an algorithm to train a model. they then use the model to make a forecast using an input dataset group. to help you get started, amazon forecast provides predefined algorithms, arima, deepar, ets, npts, and profit. you can also use the automl feature. it will try all the algorithms to see which ones the best at predicting data. when you prepare data for training and machine learning, you typically hold back data to use when you validate and score the model. the data that you hold back is usually a random sample of your available data. with time series data, you must process your data differently because of a correlation between time. when you import your data, amazon forecast breaks it into training and test datasets, which the diagram shows. the training data is used to train the model, which is then tested against the data that was held back. you can specify multiple backtest windows, which will split the data multiple times, train the model, and use metrics to determine which model gives the best results. the default backtest window is 1. you can change how amazon forecast splits the data by setting the backtest window offset parameter when you create the predictor. if you dont set this value, the algorithms use default values. after youve trained a model, you will need to measure its accuracy, which youll learn about next. the first amazon forecast evaluation metric is the weighted quantile loss, or w quantile loss. when amazon forecast creates a forecast, it provides probabilistic predictions at three distinct quantiles, 10, 50, and 90. these prediction quantiles show you how much uncertainty is associated with each forecast. a p10 quantile predicts that 10 of the time the true value will be less than the predicted value. for example, suppose that you are a retailer. you want to forecast product demand for winter gloves that sell well only during the fall and winter. say that you dont have sufficient storage space and the cost of invested capital is high, or that the price of being overstocked on winter gloves concerns you. then you might use the p10 quantile to order a relatively low number of winter gloves. you know that the p10 forecast overestimates the demand for your winter gloves only 10 of the time, so youll be sold out of your winter gloves for 90 of the time. a p50 quantile predicts that 50 of the time the true value will be less than the predicted value. continuing the winter gloves example, say you know that there will be a moderate amount of demand for the gloves and you arent concerned about being overstocked. then you might choose to use the p50 quantile to order gloves. a p90 quantile predicts that 90 of the time the true value will be less than the predicted value. suppose you determine that being understocked on gloves will result in large amounts of lost revenue. for example, the cost of not selling gloves is extremely high or the cost of invested capital is low. in this case, you might choose to use the p90 quantile to order gloves. amazon forecast also calculates the associated loss at each quantile. weighted quantile loss calculates how far off the forecast a certain quantile is from actual demand in either direction. lower w quantile loss metrics mean that the models forecasts are more reliable. the root mean square error, or rmse, is another method for evaluating the reliability of your forecasts. like w quantile loss, rmse calculates how far off the forecasted values were from the actual test data. the rmse finds the difference between the actual target value in the dataset and the forecasted value for that time period, and it then squares the differences. the example shows how to calculate rmse. the rmse value represents the standard deviation of the prediction errors. this test is good for forecast validity when the errors are mostly of the same size, that is, there arent many outliers. lower rmse metrics indicate that the models forecasts are more reliable. here is an example of how a web retailer might use the accuracy metrics to evaluate a forecast. the retailer wants to predict the demand for sales of a particular brand of shoes. they input the sales records for this brand into amazon forecast to create a predictor. the predictor provides a forecasted demand of 1,000 pairs with the p10, p50, and p90 values shown. the weighted quantile loss values indicate that 10 of the time there will be fewer than 880 pairs sold, 50 of the time fewer than 1,050 pairs will be sold, and 90 of the time fewer than 1,200 pairs will be sold. the retailer can then use these values to determine which level of inventory to hold. they can base their decision on their assessment of the risk that they wont be able to fulfill orders, or that theyll have excess inventory. some key takeaways from this section of the module include, you can use amazon forecast to train and use a model for time series data. there are specific schemas defined for domains such as retail and ec2 capacity planning, or you can use a custom schema. you need to supply at least the time series data, but can also provide metadata and related data to add more information to the model. as with most supervised machine learning problems, your data is split into training and testing data, but takes into account the time element. use rmse and w quantile loss metrics to evaluate the efficiency of the model. thats it for this video, well see you in the next one.\n",
      "\n",
      "\n",
      "\n",
      "File: Mod05_Sect03_part3.txt\n",
      "\n",
      "\n",
      "Normalized Text: hi, welcome back. well continue exploring video analysis by reviewing how to create the test dataset. the final step before you train your model is to identify a test dataset. you will use this test dataset to validate and evaluate the models performance. youll do this by performing an inference on the images in the test dataset. youll then compare the results with the labeling information thats in the training dataset. you can create your own test dataset. alternatively, you can use amazon recognition custom labels to split your training dataset into two datasets by using an 8020 split. this split means that 80 of the data is used for training and 20 is used for testing. after you define the training and test datasets, amazon recognition custom labels can automatically train the model for you. the service automatically loads and inspects the data, selects the correct machine learning algorithms, trains a model, and provides model performance metrics. youre charged for the amount of time a model takes to train. a dataset that contains more images and labels will take longer to train. when trainings complete, you evaluate the performance of the model. during testing, amazon recognition custom labels predicts if a test image contains a custom label. the confidence score is a value that quantifies the certainty of the models prediction. because this is a classification problem, the results can be mapped to a confusion matrix. with a true positive, the model correctly predicts the presence of the custom label in the test image. that is, the predicted label is also a ground truth label for that image. for example, amazon recognition custom labels correctly returns a cat label when a cat is present in an image. for a false positive, the model incorrectly predicts the presence of a custom label in a test image. that is, the predicted label isnt a ground truth label for the image. for example, amazon recognition custom labels returns a cat label, but theres no cat label in the ground truth for that image. for a false negative, the model doesnt predict that a custom label is present in the image, but the ground truth for that image includes this label. for example, amazon recognition custom labels doesnt return a cat custom label for an image that contains a cat. with a true negative, the model correctly predicts that a custom label isnt present in the test image. for example, amazon recognition custom labels doesnt return a cat label for an image that doesnt contain a cat. the console provides access to true positive, false positive, and false negative values for each image in your test dataset. these prediction results are used to calculate the various metrics for each label and an aggregate of metrics for your entire test set. the same definitions apply to predictions that the model makes at the bounding box level. with bounding boxes, all metrics are calculated over each bounding box in each test image, regardless of whether the boxes are prediction or ground truth. to help you, amazon recognition custom labels provides various metrics. for example, you can view summary metrics and evaluation metrics for each label. it also provides precision metrics for each label, and an average precision metric for the entire test data set. precision is the proportion of positive results that were correctly classified. amazon recognition custom labels provides average recall metrics for each label and an average recall metric for the entire test data set. recall is the fraction of your test set labels that were correctly classified. using the previous example of cats, that would be how many cats were correctly classified. the service also provides an average model performance score for each label, and an average model performance score for the entire test dataset. the f1 score combines precision and recall together to give you just one number that quantifies the overall performance of a particular machine learning algorithm. you might use the f1 score when you have a class imbalance, but you also want to preserve the equality between precision and sensitivity. a higher value means better model performance for both recall and precision. if youre satisfied with the accuracy of your model, you can start using it. thats it for part 3 of this section. well see you again for part 4 where well review how to evaluate and improve your model.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def normalize_text(text):\n",
    "    normalized_text = normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
    "    normalized_text = normalized_text.lower()\n",
    "    words = word_tokenize(normalized_text)\n",
    "    words = [word for word in words if word.isalnum()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    normalized_text = ' '.join(words)\n",
    "    return normalized_text\n",
    "\n",
    "def normalization_of_files(folder_path):\n",
    "    normalized_texts = {}\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                normaalize_text = nomalization_of_text(text)\n",
    "                normalized_texts[filename] = normaalize_text\n",
    "    return normalized_texts\n",
    "\n",
    "if not os.path.exists(\"normalized_text\"):\n",
    "    os.makedirs(\"normalized_text\")\n",
    "    \n",
    "folder_path = 'text_data/'  \n",
    "normalized_texts = normalization_of_files(folder_path)\n",
    "for filename, normaalize_text in normalized_texts.items():\n",
    "    print(\"File:\", filename)\n",
    "    print(\"\\n\")\n",
    "    print(\"Normalized Text:\", normaalize_text)\n",
    "    print(\"\\n\\n\")\n",
    "    with open(os.path.join(\"normalized_text\", filename), 'w', encoding='utf-8') as file:\n",
    "        file.write(normaalize_text)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extracting key phrases and topics\n",
    "([Go to top](#Capstone-8:-Bringing-It-All-Together))\n",
    "\n",
    "Use this section to extract the key phrases and topics from the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file: Mod04_Sect02_part2.txt\n",
      "Key Phrases:\n",
      "time series\n",
      "amazon forecast\n",
      "appendix excellent\n",
      "arent independent\n",
      "want determine\n",
      "fourth quarter\n",
      "noise separate\n",
      "time series data\n",
      "excellent time series\n",
      "time series support\n",
      "trend time series\n",
      "face time series\n",
      "random time series\n",
      "wrangling time series\n",
      "\n",
      "Key Topics:\n",
      "time series data\n",
      "excellent time series\n",
      "time series support\n",
      "trend time series\n",
      "face time series\n",
      "random time series\n",
      "wrangling time series\n",
      "\n",
      "Processed file: Mod05_Sect03_part4_ver2.txt\n",
      "Key Phrases:\n",
      "custom labels\n",
      "amazon recognition\n",
      "bounding box\n",
      "bounding boxes\n",
      "false positives\n",
      "models calculated\n",
      "calculated threshold\n",
      "custom labels operation\n",
      "detect custom labels\n",
      "recognition custom labels\n",
      "custom labels returned\n",
      "custom labels accuracy\n",
      "number custom labels\n",
      "custom labels console\n",
      "\n",
      "Key Topics:\n",
      "custom labels operation\n",
      "detect custom labels\n",
      "recognition custom labels\n",
      "custom labels returned\n",
      "custom labels accuracy\n",
      "number custom labels\n",
      "custom labels console\n",
      "\n",
      "Processed file: Mod05_Intro.txt\n",
      "Key Phrases:\n",
      "computer vision\n",
      "custom dataset\n",
      "machine learning\n",
      "prepare custom\n",
      ", well\n",
      "image video\n",
      "object detection\n",
      "prepare custom dataset\n",
      "computer vision space\n",
      "overview computer vision\n",
      "cases computer vision\n",
      "computer vision .\n",
      "finally , well\n",
      ", computer vision\n",
      "\n",
      "Key Topics:\n",
      "prepare custom dataset\n",
      "computer vision space\n",
      "overview computer vision\n",
      "cases computer vision\n",
      "computer vision .\n",
      "finally , well\n",
      ", computer vision\n",
      "\n",
      "Processed file: Mod02_Sect04.txt\n",
      "Key Phrases:\n",
      "machine learning\n",
      "jupyter notebooks\n",
      "youll use\n",
      "also provides\n",
      "get started\n",
      "problem domain\n",
      "amazon sagemaker\n",
      "machine learning .\n",
      "deploy machine learning\n",
      "frequently machine learning\n",
      "machine learning cloud\n",
      "machine learning combine\n",
      "machine learning developers\n",
      "machine learning experience\n",
      "\n",
      "Key Topics:\n",
      "machine learning .\n",
      "deploy machine learning\n",
      "frequently machine learning\n",
      "machine learning cloud\n",
      "machine learning combine\n",
      "machine learning developers\n",
      "machine learning experience\n",
      "\n",
      "Processed file: Mod05_Sect02_part2.txt\n",
      "Key Phrases:\n",
      "amazon recognition\n",
      "recognition video\n",
      "bounding box\n",
      "data stream\n",
      "streaming video\n",
      "stored videos\n",
      ". amazon\n",
      "amazon recognition video\n",
      ". amazon recognition\n",
      "use amazon recognition\n",
      "amazon recognition computer\n",
      "amazon recognition detects\n",
      "amazon recognition financial\n",
      "amazon recognition integrated\n",
      "\n",
      "Key Topics:\n",
      "amazon recognition video\n",
      ". amazon recognition\n",
      "use amazon recognition\n",
      "amazon recognition computer\n",
      "amazon recognition detects\n",
      "amazon recognition financial\n",
      "amazon recognition integrated\n",
      "\n",
      "Processed file: Mod03_Sect04_part2.txt\n",
      "Key Phrases:\n",
      "missing values\n",
      "describes number\n",
      "machine learning\n",
      "safe high\n",
      "sold state\n",
      "state california\n",
      "type sold\n",
      "missing values .\n",
      "impute missing values\n",
      "handle missing values\n",
      "missing values automatically\n",
      "missing values randomly\n",
      "percentage missing values\n",
      "update missing values\n",
      "\n",
      "Key Topics:\n",
      "missing values .\n",
      "impute missing values\n",
      "handle missing values\n",
      "missing values automatically\n",
      "missing values randomly\n",
      "percentage missing values\n",
      "update missing values\n",
      "\n",
      "Processed file: Mod02_Sect05.txt\n",
      "Key Phrases:\n",
      "machine learning\n",
      "computer vision\n",
      "managed services\n",
      "5 going\n",
      "able answer\n",
      "add sophisticated\n",
      "addition scenarios\n",
      "challenges machine learning\n",
      "many machine learning\n",
      "machine learning problems\n",
      "formulate machine learning\n",
      "machine learning capabilities\n",
      "machine learning knowledge\n",
      "operating machine learning\n",
      "\n",
      "Key Topics:\n",
      "challenges machine learning\n",
      "many machine learning\n",
      "machine learning problems\n",
      "formulate machine learning\n",
      "machine learning capabilities\n",
      "machine learning knowledge\n",
      "operating machine learning\n",
      "\n",
      "Processed file: Mod03_Sect06.txt\n",
      "Key Phrases:\n",
      "low latency\n",
      "secure https\n",
      "entire dataset\n",
      "processing steps\n",
      "batch transform\n",
      "youre planning\n",
      "youve trained\n",
      "low latency high\n",
      "securely low latency\n",
      "low latency containers\n",
      "runs low latency\n",
      "secure https endpoint\n",
      "inference low latency\n",
      "data processing steps\n",
      "\n",
      "Key Topics:\n",
      "low latency high\n",
      "securely low latency\n",
      "low latency containers\n",
      "runs low latency\n",
      "secure https endpoint\n",
      "inference low latency\n",
      "data processing steps\n",
      "\n",
      "Processed file: Mod03_WrapUp.txt\n",
      "Key Phrases:\n",
      "amazon sagemaker\n",
      "machine learning\n",
      "build jupyter\n",
      "business request\n",
      "crossvalidation test\n",
      "examine preprocess\n",
      "formulate problem\n",
      "amazon sagemaker hyperparameter\n",
      "amazon sagemaker train\n",
      "create amazon sagemaker\n",
      "using amazon sagemaker\n",
      "use amazon sagemaker\n",
      "host machine learning\n",
      "amazon sagemaker .\n",
      "\n",
      "Key Topics:\n",
      "amazon sagemaker hyperparameter\n",
      "amazon sagemaker train\n",
      "create amazon sagemaker\n",
      "using amazon sagemaker\n",
      "use amazon sagemaker\n",
      "host machine learning\n",
      "amazon sagemaker .\n",
      "\n",
      "Processed file: Mod04_Sect02_part1.txt\n",
      "Key Phrases:\n",
      "time series\n",
      "finely grained\n",
      "missing value\n",
      "example ,\n",
      "grained time\n",
      "daylight savings\n",
      "outofstock situation\n",
      "time series specific\n",
      "time series data\n",
      "finely grained time\n",
      "addition time series\n",
      "extrapolate time series\n",
      "irregular time series\n",
      "processing time series\n",
      "\n",
      "Key Topics:\n",
      "time series specific\n",
      "time series data\n",
      "finely grained time\n",
      "addition time series\n",
      "extrapolate time series\n",
      "irregular time series\n",
      "processing time series\n",
      "\n",
      "Processed file: Mod02_Sect01.txt\n",
      "Key Phrases:\n",
      "machine learning\n",
      "deep learning\n",
      "building machines\n",
      "artificial intelligence\n",
      "email message\n",
      "neural network\n",
      "algorithms statistical\n",
      "machine learning subset\n",
      "machine learning practitioners\n",
      "train machine learning\n",
      "advance machine learning\n",
      "introduction machine learning\n",
      "machine learning competition\n",
      "machine learning fits\n",
      "\n",
      "Key Topics:\n",
      "machine learning subset\n",
      "machine learning practitioners\n",
      "train machine learning\n",
      "advance machine learning\n",
      "introduction machine learning\n",
      "machine learning competition\n",
      "machine learning fits\n",
      "\n",
      "Processed file: Mod05_Sect03_part1.txt\n",
      "Key Phrases:\n",
      "amazon recognition\n",
      "machine learning\n",
      "custom labels\n",
      "computer vision\n",
      "recognition custom\n",
      "8 hearts\n",
      "often takes\n",
      "amazon recognition custom\n",
      "train amazon recognition\n",
      "amazon recognition recognizes\n",
      "capabilities amazon recognition\n",
      "use amazon recognition\n",
      "card amazon recognition\n",
      "want amazon recognition\n",
      "\n",
      "Key Topics:\n",
      "amazon recognition custom\n",
      "train amazon recognition\n",
      "amazon recognition recognizes\n",
      "capabilities amazon recognition\n",
      "use amazon recognition\n",
      "card amazon recognition\n",
      "want amazon recognition\n",
      "\n",
      "Processed file: Mod06_WrapUp.txt\n",
      "Key Phrases:\n",
      "ml services\n",
      "good job\n",
      "see next\n",
      "thanks watching\n",
      "well see\n",
      "available nlp\n",
      "services available\n",
      "ml services available\n",
      "well see next\n",
      "managed ml services\n",
      "amazon ml services\n",
      "see next module\n",
      "services available nlp\n",
      "ml services describe\n",
      "\n",
      "Key Topics:\n",
      "ml services available\n",
      "well see next\n",
      "managed ml services\n",
      "amazon ml services\n",
      "see next module\n",
      "services available nlp\n",
      "ml services describe\n",
      "\n",
      "Processed file: Mod01_Course Overview.txt\n",
      "Key Phrases:\n",
      "machine learning\n",
      ", youll\n",
      ". section\n",
      "youll learn\n",
      "artificial intelligence\n",
      "language processing\n",
      "natural language\n",
      "machine learning pipeline\n",
      "certified machine learning\n",
      "implement machine learning\n",
      "machine learning specialty\n",
      "intelligence machine learning\n",
      "host machine learning\n",
      "machine learning engineer\n",
      "\n",
      "Key Topics:\n",
      "machine learning pipeline\n",
      "certified machine learning\n",
      "implement machine learning\n",
      "machine learning specialty\n",
      "intelligence machine learning\n",
      "host machine learning\n",
      "machine learning engineer\n",
      "\n",
      "Processed file: Mod02_WrapUp.txt\n",
      "Key Phrases:\n",
      "machine learning\n",
      "artificial intelligence\n",
      "learned recognize\n",
      "ai landscape\n",
      "algorithms develop\n",
      "available data\n",
      "broader ai\n",
      "recognize machine learning\n",
      "challenges machine learning\n",
      "defining machine learning\n",
      "developing machine learning\n",
      "machine learning application\n",
      "machine learning applies\n",
      "machine learning deep\n",
      "\n",
      "Key Topics:\n",
      "recognize machine learning\n",
      "challenges machine learning\n",
      "defining machine learning\n",
      "developing machine learning\n",
      "machine learning application\n",
      "machine learning applies\n",
      "machine learning deep\n",
      "\n",
      "Processed file: Mod05_Sect02_part1_ver2.txt\n",
      "Key Phrases:\n",
      "amazon recognition\n",
      "also known\n",
      "lambda function\n",
      "confidence score\n",
      "aspects property\n",
      "stored videos\n",
      "inappropriate content\n",
      ". amazon recognition\n",
      "uses amazon recognition\n",
      "amazon recognition analyzes\n",
      "amazon recognition awsmanaged\n",
      "amazon recognition computer\n",
      "amazon recognition designed\n",
      "amazon recognition fall\n",
      "\n",
      "Key Topics:\n",
      ". amazon recognition\n",
      "uses amazon recognition\n",
      "amazon recognition analyzes\n",
      "amazon recognition awsmanaged\n",
      "amazon recognition computer\n",
      "amazon recognition designed\n",
      "amazon recognition fall\n",
      "\n",
      "Processed file: Mod07_Sect01.txt\n",
      "Key Phrases:\n",
      "machine learning\n",
      "best practices\n",
      "congratulations completing\n",
      "passing score\n",
      "aws certification\n",
      "review youve\n",
      "youve learned\n",
      "machine learning specialty\n",
      "certified machine learning\n",
      "academy machine learning\n",
      "machine learning solutions\n",
      "implement machine learning\n",
      "machine learning course\n",
      "amazon machine learning\n",
      "\n",
      "Key Topics:\n",
      "machine learning specialty\n",
      "certified machine learning\n",
      "academy machine learning\n",
      "machine learning solutions\n",
      "implement machine learning\n",
      "machine learning course\n",
      "amazon machine learning\n",
      "\n",
      "Processed file: Mod05_Sect03_part2.txt\n",
      "Key Phrases:\n",
      "ground truth\n",
      "sagemaker ground\n",
      "machine learning\n",
      "bounding box\n",
      "automated data\n",
      "recognition custom\n",
      "custom labels\n",
      "sagemaker ground truth\n",
      "ground truth uses\n",
      ". sagemaker ground\n",
      "ground truth functionality\n",
      "ground truth runs\n",
      "ground truth starts\n",
      "ground truth times\n",
      "\n",
      "Key Topics:\n",
      "sagemaker ground truth\n",
      "ground truth uses\n",
      ". sagemaker ground\n",
      "ground truth functionality\n",
      "ground truth runs\n",
      "ground truth starts\n",
      "ground truth times\n",
      "\n",
      "Processed file: Mod02_Sect03.txt\n",
      "Key Phrases:\n",
      "later course\n",
      "machine learning\n",
      "iso code\n",
      "day week\n",
      "feature engineering\n",
      "birth month\n",
      "impacts success\n",
      "later course .\n",
      "experts later course\n",
      "avoid later course\n",
      "detail later course\n",
      "engineering later course\n",
      "learn later course\n",
      "sources later course\n",
      "\n",
      "Key Topics:\n",
      "later course .\n",
      "experts later course\n",
      "avoid later course\n",
      "detail later course\n",
      "engineering later course\n",
      "learn later course\n",
      "sources later course\n",
      "\n",
      "Processed file: Mod03_Sect08.txt\n",
      "Key Phrases:\n",
      "training job\n",
      "objective metric\n",
      "hyperparameter tuning\n",
      "gradient descent\n",
      "machine learning\n",
      "neural network\n",
      "enough variation\n",
      "distributed training job\n",
      "automatic hyperparameter tuning\n",
      "returns training job\n",
      "training job highest\n",
      "training job runs\n",
      "instances training job\n",
      "one training job\n",
      "\n",
      "Key Topics:\n",
      "distributed training job\n",
      "automatic hyperparameter tuning\n",
      "returns training job\n",
      "training job highest\n",
      "training job runs\n",
      "instances training job\n",
      "one training job\n",
      "\n",
      "Processed file: Mod03_Sect04_part1.txt\n",
      "Key Phrases:\n",
      "machine learning\n",
      "multiple columns\n",
      "ordinal relationship\n",
      "make sure\n",
      "feature extraction\n",
      "feature selection\n",
      "youll need\n",
      "improve machine learning\n",
      "machine learning algorithms\n",
      "specific machine learning\n",
      "data multiple columns\n",
      "machine learning model\n",
      "help make sure\n",
      "youll need make\n",
      "\n",
      "Key Topics:\n",
      "improve machine learning\n",
      "machine learning algorithms\n",
      "specific machine learning\n",
      "data multiple columns\n",
      "machine learning model\n",
      "help make sure\n",
      "youll need make\n",
      "\n",
      "Processed file: Mod03_Sect01.txt\n",
      "Key Phrases:\n",
      "machine learning\n",
      "credit card\n",
      "determine type\n",
      "reduction number\n",
      ". first\n",
      ". section\n",
      "composition wine\n",
      "machine learning pipeline\n",
      "machine learning model\n",
      "appropriate machine learning\n",
      "irvine machine learning\n",
      "machine learning activities\n",
      "machine learning purpose\n",
      "machine learning repository\n",
      "\n",
      "Key Topics:\n",
      "machine learning pipeline\n",
      "machine learning model\n",
      "appropriate machine learning\n",
      "irvine machine learning\n",
      "machine learning activities\n",
      "machine learning purpose\n",
      "machine learning repository\n",
      "\n",
      "Processed file: Mod04_Intro.txt\n",
      "Key Phrases:\n",
      "amazon forecast\n",
      "time series\n",
      "going look\n",
      "series data\n",
      "4 aws\n",
      "academy machine\n",
      "aws academy\n",
      "time series data\n",
      "amazon forecast make\n",
      "solved amazon forecast\n",
      "use amazon forecast\n",
      "using amazon forecast\n",
      "amazon forecast ,\n",
      "look amazon forecast\n",
      "\n",
      "Key Topics:\n",
      "time series data\n",
      "amazon forecast make\n",
      "solved amazon forecast\n",
      "use amazon forecast\n",
      "using amazon forecast\n",
      "amazon forecast ,\n",
      "look amazon forecast\n",
      "\n",
      "Processed file: Mod06_Sect02.txt\n",
      "Key Phrases:\n",
      "amazon lex\n",
      "amazon polly\n",
      "amazon comprehend\n",
      "amazon translate\n",
      "use cases\n",
      "amazon transcribe\n",
      "use case\n",
      "common use cases\n",
      ". amazon lex\n",
      ". amazon polly\n",
      "cases amazon lex\n",
      "cases amazon polly\n",
      ". amazon comprehend\n",
      ". amazon translate\n",
      "\n",
      "Key Topics:\n",
      "common use cases\n",
      ". amazon lex\n",
      ". amazon polly\n",
      "cases amazon lex\n",
      "cases amazon polly\n",
      ". amazon comprehend\n",
      ". amazon translate\n",
      "\n",
      "Processed file: Mod03_Sect02_part1.txt\n",
      "Key Phrases:\n",
      "machine learning\n",
      "credit card\n",
      "target answer\n",
      "api sdks\n",
      "file system\n",
      "training jobs\n",
      "made available\n",
      "know target answer\n",
      "target answer prediction\n",
      "machine learning algorithm\n",
      "machine learning problems\n",
      "machine learning repositories\n",
      "needed machine learning\n",
      "supervised machine learning\n",
      "\n",
      "Key Topics:\n",
      "know target answer\n",
      "target answer prediction\n",
      "machine learning algorithm\n",
      "machine learning problems\n",
      "machine learning repositories\n",
      "needed machine learning\n",
      "supervised machine learning\n",
      "\n",
      "Processed file: Mod03_Sect03_part3.txt\n",
      "Key Phrases:\n",
      "linear relationship\n",
      "fixed acidity\n",
      "citric acid\n",
      "heat map\n",
      "like saying\n",
      "saying proportional\n",
      "two variables\n",
      "linear relationship among\n",
      "acid fixed acidity\n",
      "like saying proportional\n",
      "theres linear relationship\n",
      "linear relationship quantified\n",
      "quantify linear relationship\n",
      "fixed acidity measure\n",
      "\n",
      "Key Topics:\n",
      "linear relationship among\n",
      "acid fixed acidity\n",
      "like saying proportional\n",
      "theres linear relationship\n",
      "linear relationship quantified\n",
      "quantify linear relationship\n",
      "fixed acidity measure\n",
      "\n",
      "Processed file: Mod04_Sect01.txt\n",
      "Key Phrases:\n",
      "means theres\n",
      "time series\n",
      "many opportunities\n",
      "one variable\n",
      "theres one\n",
      "seasonal patterns\n",
      "time component\n",
      "means theres one\n",
      "theres one variable\n",
      ", means theres\n",
      "time series data\n",
      "makes time series\n",
      "think time series\n",
      "time series problems\n",
      "\n",
      "Key Topics:\n",
      "means theres one\n",
      "theres one variable\n",
      ", means theres\n",
      "time series data\n",
      "makes time series\n",
      "think time series\n",
      "time series problems\n",
      "\n",
      "Processed file: Mod02_Intro.txt\n",
      "Key Phrases:\n",
      "machine learning\n",
      "artificial intelligence\n",
      "challenges youll\n",
      "youll face\n",
      "process ,\n",
      ". well\n",
      "face .\n",
      "machine learning .\n",
      "academy machine learning\n",
      "introduce machine learning\n",
      "machine learning deep\n",
      "machine learning instead\n",
      "machine learning used\n",
      "recognize machine learning\n",
      "\n",
      "Key Topics:\n",
      "machine learning .\n",
      "academy machine learning\n",
      "introduce machine learning\n",
      "machine learning deep\n",
      "machine learning instead\n",
      "machine learning used\n",
      "recognize machine learning\n",
      "\n",
      "Processed file: Mod02_Sect02.txt\n",
      "Key Phrases:\n",
      "machine learning\n",
      "supervised learning\n",
      "aws deepracer\n",
      "computer vision\n",
      "reinforcement learning\n",
      "program trained\n",
      "credit card\n",
      "machine learning program\n",
      "machine learning thats\n",
      "area machine learning\n",
      "machine learning ,\n",
      ". machine learning\n",
      "explored machine learning\n",
      "machine learning programs\n",
      "\n",
      "Key Topics:\n",
      "machine learning program\n",
      "machine learning thats\n",
      "area machine learning\n",
      "machine learning ,\n",
      ". machine learning\n",
      "explored machine learning\n",
      "machine learning programs\n",
      "\n",
      "Processed file: Mod03_Sect05.txt\n",
      "Key Phrases:\n",
      "amazon sagemaker\n",
      "target variable\n",
      "linear learner\n",
      "training data\n",
      "kfold crossvalidation\n",
      "machine learning\n",
      "large number\n",
      ". amazon sagemaker\n",
      "amazon sagemaker provides\n",
      "amazon sagemaker container\n",
      "amazon sagemaker linear\n",
      "amazon sagemaker includes\n",
      "amazon sagemaker python\n",
      "amazon sagemaker requires\n",
      "\n",
      "Key Topics:\n",
      ". amazon sagemaker\n",
      "amazon sagemaker provides\n",
      "amazon sagemaker container\n",
      "amazon sagemaker linear\n",
      "amazon sagemaker includes\n",
      "amazon sagemaker python\n",
      "amazon sagemaker requires\n",
      "\n",
      "Processed file: Mod03_Sect07_part1.txt\n",
      "Key Phrases:\n",
      "label class\n",
      "common metrics\n",
      "cat ,\n",
      "metric choose\n",
      "metrics used\n",
      "got cat\n",
      "predicted class\n",
      "common metrics used\n",
      "cat , identified\n",
      "cat , true\n",
      "got cat ,\n",
      "label class also\n",
      "class cat ,\n",
      "actual label class\n",
      "\n",
      "Key Topics:\n",
      "common metrics used\n",
      "cat , identified\n",
      "cat , true\n",
      "got cat ,\n",
      "label class also\n",
      "class cat ,\n",
      "actual label class\n",
      "\n",
      "Processed file: Mod03_Sect02_part3.txt\n",
      "Key Phrases:\n",
      "make sure\n",
      "event history\n",
      "machine learning\n",
      "multiple sources\n",
      "account activity\n",
      "used obtain\n",
      "glue make\n",
      "obtain data multiple\n",
      "make sure understand\n",
      "make sure youre\n",
      "make sure data\n",
      "need make sure\n",
      "also make sure\n",
      "data multiple sources\n",
      "\n",
      "Key Topics:\n",
      "obtain data multiple\n",
      "make sure understand\n",
      "make sure youre\n",
      "make sure data\n",
      "need make sure\n",
      "also make sure\n",
      "data multiple sources\n",
      "\n",
      "Processed file: Mod03_Sect03_part2.txt\n",
      "Key Phrases:\n",
      "scatter plot\n",
      "credit card\n",
      "theyre used\n",
      "particular feature\n",
      "descriptive statistics\n",
      "answer questions\n",
      "left diagram\n",
      "scatter plot matrices\n",
      "create scatter plot\n",
      "pairwise scatter plot\n",
      ". scatter plot\n",
      "use scatter plot\n",
      "scatter plot good\n",
      "scatter plot help\n",
      "\n",
      "Key Topics:\n",
      "scatter plot matrices\n",
      "create scatter plot\n",
      "pairwise scatter plot\n",
      ". scatter plot\n",
      "use scatter plot\n",
      "scatter plot good\n",
      "scatter plot help\n",
      "\n",
      "Processed file: Mod03_Sect03_part1.txt\n",
      "Key Phrases:\n",
      "machine learning\n",
      "domain knowledge\n",
      "correct data\n",
      "data frame\n",
      "row labels\n",
      "data type\n",
      "pandas documentation\n",
      "correct data types\n",
      "correct data type\n",
      "solve machine learning\n",
      "machine learning algorithms\n",
      "domain knowledge access\n",
      "domain knowledge problem\n",
      "least domain knowledge\n",
      "\n",
      "Key Topics:\n",
      "correct data types\n",
      "correct data type\n",
      "solve machine learning\n",
      "machine learning algorithms\n",
      "domain knowledge access\n",
      "domain knowledge problem\n",
      "least domain knowledge\n",
      "\n",
      "Processed file: Mod03_Sect02_part2.txt\n",
      "Key Phrases:\n",
      "aws glue\n",
      "amazon s3\n",
      "etl jobs\n",
      "machine learning\n",
      "data catalog\n",
      "python scala\n",
      "youll need\n",
      "use aws glue\n",
      ". aws glue\n",
      "aws glue api\n",
      "aws glue consists\n",
      "aws glue crawls\n",
      "aws glue enables\n",
      "aws glue fully\n",
      "\n",
      "Key Topics:\n",
      "use aws glue\n",
      ". aws glue\n",
      "aws glue api\n",
      "aws glue consists\n",
      "aws glue crawls\n",
      "aws glue enables\n",
      "aws glue fully\n",
      "\n",
      "Processed file: Mod03_Intro.txt\n",
      "Key Phrases:\n",
      "machine learning\n",
      "amazon sagemaker\n",
      "applied many\n",
      "pipeline applied\n",
      "learning pipeline\n",
      "using amazon\n",
      "able formulate\n",
      "machine learning pipeline\n",
      "many machine learning\n",
      "machine learning model\n",
      "machine learning problem\n",
      "machine learning ,\n",
      "academy machine learning\n",
      "entire machine learning\n",
      "\n",
      "Key Topics:\n",
      "machine learning pipeline\n",
      "many machine learning\n",
      "machine learning model\n",
      "machine learning problem\n",
      "machine learning ,\n",
      "academy machine learning\n",
      "entire machine learning\n",
      "\n",
      "Processed file: Mod03_Sect04_part3.txt\n",
      "Key Phrases:\n",
      "feature set\n",
      "holdout set\n",
      "box plot\n",
      "actually belongs\n",
      "another column\n",
      "belongs another\n",
      "calculated counting\n",
      "actually belongs another\n",
      "belongs another column\n",
      "made holdout set\n",
      "calculated counting number\n",
      "counting number mistakes\n",
      "mistakes made holdout\n",
      "number mistakes made\n",
      "\n",
      "Key Topics:\n",
      "actually belongs another\n",
      "belongs another column\n",
      "made holdout set\n",
      "calculated counting number\n",
      "counting number mistakes\n",
      "mistakes made holdout\n",
      "number mistakes made\n",
      "\n",
      "Processed file: Mod06_Intro.txt\n",
      "Key Phrases:\n",
      "language processing\n",
      "natural language\n",
      "amazon ml\n",
      "managed amazon\n",
      "introduction natural\n",
      "ml services\n",
      "applications .\n",
      "natural language processing\n",
      "introduction natural language\n",
      "managed amazon ml\n",
      "amazon ml services\n",
      "introduce natural language\n",
      "language processing hi\n",
      "language processing ,\n",
      "\n",
      "Key Topics:\n",
      "natural language processing\n",
      "introduction natural language\n",
      "managed amazon ml\n",
      "amazon ml services\n",
      "introduce natural language\n",
      "language processing hi\n",
      "language processing ,\n",
      "\n",
      "Processed file: Mod03_Sect07_part2.txt\n",
      "Key Phrases:\n",
      "correctly identified\n",
      "many false\n",
      "cats correctly\n",
      "choose wanted\n",
      "sometimes referred\n",
      "number actual\n",
      "total number\n",
      "cats correctly identified\n",
      "total number actual\n",
      "many false positives\n",
      "choose wanted make\n",
      "correctly identified divided\n",
      "divide total number\n",
      "percentage cats correctly\n",
      "\n",
      "Key Topics:\n",
      "cats correctly identified\n",
      "total number actual\n",
      "many false positives\n",
      "choose wanted make\n",
      "correctly identified divided\n",
      "divide total number\n",
      "percentage cats correctly\n",
      "\n",
      "Processed file: Mod06_Sect01.txt\n",
      "Key Phrases:\n",
      "machine learning\n",
      "knowledge graph\n",
      "example ,\n",
      "youve seen\n",
      "also known\n",
      "nlp systems\n",
      ". example\n",
      "expertise machine learning\n",
      "machine learning development\n",
      "predate machine learning\n",
      "predates machine learning\n",
      "solve machine learning\n",
      "thats machine learning\n",
      "features machine learning\n",
      "\n",
      "Key Topics:\n",
      "expertise machine learning\n",
      "machine learning development\n",
      "predate machine learning\n",
      "predates machine learning\n",
      "solve machine learning\n",
      "thats machine learning\n",
      "features machine learning\n",
      "\n",
      "Processed file: Mod05_WrapUp_ver2.txt\n",
      "Key Phrases:\n",
      "computer vision\n",
      "custom data\n",
      "data set\n",
      "prepare custom\n",
      "available image\n",
      "concludes introduction\n",
      "ground truth\n",
      "custom data set\n",
      "prepare custom data\n",
      "cases computer vision\n",
      "data set object\n",
      "introduction computer vision\n",
      "required prepare custom\n",
      "used prepare custom\n",
      "\n",
      "Key Topics:\n",
      "custom data set\n",
      "prepare custom data\n",
      "cases computer vision\n",
      "data set object\n",
      "introduction computer vision\n",
      "required prepare custom\n",
      "used prepare custom\n",
      "\n",
      "Processed file: Mod05_Sect01_ver2.txt\n",
      "Key Phrases:\n",
      "computer vision\n",
      "home run\n",
      "object segmentation\n",
      "automated extraction\n",
      "extraction information\n",
      "analysis includes\n",
      "use case\n",
      "computer vision technologies\n",
      "computer vision automated\n",
      "computer vision exciting\n",
      "computer vision incorporated\n",
      "computer vision often\n",
      "computer vision problems\n",
      "computer vision two\n",
      "\n",
      "Key Topics:\n",
      "computer vision technologies\n",
      "computer vision automated\n",
      "computer vision exciting\n",
      "computer vision incorporated\n",
      "computer vision often\n",
      "computer vision problems\n",
      "computer vision two\n",
      "\n",
      "Processed file: Mod03_Sect07_part3.txt\n",
      "Key Phrases:\n",
      "positive rate\n",
      "mean squared\n",
      "false positive\n",
      "also known\n",
      "squared error\n",
      "case ,\n",
      "classified samples\n",
      "false positive rate\n",
      "true positive rate\n",
      "mean squared error\n",
      "positive rate equal\n",
      "positive rate subtracting\n",
      "positive rate usually\n",
      "positive rate graph\n",
      "\n",
      "Key Topics:\n",
      "false positive rate\n",
      "true positive rate\n",
      "mean squared error\n",
      "positive rate equal\n",
      "positive rate subtracting\n",
      "positive rate usually\n",
      "positive rate graph\n",
      "\n",
      "Processed file: Mod04_WrapUp.txt\n",
      "Key Phrases:\n",
      "amazon forecast\n",
      "business problem\n",
      "challenges working\n",
      "list steps\n",
      "make prediction\n",
      "problem solved\n",
      "required create\n",
      "amazon forecast ,\n",
      "amazon forecast make\n",
      "solved amazon forecast\n",
      "use amazon forecast\n",
      "using amazon forecast\n",
      "business problem solved\n",
      "list steps required\n",
      "\n",
      "Key Topics:\n",
      "amazon forecast ,\n",
      "amazon forecast make\n",
      "solved amazon forecast\n",
      "use amazon forecast\n",
      "using amazon forecast\n",
      "business problem solved\n",
      "list steps required\n",
      "\n",
      "Processed file: Mod04_Sect02_part3.txt\n",
      "Key Phrases:\n",
      "amazon forecast\n",
      "quantile loss\n",
      "winter gloves\n",
      "time series\n",
      "less predicted\n",
      "machine learning\n",
      "w quantile\n",
      "amazon forecast .\n",
      "amazon forecast create\n",
      "amazon forecast breaks\n",
      "amazon forecast creates\n",
      "amazon forecast inspects\n",
      "amazon forecast predictors\n",
      "amazon forecast splits\n",
      "\n",
      "Key Topics:\n",
      "amazon forecast .\n",
      "amazon forecast create\n",
      "amazon forecast breaks\n",
      "amazon forecast creates\n",
      "amazon forecast inspects\n",
      "amazon forecast predictors\n",
      "amazon forecast splits\n",
      "\n",
      "Processed file: Mod05_Sect03_part3.txt\n",
      "Key Phrases:\n",
      "amazon recognition\n",
      "recognition custom\n",
      "custom labels\n",
      "ground truth\n",
      "test dataset\n",
      ", amazon\n",
      "correctly classified\n",
      "amazon recognition custom\n",
      "recognition custom labels\n",
      ", amazon recognition\n",
      "use amazon recognition\n",
      ". amazon recognition\n",
      "test dataset .\n",
      "custom labels doesnt\n",
      "\n",
      "Key Topics:\n",
      "amazon recognition custom\n",
      "recognition custom labels\n",
      ", amazon recognition\n",
      "use amazon recognition\n",
      ". amazon recognition\n",
      "amazon recognition\n",
      "test dataset .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def download_stopwords():\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "def extract_key_phrases(text):\n",
    "    # Tokenization of the text data \n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Removing all the stop words\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    phrase_one = BigramCollocationFinder.from_words(filtered_words)\n",
    "    phrase_two = TrigramCollocationFinder.from_words(filtered_words)\n",
    "\n",
    "    phrase_one_score = phrase_one.score_ngrams(BigramAssocMeasures.likelihood_ratio)\n",
    "    phrase_two_score = phrase_two.score_ngrams(TrigramAssocMeasures.likelihood_ratio)\n",
    "\n",
    "\n",
    "    best_phrases_one_list = [phrases_one_list for phrases_one_list, score in sorted(phrase_one_score, key=lambda x: -x[1])[:7]]\n",
    "    best_phrases_two_list = [phrases_two_list for phrases_two_list, score in sorted(phrase_two_score, key=lambda x: -x[1])[:7]]\n",
    "\n",
    "    topics_list = []\n",
    "    for ngram, score in sorted(phrase_one_score + phrase_two_score, key=lambda x: -x[1])[:7]:\n",
    "        topics_list.append(' '.join(ngram))\n",
    "\n",
    "    return best_phrases_one_list, best_phrases_two_list, topics_list\n",
    "\n",
    "def process_normalized_files(input_folder, output_folder):\n",
    "    \n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(input_folder, filename), 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                \n",
    "                key_bigrams, key_trigrams, topics_list = extract_key_phrases(text)\n",
    "                \n",
    "                output_file_path = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}_key_phrases.txt\")\n",
    "                with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "                    output_file.write(\"Key Phrases:\\n\")\n",
    "                    for phrases_one_list in key_bigrams:\n",
    "                        output_file.write(' '.join(phrases_one_list) + '\\n')\n",
    "                    for phrases_two_list in key_trigrams:\n",
    "                        output_file.write(' '.join(phrases_two_list) + '\\n')\n",
    "                    output_file.write(\"\\nKey Topics:\\n\")\n",
    "                    for topic in topics_list:\n",
    "                        output_file.write(topic + '\\n')\n",
    "                \n",
    "                print(f\"Processed file: {filename}\")\n",
    "                print(\"Key Phrases:\")\n",
    "                for phrases_one_list in key_bigrams:\n",
    "                    print(' '.join(phrases_one_list))\n",
    "                for phrases_two_list in key_trigrams:\n",
    "                    print(' '.join(phrases_two_list))\n",
    "                print(\"\\nKey Topics:\")\n",
    "                for topic in topics_list:\n",
    "                    print(topic)\n",
    "                print()\n",
    "\n",
    "\n",
    "normalized_text_folder = 'normalized_text/'  \n",
    "output_folder = 'key_phrases/'  \n",
    "process_normalized_files(normalized_text_folder, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating the dashboard\n",
    "([Go to top](#Capstone-8:-Bringing-It-All-Together))\n",
    "\n",
    "Use this section to create the dashboard for your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e26836136449549794a876cbe2470d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Text(value='', description='Search:', layout=Layout(width='50%'), placeholder='Enter key phraseâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170d835b3c034da494cc23f2b1e78e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d72c66ad9648d38a99c452c05b5d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(height='400px', width='70%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s3_bucket = 'aws-tc-largeobjects'\n",
    "s3_prefix = 'CUR-TF-200-ACMNLP-1/video/'\n",
    "\n",
    "search_field = widgets.Text(placeholder='Enter key phrase and topics', description='Search:', layout=widgets.Layout(width='50%'))\n",
    "search_button = widgets.Button(description='Search', layout=widgets.Layout(margin='0 10px 0 0'))\n",
    "clear_button = widgets.Button(description='Clear', layout=widgets.Layout(margin='0 0 0 10px'))\n",
    "output_box = widgets.Output()\n",
    "video_display = widgets.Output(layout=widgets.Layout(width='70%', height='400px'))\n",
    "\n",
    "search_box = widgets.HBox([search_field, search_button, clear_button], layout=widgets.Layout(justify_content='flex-start'))\n",
    "\n",
    "display(search_box, output_box, video_display)\n",
    "\n",
    "def search_function(button):\n",
    "    search_term = search_field.value.lower()\n",
    "    \n",
    "    matching_files = []\n",
    "    for filename in os.listdir('key_phrases/'):\n",
    "        with open(os.path.join('key_phrases/', filename), 'r', encoding='utf-8') as file:\n",
    "            key_phrases = file.read().lower()\n",
    "            if search_term in key_phrases:\n",
    "                matching_files.append(filename[:-16])\n",
    "    \n",
    "    with output_box:\n",
    "        clear_output(wait=True) \n",
    "        if matching_files:\n",
    "            print(\"Matching files:\")\n",
    "            for file in matching_files:\n",
    "                matching_file_button = widgets.Button(description=file, layout=widgets.Layout(width='auto'))\n",
    "                matching_file_button.on_click(lambda x, file=file: display_video(file))\n",
    "                display(matching_file_button)\n",
    "        else:\n",
    "            print(\"No matching files found.\")\n",
    "\n",
    "def display_video(file_name):\n",
    "    with video_display:\n",
    "        clear_output(wait=True)\n",
    "    \n",
    "    s3 = boto3.client('s3')\n",
    "    video_url = f'https://{s3_bucket}.s3.amazonaws.com/{s3_prefix}{file_name}.mp4'\n",
    "    with video_display:\n",
    "        display(Video(video_url, width=800)) \n",
    "\n",
    "def clear_video(button):\n",
    "    with video_display:\n",
    "        clear_output(wait=True)\n",
    "\n",
    "search_button.on_click(search_function)\n",
    "clear_button.on_click(clear_video)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "b71a13339a0be9489ff337af97259fe0ed71e682663adc836bae31ac651d564e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
